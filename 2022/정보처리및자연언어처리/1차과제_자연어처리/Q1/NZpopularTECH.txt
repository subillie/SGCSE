  

        CHAPTER THIRTEEN    
          Geothermal Generation        
      N  EW ZEALAND has a number
of geothermal fields, most associated with the Taupo vol-
canic zone which runs north-east from the central North
Island to the Bay of Plenty.   In these fields the intense
heat beneath the earth 's surface  'superheats' underground
water under pressure in reservoirs found in areas of
fractured, fissured and permeable rock.   The water does not
boil and become steam until the pressure is released.   The
task is to extract the water or steam from the reservoir and
deliver it as steam to a turbine.  
    Interest in exploiting this source of energy arose
after the Second World War, when it was becoming apparent
that sources other than hydro-electricity might have to be
looked at to satisfy the North Island 's demand for power.
  Semple stated in 1947 that the use of geothermal steam
should not be overlooked and endorsed immediate
investigations.    1       New Zealand
soon became an international leader in the exploitation of
geothermal resources.    
      Wairakei    
    The power station at Wairakei, just north of Taupo,
was the second large-scale geothermal generating station in
the world and the first to use 'water-dominated' or 'wet'
steam.    2       'Wet' steam, which is
characteristic of New Zealand fields, is produced when the
temperature gradient in the field is insufficient to convert
all the water into  'dry' steam.  
    Geothermal energy was first exploited at
Larderello in northern Italy around 1897.   At first it
was impossible to use the natural steam directly because it
was too corrosive of the metals then used in turbines&semi; instead
it was used simply as a source of heat to raise steam in a
boiler.   By 1913 a 250 kW steam-turbine-driven generator
using this method was supplying power to the ancient city of
Volterra.   These developments were familiar to engineers
and others in New Zealand.   In February 1918 the Masterton
Chamber of Commerce asked for an enquiry to see whether the
same technique could be used here.   Nothing seems to have
come of this, but others kept asking the same question.   In
1920 an Auckland entrepreneur asked the government for the
right to generate electric power by geothermal means near
Waiotapu.   Birks reported that generation would be
feasible, but on too small a scale in comparison with
hydro-electric power to be of any commercial use.   In
1924 Dr E. Marsden tried to interest Furkert in the idea after
the journal   Nature   published an article on the
use of steam in Italy.   Furkert 's reply was much the same
as   Birks'   had been - geothermal power would only be of
interest if it was essential to use steam, and in New Zealand
it was not.   Furkert visited Larderello in the late
1920s but saw nothing to make him change his mind.  
    By the 1930s some scientists had become interested in
the chemistry of geothermal water, and a large number of
shallow bores were drilled at Rotorua, Taupo, Tokaanu and
Helensville for heating and bathing.   However, few here
seem to have followed developments at Larderello in the late
1930s, when turbines that showed promising resistance to
corrosion were built.   By 1943 a 70 MW station had been
commissioned there, but it was later destroyed by the
retreating German army.   A Public Works Department engineer
who served in Italy was able to bring back a great deal of
information about what had been done there.  
    Geothermal power began to emerge as a more realistic
possibility during the period of post-war 
  map/plan  
shortages.   The Rotorua Borough Council even asked for the
Italian plant as its share of war reparations.   When
told that this was impossible, and that the State
Hydro-electric Department had no immediate intention of
developing the resource, Rotorua wanted to go it alone.
  Kissel, afraid that a makeshift drilling programme might
be undertaken, argued that any geothermal power
development should be by the state and in the long-term
interests of the system as a whole.  
    By 1947 interest had quickened after two years of
serious drought.   In June Kissel recommended that
investigations be made and trial bores sunk.   Meanwhile
local authorities kept raising the issue.   In 1948 the Bay
of Islands Electric Power Board wanted to exploit the Ngawha
geothermal field and Taupo sought a geothermal plant.   As a
result of this pressure the Commissioner of Works and
engineers visited Larderello to study the techniques used
in deep drilling and the construction of bores.   A
Geothermal Advisory Committee set up in 1949 proposed a
five-year survey of the whole thermal area from National Park
to the Bay of Plenty, but the incessant clamour for
development led to the committee recommending in November
1949 urgent investigation of the most promising area,
Wairakei.  
    Drilling began in March 1950 using light rigs that
were able to reach depths of 750 to 1,500 feet, and a camp for
50 men was set up at Taupo.   Before long it was realised
that they were dealing with something fundamentally different
from Larderello.   They were tapping superheated water that
flashed 'wet' steam when pressure was released rather than
 'dry' steam.   Vigorous investigations continued.   By
April 1951 the Ministry of Works was fairly sure that power
could be produced at Wairakei and anticipated that, as in
Italy, 
  photo  
  captions  
  cartoon  
  photo  
  caption  
deeper drilling would find dry steam at high pressure.   Two
development wells and fourteen prospecting wells had now been
drilled.  
    Cabinet approved requests for new drilling equipment
and to send three officers to Italy for first-hand study of
the drilling techniques used for handling steam at high heat
and pressure.   This experience was of considerable value,
but drilling conditions in the two countries proved to be very
different.   New and larger T12 rigs arrived in late 1952&semi;
they were able to go down to 3,000-4,000 feet and soon proved
the potential of the field, finding superheated water at up to
480&deg;C under pressure.  
    At this time there was a new twist to the story.
  Early in 1952 the United Kingdom Atomic Energy Authority
at Harwell began to consider the development of a plant to
produce 'heavy water' to be used as a  'moderator' in nuclear
reactors.   Sir John Cockcroft, the Director of Harwell,
visited New Zealand on a lecture tour that year and told the
government that momentous defence issues were at stake.
  Britain was to have its own nuclear weapons, and the real
object of the exercise was the production of plutonium rather
than electricity.   The attraction of the geothermal
resource was that nature had provided vast quantities of the
hot water needed to produce heavy water.  
    After initial discussions about other geothermal areas
Harwell advised in January 1953 that it seemed possible to
combine a heavy-water plant and an electricity-generating
plant at Wairakei, given that the former on its own would
waste more than half the available steam.   But the extent
of the resource was still an unknown quantity.   In April
1953 Davenport was on the point of recommending a 20 MW
station, but a combined plant would need more than twice as
much steam as had yet been proved.  
    In May 1953 Cabinet approved the combined plant in
principle, and in August the consultants Merz and McLellan
were engaged to report on its feasibility and cost.   The
government, which wanted to start building the station
straight away, awaited the report anxiously.   In early 1954
the British asked for six to twelve tons of heavy water a year
from 1957.   After receiving the report the Prime Minister
announced that the plant would generate 47 MW (from two 6.5 MW
high-pressure units and three 11.2 MW low-pressure units, with
the heavy-water plant interposed between them) and produce six
tons of heavy water a year as a joint venture.   In February
1955 Geothermal Developments Ltd was formed, with the New
Zealand Government and the British Atomic Energy Authority as
its shareholders.   Wairakei would produce power as a
base-load station at the end of 1958.   The State
Hydro-electric Department would buy power from the company,
and when there was no longer any need for heavy water the
plant would revert to New Zealand control.  
    However, it was by now becoming doubtful whether heavy
water had much future as a moderator, and indeed whether
moderators would be required in the next generation of
reactors.   In January 1956 the British partner withdrew
because of a relative shift in the economics of graphite- and
heavy-water-moderated reactors, combined with a doubling of
the cost of the heavy-water part of the plant.   Wairakei
became a New Zealand geothermal power project.   It was too
late for any radical redesign work, since the contracts for
the turbo-generators had been let in 1955 and fabrication was
well advanced.  
    The basic procedure developed for exploiting the field
was to drill holes down to the reservoir to depths of up to
4,000 feet.   The bores were allowed to vent into the air
for some time to discharge debris.   The holes were then
cased with a steel tube, the lower part of which was slotted
or perforated to allow the water to enter the pipe.   The
surrounding ground was extensively grouted with concrete to
stabilise it, and valves at the wellhead controlled the flow.
  The discharge from the bores goes into cyclone separators
that extract the hot water (which is more than three-quarters
of the total discharge).   The discharge enters the
separator at an angle at high speed, creating a
centrifugal action and allowing the water to drop to the
bottom and the  'dry' steam to be collected by a vertical pipe
near the top.   Waste hot water is led to two adjacent
 'silencing' towers, where its flow is split into two
contra-rotating flows.   The waste steam goes out the top
and water out the bottom and into the Waikato River via the
Wairakei Stream.   It is this waste steam issuing out of the
silencers that is the chief visual feature of the field.  
    The dry steam used for generation travels at speeds of
up to 130 miles per hour along many miles of branch pipeline
of six to twelve inches in diameter to more than twelve miles
of main 20- to 30-inch pipelines and thence to the power
station itself.   In 1972 a 42-inch low-pressure pipeline
was also installed.   At intervals along the pipelines
raised loops take up heat expansion and steam traps deal with
condensation and droplets still suspended in the steam.
  The steam then enters the powerhouse and, depending on its
pressure, drives the appropriate turbines.   High-pressure
steam joins the intermediate-pressure steam after going
through the high-pressure turbines, and intermediate-pressure
steam, after use, likewise joins the low-pressure steam
supplying the low-pressure turbines.  
    The two Wairakei stations (A and B) were sited by the
Waikato River for ease of access to cooling water.   They
were built on raft foundations because of the ground
conditions.   Large quantities of water are required to
condense the steam leaving the low-pressure turbines.
  Condensation increases the pressure differential and
allows the generation of twice the power achievable without
it.  
    An additional two intermediate-pressure machines of
11.2 MW replaced the heavy-water plant, and the planned
capacity of the station was raised to 69 MW.   Wairakei 's
machines were designed to operate variously on high (about 180
psi), intermediate (50 psi) and low (0.5 psi) steam
pressure.  
    During 1956 construction of the station and
development of the field proceeded rapidly.   The major
contract was let to International Contractors, a
combination of New Zealand, British and Swiss firms.   A
camp for 100 men was set up on the site, and some 27
production bores were established.   Work was done on
powerhouse foundations, the cooling-water pumphouse and
culverts, and on five 20-inch steam pipelines, which by 1958
extended eight miles up the valley&semi; branch pipelines were also
being built.   Up to 700 
  captions  
  photos  
were employed on the project when work was at its peak.  
    Because more steam was found in the field, a second
stage of development was proposed and two more high-pressure
11.2 MW units were included in the station, and another
adjacent station with three 30 MW mixed-pressure units (the
turbines could run at either 50 or 0.5 psi) was planned.
  These were the largest geothermal steam turbines in the
world.   This brought the total planned capacity to 190.6
MW.   It was hoped that another two 30 MW units could be
added later as Stage 3&semi; the design took this into account.  
    At this time the consultants Merz and McLellan 
  caption  
  photo  
advised that the project should proceed to Stages 2 and 3,
subject to evidence of adequate geothermal resources, and
could produce as much as 250 MW if the waste hot water could
be converted into steam.   Tenders for Stage 2 were
called.  
    By 1958 drilling for Stage 2 had begun, and the
pumphouse and powerhouse were nearly completed.   The
first unit was commissioned on 13 November 1958, but its
operation remained limited because of the extensive testing
required and early plant failures.   By March 1960 all the
Stage 1 units were producing power and the station was
generating 50 MW at peak&semi; this rose steadily as further units
were brought into service.   By 1962 the station was
achieving the high load factor that was intended.   In the
last two months of that year Wairakei produced more energy
than any other North Island station.   From 1966 (apart from
1968 when the field was partially shut down) the load factor
was a very high 85 to 90 per cent.   The remaining units
were commissioned at regular intervals, with the last 30 MW
Stage 2 unit producing power in October 1963.  
      It was hoped to use the hot water discharged in
the field for producing further steam, and techniques for
transmitting the hot water from the wellhead extraction pumps
to the power station were developed.   Water was collected
from a group of wells and kept pressurised by pumps at the
wellheads to prevent it flashing to steam before reaching the
station.   Such techniques were entirely untried for the
volumes involved.   A pilot hot-water 'flashplant' using hot
water from seven bores was eventually incorporated into the
station.   This supplied intermediate- and low-pressure
steam.  
    Even before Stage 2 was completed and the three 30 MW
machines were commissioned in 1962-63, it was clear that the
Wairakei reservoir was running down&semi; pressures and steam
production were falling off.   The commissioning of Stage 2
accentuated this problem.   The pilot hot-water flashplant
scheme was eventually commissioned successfully in July 1963,
but ran for less than a year before falling water output led
to it being abandoned.  
    Production drilling continued until the mid-1960s, and
the actual peak power output of 173 MW, still short of the
installed capacity, was achieved in 1965.   By then over 120
bores had been drilled, and the emphasis had shifted to
managing the field to sustain existing capacity.   Pressures
for the high-pressure units were allowed to drift downward
over the years, rather than a more rapid rundown at full
pressure.  
    The idea of flashing the separated high-pressure bore
water was not forgotten.   It was applied again from the
mid-1960s, first with double flash units installed at selected
high-pressure wellheads, and then with the establishment of
several semi-centralised double flash plants.   These
produce additional steam by allowing high-pressure hot water
to flash to steam twice (producing intermediate pressure)
or three times (low pressure).   In November 1982 the
process of derating the high-pressure bores to intermediate
pressure began, as pressures had fallen to a level that could
no longer sustain high-pressure generation.   The station 's
capacity was reduced to 157.2 MW as the high-pressure turbines
were decommissioned.  
    By September 1988 the Te Mihi development involving
three wells was completed.   By adding 18 MW, this restored
the station 's output to close to its capacity.   The
reinjection of waste hot water has been investigated since
1982, and in March 1991 work that will enable the reinjection
of more than half of the total discharge was approved as a
first stage.  
    Some 50 wells are currently in production, and the
station produces about 153 MW net, but maintaining this output
requires continued work.   Although Wairakei has not
produced the 250 MW that was initially hoped for in the late
1950s, its output is close to installed capacity.   It has
been a remarkably reliable performer and has the highest load
factor of any station in the country.      
  

          Lies, Damned Lies and Benchmarks    
    By Evan Torrie    
      Computer buyers are
constantly being bombarded by vendors claiming their new
machine runs the XYZ benchmark 20% faster, or that it achieves
50% more widgets/second than the competition.   Indeed,
hardly a week goes by without someone claiming their new
product is the fastest of its type in the world.    
    But how do vendors measure this "performance" which
forms the basis of their claims?   The answer lies with
tests, commonly known as   benchmarks  , which
are explicitly designed to measure the performance of a
system.  
    Just what are these benchmarks, and should we really
take any notice of them when making a decision on which
computer to buy?    
      What Are Benchmarks?    
    A benchmark is simply a method of judging the
performance of a computer product.   Typically, this
involves measuring the time taken to perform a specific set of
carefully defined tasks, and can test anything from CPU and
disk performance through to efficiency of a mouse versus a
keyboard in a word processing program.  
    Benchmarks can be split into two types: microscopic -
those which look at the components of a system in detail, and
macroscopic - those which look at the overall performance of a
system as a whole.   Traditionally, in the microcomputer
world at least, the emphasis has been on microscopic
benchmarks.   These micro benchmarks are useful for
measuring the maximum capability of a component, but they
often do not give a true indication of how the system
performs for the average user.  
    A macroscopic benchmark measures the performance
of a real application program in a particular system
configuration.   Popular microcomputer benchmarks involve
operations such as sorting files in a database,
recalculating a spreadsheet, or scrolling through a word
processor document.   Recently, a set of application-level
benchmarks for RISC workstations has been developed by
SPEC (Systems Performance Evaluation Co-operative).   Up in
the mini and mainframe environment, real application
benchmarks are the order of the day, with the oft quoted TP1
"transactions/s" benchmark for on-line transaction
processing.  
      What Is A Good Benchmark?    
    A good benchmark must be meaningful.   In other
words, it must measure something that is relevant to the
user 's requirements.   It is no use running a benchmark
designed to measure floating point performance if the
application is a string processing program.   Similarly,
running CPU performance benchmarks is not relevant if the
major application is a database, which will usually be limited
by I/O performance.  
    Secondly, the benchmark must be accurate and
repeatable.   That is, a benchmark run tomorrow should give
the same result as it does today.   This cannot always be
guaranteed, especially with benchmarks which use
random numbers, or benchmarks designed to measure variable
factors such as network performance, but the results should be
within a quoted level of accuracy for each run.  
    Thirdly, the results of the benchmark must be
verifiably correct.   Many existing benchmarks do not
actually perform any useful computation, so it is difficult to
check whether the test has performed correctly.   With the
advent of super optimising compilers, which sometimes
optimise to the point of producing an incorrect program,
this requirement is even more vital.  
    Finally, a benchmark should give similar results for
similar systems, but should also be able to discriminate
between different systems.   Many of the popular
benchmarks fail to meet this criterion because they rely on an
intermediate step between what the person who wrote the
benchmark program intended, and what actually gets
executed - namely the compiler - which translates the
high-level language source into machine executable
instructions.   Optimising compilers are especially
notorious for not doing what the author of the benchmark
intended it to do.  
      Common Benchmarks    
    One of the reasons that benchmarking remains an art
rather than a science is that there is no official standard
for benchmarks.   Recent efforts by IEEE and the SPEC group
are attempting to overcome this, but in essence, most of the
benchmarks commonly used today are de facto standards.
  These benchmarks are often poorly-defined which in turn
leaves them open to abuse by vendors.  
      MIPS, VAXMIPS and VUPS    
    One of the biggest misnomers in the benchmark world is
the concept of Millions of Instructions Per Second (MIPS).
  Just a few years ago, it was common to quote MIPS
figures as the number of   native   instructions
executed per second.   This was fine, so long as the CPUs
being compared were the same architecture.  
    However, as soon as you try to compare different
architectures, the concept of   native   MIPS falls
by the wayside.   For example, RISC (Reduced Instruction Set
Computer) CPUs have a goal of executing 1 machine
instruction per clock cycle, whereas CISC (Complex
Instruction Set Computer) chips such as the 80386 typically
take 4 or more cycles to execute their simplest
instruction.   Thus, a 16 MHz RISC chip will execute 16
million RISC instructions per second, versus the 16 MHz
80386 's 4 million CISC instructions per second.   The
naive user would naturally assume that the RISC chip was 4
times faster than the CISC chip.  
    The fundamental flaw in this argument is that each
RISC instruction is very simple, whereas the CISC
instruction is usually much more complex.   For example,
to add a register to a memory location on a typical RISC
machine requires three instructions - one to load the data
from memory into another register, one to add the two
registers together, and another to store the new value back
into memory.   The typical CISC machine, on the other hand,
takes just one instruction to do this.  
    Because of this discrepancy between the two classes of
CPUs, and indeed even between members of the same class, e.g.,
Intel 80x86 versus DEC Vax, a convention has been established
to measure all performance   relative   to a Digital
Equipment Corporation Vax 11/780 minicomputer.   Performance
is measured by running a program on the computer in question,
and then comparing the same program on a DEC Vax 11/780.
  The Vax is taken to execute at 1 MIPS, so the measure of
performance is now termed   VAX   MIPS.   Taken a step
further, more recent nomenclature has introduced the Vax Unit
of Performance, or VUP.   One VUP is taken to be the
performance seen on a Vax 11/780, and results are measured
relative to this to obtain a VUP figure for any
processor.  
      Classic Benchmarks    
    Some benchmarks have been around long enough to be
considered as classics in the world of performance
measurement.   A few of them are beginning to show their
age now, but it is still worthwhile to investigate them to see
why they are beginning to fail.  
    The   Dhrystone   test is one of the best
general-purpose tests developed so far.   It attempts to
measure performance based on a statistical analysis of typical
programs.   It consists of integer and string processing
computations, along with numerous procedure and function
calls.   However, there are no floating-point operations
because, in general, an analysis of common user programs
showed that there was little or no floating point
computation.  
    Both the   Whetstone   and
  Linpack   tests were developed to simulate
typical scientific work, and hence involve almost pure
floating point operations.   The Whetstone benchmark was
based on an analysis of almost a thousand Algol programs.
  The Linpack benchmark simulates typical matrix
computations such as matrix multiplication, and was designed
to test how well vector computers such as the Cray 1
handled vector computations.  
      Other Benchmarks    
    The   SPEC   group was formed just over a
year ago to establish a common basis for testing high
performance workstations.   The benchmarks in the SPEC
1.0 suite of tests consist of Fortran and C programs which
test CPU-intensive computation associated with scientific
and engineering applications such as electronic CAE, CASE and
science applications.   Future SPEC benchmarks will
measure I/O, memory and network performance.  
    One of the most useful benchmarks in the minicomputer
and mainframe world is the   TP1   benchmark.
  This measures on-line transaction performance on
multiuser hosts by simulating a series of banking
transactions.  
    Benchmarking in the graphics area has been extremely
limited to date.   This is surprising considering the trend
towards WYSIWYG systems at all ends of the spectrum.   One
of the problems is that graphics can mean different things to
different people.   For example, three dimensional graphics
presents an entirely different set of problems from two
dimensional graphics.  
    The problem also arises of what we are actually trying
to test.   Should we be testing throughput, i.e., how
quickly we can put a precomputed image on the screen, or
should we be measuring manipulation, i.e., how fast we can
compute that image?  
    Benchmark testing of I/O performance has also been
limited.   Much of what has been done has been too
simple, preferring to measure easily obtained figures
such as peak transfer rates rather than more realistic
situations such as random access or repeated access to
directories and file allocation tables.  
    Another factor that could command an entire article
all by itself is the issue of network performance.   As the
year of the LAN threatens to draw ever closer, networks and
network software need to be examined to determine which system
provides the maximum throughput for a particular
environment.   This can depend on many factors, from
the networking medium on the hardware side, through to the
efficiency of the network software with varying numbers of
users.  
    Many of these decisions depend on the particular
environment in which the system will operate.   Unless you
know of someone who is running an identical setup to yours, it
is almost impossible to gauge exactly how well a
particular setup will perform.  
      What Are The Problems With Micro Benchmarks
...    
    The problem with any microscopic benchmark such as the
Dhrystone benchmark is its sensitivity to factors external to
those which the benchmark is trying to test.   The ideal
micro benchmark would isolate one component of the system,
and test just that component.   Unfortunately, as
computers become more sophisticated, that ideal is
becoming harder and harder to achieve.  
    For example, the Dhrystone benchmark makes
extensive use of small string processing subroutines on fixed
length strings.   Optimising compilers have been developed
which detect that the subroutine can be "inlined" -
instead of making a subroutine call, the subroutine can be
coded directly into the calling routine.   Compilers can
detect that the strings are always the same length which
makes for more efficient code on some processors.   The
Dhrystone test is also very small, using less than 32K for its
data structures and program code on most machines.   For
machines with cache memories, this means that the Dhrystone
can fit entirely in cache, something which is not true of
most real applications.  
    Optimising compilers have certainly made work harder
for the benchmark developer.   Most benchmarks simulate late
amounts of processing by repeating a small set of
instructions multiple times.   Compilers can often detect
that rather than doing something different each time, the set
of instructions is performing the same operations and
producing the same results each time.   So instead of
executing the loop many times, the compiler will optimise the
loop down to just one repetition.   The results obtained
will remain the same, but instead of taking 10 seconds for a
million repetitions, benchmarkers will suddenly find that
their test runs in just a few microseconds!  
    A good example of how factors such as the compiler,
linker, and cache memory affect the Dhrystone benchmark
can be seen with the figures obtained from various 80386
machines.   For 16 MHz and 20 MHz machines, results range
from 1724 to 9436 Dhrystones per second.   This variation of
5.5 in performance is certainly not due to the CPU, which
differs only by a factor of 1.25 in clock speed, even though
the Dhrystone purports to measure CPU performance.  
      ... And Macroscopic Benchmarks?    
    Although macroscopic benchmarks are a much better
indication of real system performance as a whole, they
rely on measuring a "typical" application as seen by the
benchmark developer, and thus are subject to the judgement of
the developer.  
    For instance, most database benchmarks put great
emphasis on sorting large amounts of data, since this is
perceived to be a major database operation.
  However, there are many applications which require very
little sorting, and for which fast searching of an unsorted
database is more important.  
    The other problem with system benchmarks is that they
try to simulate a typical user 's operations.   Yet, because
of the difficulty in setting up a benchmark, many of these
system benchmarks operate in a small environment and then
extrapolate the figures to a large environment.  
    A real-life database, for example, may have 10,000 to
100,000 clients on record.   But most database benchmarks
work with a limited subset of records, say 1,000, and then
extrapolate the results to the larger values.   This is a
very dubious operation, since some products use algorithms
which work well with small numbers of records, but degrade
significantly when the number of records increases.
  Equally, some products designed for large applications
will use algorithms which do not perform very well with small
numbers of records, but require numbers to be above a
"break-even point" before they really start to show their true
performance.  
        Suggestions For Better
Benchmarks    
    The computer industry has realised that many of the
common benchmarks have major flaws, and are working to correct
them.   What can be done to improve the situation?  
    Benchmarks based on isolating system components
need to improve their degree of isolation and modelling of
real world applications.   This may involve rewriting
programs to eliminate any possible compiler optimisation, as
well as increasing the size of tests to reflect real system
usage.   Tests such as the SPEC suite are attempting to
accomplish this with their requirement for full
documentation of any optimisation.  
    As well as measuring the system performance as a
whole, emphasis should also move to how well a system
interacts with the user.   Simple facets of computer
operation such as installation, training and cost of
support are often neglected in favour of hardware cost by the
computer purchaser.  
      WIMP   systems, for example, place a heavy
demand on hardware, requiring fast processors, large high
resolution screens and megabytes of memory.   Hence, the
hardware for a WIMP machine usually costs much more than an
equivalent character oriented computer.  
    However, WIMP proponents have always claimed that
a windowing menu-based system increases productivity in
everyday use, user training and user support.
  Unfortunately, there has been very little qualitative
research done on the benefits of a WIMP system, and this is
one area where a lot more work needs to be done.  
      Do We Really Need Them?    
    Given that there seem to be so many problems with
existing benchmarks, is there reason to take any heed of them
when making a computer purchase decision?  
    Well, yes and no.   Nearly all of the commonly
quoted benchmarks measure only CPU performance.   If
your work involves massive computation problems, such as
scientific or technical research, then a performance
measure such as the Whetstone or Dhrystone test is
probably a pretty accurate indicator of the relative speed
of various machines.  
    However, business use places very different demands on
a system from technical use.   Databases, for example, are
usually heavily I/O oriented which requires fast disk
performance.   Similarly, the increasing acceptance of
windowing-based systems demands a fast graphics subsystem.  
    Although micro benchmarks are the most common and
popular tests, the real issue of any performance
consideration is how well the   entire   system
runs when used for practical work.   This involves not only
the components such as CPU, I/O, and graphics, but also
the human interface features of the software itself.  
    In the future real application benchmarks which
test all these factors will become commonplace.   In the
meantime,   caveat emptor   remains a valid maxim
for any computer purchaser when confronted with a
vendor-supplied benchmark.      
  

        9 OPERATING EXPERIENCE    
    While the HVDC link has had its technical problems and
consequent operating frustrations and
anxieties,    29     at least until solutions
could be found, it nevertheless continues to fulfil its
transmission objectives.  
    In 1989 dollars it has cost an average of $4.89 million
to operate and maintain for each of its 25 years.   This
includes two years when cable repairs cost $19 million and
$15.8 million respectively.   During 1988/89 its $5.75
million cost included a significant amount of upgrading, repair
and corrective work.    caption    photo  
    Since 1970, except for one year, the link has met the
system 's 91 percent annual target availability.   To ensure
the target is met, the yearly aim is 95 percent availability,
which, in fact, is required for some seven specific months each
year.   The result has been the consistent achievement of one
of the world 's highest availability and utilisation
figures, comparing favourably with even some of the more modern
thyristor links.   In doing so it delivers some 20 to 30
percent of the North Island 's electricity requirements at
around 3.758  cents/kWh less than the average cost of North
Island units generated from a mixture of hydro and thermal
power stations.  
    Since its commissioning, the link has undergone various
modifications, such as aseismic (anti-earthquake) strengthening
of the stands which support the valves and smoothing reactors,
in a steady continuing development to keep ahead of maintenance
needs and improve operational
performance.      30    
      MERCURY-ARC VALVES    
    The mercury-arc valves have been progressively modified
to reduce the number of consequential arc-backs.   The
phenomenon of arc-backs, arc-throughs, main valve commutation
failures etc occurred much as expected when the project was
completed.    caption    photo  
    However, unexpected consequential arc-backs, a serious
form of random and persistent short circuit of two or three
valves, occurred in both rectifier and inverter operation.
  In fact, their appearance on the New Zealand system was the
first anywhere, although they later began to occur elsewhere
around the world.   Modifications to reduce their increasing
frequency were begun in 1969.   By the following year 25
percent of the work had been completed, with the link
maintaining a 94.5 percent availability, comparing well
with most hydro-electric stations.  
    By 1971, 85 percent of the valves had been modified but
the performance of the others continued to deteriorate, the
department 's annual report stating that   "the overall
improvement [resulting from the modifications] in valve
performance does not appear to be as good as expected"  .
  By 1972, however, things had improved and the department
reported that while modifications had still not been completed,
partly because of new and unexpected problems with the
modifications themselves, and partly because of recent
developments which were being incorporated   "the
situation regarding valve performance is much more promising
than it has been for some time"  .    photo  
  caption  
    A year later both modification progress and the
introduction of improved maintenance techniques had reduced
transient valve incidents from 97 the previous year to 54 in
1973.   A year later the number was reduced to ten.  
    Nevertheless, as the valves aged the frequency of all
types of valves incidents occurred at an increasing rate.
  It was eventually found that the anode porcelain
characteristics influenced the rate of deterioration, more
particularly in the case of inverter operation.   New
porcelain anodes were fitted, the valve anode and grid
assemblies were modified and more stringent de-gassing
procedures introduced.   Because valve commutation under low
current conditions, particularly at valve-group starting, was
significantly influenced by stray capacitances of the group
circuits, damping circuits were included in all groups where
they were required.  
    During the first years, some valve internal components
maintenance was carried out by the Swedish manufacturers but
the department later developed the expertise and equipment to
do the work in New Zealand.  
    Most other equipment has performed as well as its AC
counterparts on the rest of the national New Zealand
electricity system.    
      TRANSMISSION LINE    
    There are strong winds, often up to severe gale force,
over the line 's North Island coastal section between Oteranga
Bay and Haywards.   These make maintenance work, such as
replacing insulator strings, the occasion for determined effort
and perseverance by line staff.   Often, there is not the
choice of waiting for better weather before carrying out urgent
remedial work both on the line and at the Oteranga Bay terminal
station.   Operational constraints and the cost implications
of an unscheduled shutdown of the link, especially at times of
high North Island demand, mean that usually the work has to be
done at short notice, whatever the conditions at the time.
  Gales can also cause difficult and sometimes dangerous
driving conditions to Oteranga Bay.   On at least one
occasion a heavy vehicle has been blown off the road and
overturned.  
    However, early maintenance experience was dominated by
insulator-fitting problems, particularly ball hook failures,
bent hanger brackets, trouble with inadequate strength and/or
crossarm bracing.   In August 1975 seven consecutive towers
collapsed and another was badly damaged in Canterbury during a
severe wind-storm.   Transmission was interrupted for five
days while the damaged section was replaced with a temporary
deviation using guyed emergency towers, a massive job with line
gangs working from dawn to dusk with the aid of an Air Force
helicopter.   The deviation was eventually replaced with
a section of new line.  
    By the late 1970s towers near Oteranga Bay were
severely corroded and another near Johnsonville was found to
have a badly cracked leg member.   The latter was replaced in
1981 and a new section of ten towers was built at Oteranga Bay
the following year.   In October 1988 two towers collapsed in
central Canterbury during a severe gale, and were replaced
by emergency towers within 32 hours with winds exceeding 160
km/h, and later by permanent structures.  
    In the early 1970s insulators began to fail due to
cracking.   Rapid wear on North Island insulator fittings was
also being caused by the high number of windy days.   By 1974
insulator cracking on the North Island transmission line
section 's positive pole  photo  
  caption  
had reached such a level that it had to be fitted with
redesigned insulators.   The work was carried out during the
1976 annual maintenance shutdown.  
    During the 1980s insulator cracking gradually
increased.   However, plans to reinsulate more of the line
were overtaken by the need to reinsulate the whole line because
of insulator porosity problems and the need to cope with the
increased operating voltage from 250 kV to 350 kV in readiness
for the upgraded hybrid link.  
    The cracking problems arose as a combination of the
relative newness of contemporary HVDC insulator technology and
the severity of the coastal marine environment.   Being
unique in these respects, they attracted world-wide attention.
  Three different causes of cracking were identified over
many years.  
    The first cause, identified in the early 1970s, was
traced to the insulator pin 's zinc sacrificial collar which
provided a shield against rapid electrolytic corrosion from DC
leakage currents.   But instead of pure zinc, the collar was
found to also contain a small amount of aluminium to help in
its casting.   The alloy 's intergranular corrosion caused the
collar to expand and apply a bursting pressure to the brittle
porcelain "rain-shed" with subsequent radial cracking.
  Collars were henceforth made from pure zinc.  
    In 1982, however, the new insulators were also found to
be cracking.   This second cause was found to be rusting of
the embedded section of the insulator pin below the sacrificial
collar with the resulting expansion force cracking the
porcelain.   The rusting itself was caused by very high
leakage currents on the line 's coastal section bypassing the
sacrificial collar and being driven deep into the pin cement.
  The best solution to this problem proved to be covering the
pin 's entire embedded length with an insulating epoxy
coating.  
    The third cause appeared in 1986 when a string of
DC-type insulators on a North Island AC line exploded.   The
insulators were found to have severe porosity which resulted in
electrical puncturing and subsequent mechanical separation by
the blast from a flashover to the tower.   The maker (NGK of
Japan) was adamant that the porosity was not a manufacturing
flaw and must have developed while the insulators were in
service, and as such was a completely new phenomenon.
  Urgent investigations found the DC line 's insulators to be
similarly affected along its whole length, the most damaged
being those subject to the greatest mechanical stress either
from line loads or stresses by pin/collar expansion.   Later
studies showed the porosity growth phenomenon to be limited to
conventional porcelain&semi; the improved alumina porcelain used by
NGK since about 1970 was not susceptible.  
    New insulators purchased for the line 's reinsulation
represent state-of-the-art HVDC insulator design.   As well
as being of improved electrical performance, they are also
proofed against the three known failures with pure zinc
collars, epoxy-coated pins and alumina porcelain and are also
fitted with sacrificial collars around the caps.  
      OTERANGA BAY CABLE TERMINAL STATION    
    Prevailing southerly conditions sweeping in from Cook
Strait bring salt-laden moisture which often forms heavy salt
deposits on insulators at Oteranga Bay and the immediately
adjacent transmission line.  
    Although some insulator-washing equipment was installed
at Oteranga Bay as part of the initial installation, during the
earlier years flashovers caused by saturated salt resulted
in automatic line-protection-initiated outages on one or both
poles.   Maintenance staff were then called urgently to wash
the insulators with equipment especially designed for the
purpose.  
    Over the years various anti-salt methods were tried
including water repellant substances such as silicone grease.
  None proved very successful.   Permanently installed
water spraying equipment of an improved type, and activated
automatically from current leakage monitors, have since
proved satisfactory for most situations.  
    Salt contamination caused other problems to Oteranga
Bay 's outdoor switchyard equipment, particularly to airbreak
switches whose many small components (springs, pivots etc)
corroded rapidly and caused a loss of contact pressure.   In
spite of regular maintenance, on several occasions this led to
contact overheating and consequent complete burn-out of the
switches.   The switches eventually became quite unreliable
and were replaced with others of a more suitable design.  
    Salt also attacked the aluminium alloy operating arms
of some of the switches which resulted in the metallurgical
phenomenon of "exfoliation".   The trouble was eliminated
with the substitution of arms using an alloy of different
composition.  
    The line 's South Island section has suffered only about
5 percent of the North 's salt-pollution problems.  
      COMMUNICATIONS EQUIPMENT    
    Although the original communications system gave
satisfactory service for over     2O    20    
years, it was replaced in 1988 after the radio valve equipment
became difficult to maintain because of wiring deterioration
and lack of spare parts.   It also suffered from an
unsatisfactory noise performance&semi; mains power was required for
its operation&semi; it had insufficient capacity&semi; and there was
a lack of flexibility when a submarine cable was taken out
of service.  
    The system was changed from two systems in tandem to
three systems with the installation of terminal equipment at
Oteranga Bay.   The change was necessary because of large
losses occurring with the mismatch in impedance between the
transmission line and submarine cable sections.  
    The new system was designed and installed by
corporation staff using power line carrier equipment from Spain
and manufactured by Dimat.   The system considerably
reduced the noise level which by modern communications
standards had become unacceptable.   This was achieved
despite the doubling of the voice-frequency channels to four
and the lowering of the transmitter power to 80 W.   The
control signalling equipment was also upgraded at the same
time, and provision made for collecting data from both terminal
stations.   The new equipment is provided with a 24 V
communications battery in case local power fails.  
      SUBMARINE CABLES    
    By international standards, the   cables'   overall
performance has been very good.   But lying in a harsh
environment and having been laid with relatively elementary
techniques compared with those available today, it was
inevitable that in places the cables would be suspended across
rocks.   At the Oteranga Bay ends of the cables,
bend-restricting armour provides some protection from this.  
    It was eight years before the first electrical fault
occurred.   When in May 1973 a fault did occur, on cable 1 at
the Fighting Bay shore joint, severe burning of the paper
insulation made it impossible to determine the cause.
  Consequently, the cable was cut back on the seaward side to
sufficiently clear the area affected by the entrance of water,
a new joint made and the cable returned to service in September
1973.  
    A similar fault developed in June 1976 on cable 3.
  Severe burning of the paper insulation again made it
impossible to determine the cause.   One possible reason was
the conductors pulling apart from the soldered joining ferrule.
  A joint-brazing technique was developed (and used also in
subsequent repairs) and the cable returned to service in August
1976.  
    A few days later a greatly more serious fault developed
on cable 1, this time in 120 m of water some 15.5 km from
Fighting Bay.   The fault was in the rigid repair joint which
had been made after the cable had "kinked" during its original
laying in 1965.  
      Photinia   was recalled from her North Atlantic
voyaging in October and spent the next five months at a
shipyard while her cable-laying gear was refitted.   After
sea trials in Scotland, using a 700 m length of scrap cable
similar to the Cook Strait cables, she left for New Zealand in
March the following year and arrived six weeks later.   The
sea trials were repeated in Cloudy Bay (near Fighting Bay).
  Then, having loaded 2000 m of cable from the spare stock
kept in Wellington,   Photinia   was moored above the
fault to a complication of anchors and buoys laid by her
support vessel   Lady Vera  , also from Britain and
specially designed to handle oil rig moorings.  
    The repair task began on 13 June.   It was a
difficult job.   On one occasion, for instance, part of the
mooring set-up failed during a storm and the entire system had
to be brought back to Wellington for modifications.  
    The repair process used was that originally attempted
during the cable-laying repair in 1965.   The work, briefly,
involved lifting the cable 10 m above the seabed on the
Oteranga Bay side in the vicinity of the fault and cutting it
with an explosive grapnel from   Lady Vera  .   The
Fighting Bay station continued to feed gas to prevent as much
water as possible from entering the cable, while the Oteranga
Bay end was lifted aboard   Photinia  , the damaged
length cut out, the cable-end capped and laid back on the
seabed.   The Fighting Bay end was then lifted on board, cut
back to good cable,joined to a new length in the ship 's hold
and returned to the seabed as the ship moved towards Oteranga
Bay.   That end was then lifted on board and attached to the
new cable by a "loop" joint.   The loop was returned to the
seabed.   Each of the joints was contained within a rigid
steel frame to both assist in re-laying and as means of
protection on the seabed.  
    By good fortune, the fault had occurred while the link
was being restored to service after a period during which it
had been out of service.   There was therefore no current
flow to burn the insulation, so the joint itself was not
seriously damaged to the extent where evidence of the cause was
obliterated.   The conductors were found to have pulled apart
from the sweated joining ferrule and there was clear evidence
of electrical breakdown of the paper insulation.  
    The repaired cable was finally returned to service on
12 August 1977 and, after discharging surplus gear,
  Photinia   sailed for Britain a week later to resume
her usual trans-Atlantic voyaging.  
    The writer experienced some of the vicissitudes of the
job during a visit to the   Photinia   for several days
while she lay in Cook Strait.   For part of the time the 
ship was buffetted by gale-force winds.   Tethered securely
to her moorings, she responded  photos  
  captions  
only with considerable difficulty to the seas which crashed
against her in sheets of flying spray.   Her mooring wires,
strained bar-tight, continuously "sang", or vibrated, in
high-pitched resonance until one snapped with such force that
the ship recoiled visibly at the sudden release from one of its
restraints.  
      The return to Wellington involved climbing down a
rope ladder into a lifeboat to be carried to   Lady
Vera  , lying several kilometres off, for the rest of the
trip to Wellington.   Although the weather had by then
moderated to a degree, the lifeboat nevertheless rose and fell
some considerable distance at   Photinia'  s side and it
was necessary to judge the moment carefully before jumping from
the ladder.   The journey to   Lady Vera   was long,
wet and uncomfortable, the seas still high enough for both
  Photinia   and   Lady Vera   to be completely
lost to sight in the troughs.   Transferring to   Lady
Vera   also required agility and fine judgement to jump to
her after-deck, which was being continually washed fore-and-aft
by breaking seas.   It was as well the writer was a former
merchant seaman!  
    Unhappily, in 1978   Photinia   was driven ashore
in Lake Michigan by 100 km/h winds and became a total loss.
  However, her crew were rescued, three at a time, by a Coast
Guard helicopter.   Lost with her, but fortunately later
recovered, was a bronze plaque presented by the New Zealand
Electricity Department in recognition of her service in
originally laying the cables and in the 1977 Cook Strait
repair.  
    In July 1980 cable 3 failed again at the Fighting Bay
shore joint although no damage similar to previous shore joint
faults was found.   The fault was repaired with the
conductors being brazed together as in the 1977 submarine
repair.  
    During the repair the cable 's Oteranga Bay shore joint
was stripped down and evidence found of differential movement
between the conductor and the cable insulation, with
significant distortion of the paper insulation.   The joint
was replaced and other joints were later found to have similar
distortions.   It was concluded, therefore, that such
differential movement may have been a significant factor in
previous insulation failures.  
  caption    photo  
    The first problem associated with the suspension of
cables over rocks occurred in October 1981 during the
re-gassing of cable 1 when, following work on an Oteranga Bay
shore joint, gas leaked from under the rubber sheath
surrounding the bend-restricting armour adjacent to the shore
joint.   This indicated a defect in the cable 's lead sheath
somewhere in the specially reinforced bend-restricting armour
section of the cable out in Cook Strait.   The problem was to
pinpoint its exact location.  
    With assistance from the nuclear sciences section of
the Department of Scientific and Industrial Research
(DSIR), a new method of locating gas leaks was developed,
followed by considerable research and development to perfect
formulae for interpreting the resulting information.   The
new method proved remarkably accurate.   With radioactivity
detectors fixed at various positions on the cable and the
injection of krypton 85 as a tracer gas, the defect was found
to be 1746 m from the shore at one end of a 26 m span where the
cable was suspended over rocks.  
    In the summer of 1982/83 the 3000t, shallow draft
cable-laying British ship   Luminence   was used to lift
and repair the cable.   The ship had carried out a similar
repair in Scotland earlier in 1982 and recently had completed
the Orkney connection with the mainland which involved
crossing the 36 km Pentland Firth, a stretch of sea similar to
Cook Strait.   Her double-sheathed hull also provided an
added safeguard while working close to rock outcrops.  
    The job itself was entrusted to the submarine cable
unit of Balfour Kirkpatrick Ltd, part of the company which had
manufactured and laid the original cables.   The work was
difficult and slow.   The cable was cut 900 m from shore and
recovered out to 2000 m with the first 900 m being pulled
ashore.   A new length was attached by way of a rigid joint
and laid back to shore.  
    During the repair process a retired Australian
metallurgist specialising in lead, Dr R C Gifkin, made an
intensive investigation into the exact reason for the lead
sheath 's failure in order to determine possible ways to avoid
such failures on the other cables.   Dr Gifkin found the
cause to be a combination of lead sheath fatigue and creep
failure where the cable touched down at the end of a suspension
point lying over rocks.  
    Diving surveys established where the cables were
suspended for more than 10 m.   Many suspensions were later
stabilised on cables 2 and 3 with mid-span concrete pyramids to
reduce cable sway during tidal current changes.   Concrete
"pillows" were also inserted under the cables where they first
touched down on rock outcrops to shift the point of suspension
to a fresh part of the cable.   Repairs were completed in
early March 1983.  
    During re-gassing a further gas leak occurred in cable
1 's lead sheath.   Radioactive tracer gas measurements
again indicated a lead sheath defect about 3210 m from Oteranga
Bay, beyond the previous repair and in the deeper and more
turbulent waters of the Terawhiti Rip.   However, repairs
at such a point would have needed a ship other than
  Luminence   and would have been substantially more
expensive.   Therefore it was decided to use cable 1 only in
an emergency in future and at half-capacity to reduce the
electrical stress and prolong its useful life as long as
possible.  
    The last major fault was in May 1988 when cable 2 's
sealing end exploded at Oteranga Bay and spilled many litres of
insulating oil in the switchyard.   This was the first
recorded failure anywhere on this type of cable
termination.  
    With cable 1 rated for only 50 percent loading as the
result of its previous failure, a repair force was urgently
assembled to restore the fault and maintain the link 's full
operation.   Skilled jointers were brought from Hong Kong and
Britain and some 90 major activities were undertaken to provide
the resources and facilities for the repair to be made in the
correct environment&semi; for instance, a 15 m high wood-clad
scaffold house was built to weatherproof the jointing area.  
    The repair was completed in two-and-a-half months, on
time and within budget.   Fortunately, half the repair period
was accompanied by unusually low water inflows in the South
Island so that the link 's full capacity was not
required.    31      
    Despite its various problems, however, the link has
been maintained on the basis of a 95 percent or better annual
availability, with close to 100 percent for about six months of
each year.   Except for the first few   years'  
operation and one or two later years when there were major line
or cable faults, this high availability has been consistently
achieved.  
    Importantly, the spare (third) cable has largely
enabled the link to continue at full capacity during cable
faults.   It was laid because of New Zealand 's remoteness,
the unavailability of repair vessels locally or at short notice
overseas.   Cook Strait 's variable weather was an important
factor, too, potentially adding further lengthy repair delays.
  The decision proved a wise one and has repaid its added
cost many times over during the past 25 years.      
  

        Coup on office floor: PCs set to take
over    
  blurb with photo  
      I  n 1975, Microsoft chief executive Bill Gates
made a prophecy - some day there would be a computer on every desk.
  This was inconceivable to most observers then - and an
interesting prophecy from someone not directly involved in
hardware marketing.  
    Today, that vision is clearly in sight.  
    Over the past 10 years, computer technology has changed
the way the world does business.   Over the next 10, that pace
will accelerate.   By 1999, it is estimated virtually every
office worker in America will use a desktop computer.   Their New
Zealand counterparts won't be far behind.  
    Office professionals have come to depend upon personal
computers for everything from word processing, to record
keeping, to data management.   The number of computer users
has grown year by year as hardware and software technology improves
and costs decrease.  
    Personal computer hardware has become more powerful with
the incredible advancement in chip technology.   Over the
past 15 years microprocessor-chip speed and memory-chip
capacity has more than doubled every 3 years.   This rate will
continue for at least 10 more years.   Today 's latest
processor chips, like the Intel 80386 and Motorola 68030,
operate at more than 10 times the processor speed in the original
MS-Dos personal computers.  
    On the software side, the Microsoft OS/2, for example, is
designed to be much more than just another version of MS-Dos.
  It extends the capabilities of the new hardware by
expanding ways people interact with their computers and how they
work together in group settings.  
    Let 's look at what some of these capabilities will mean
to an average user of the 1990s.  
    Imagine a salesperson preparing a report on a word
processor.   While working, this person refers frequently to a
variety of business and encyclopaedic sources stored on a
CD-Rom disc, inserting data and graphics from them into the
document.   A request from the boss for an up-to-the-minute
analysis of regional sales is received on electronic mail.
  The salesperson immediately calls up a preformatted spreadsheet
file and attaches it to a centralised database from a network.
  The spreadsheet automatically queries the database, brings up
the pertinent figures, and makes the necessary calculations.  
    Our salesperson can chart the numbers generated by the
spreadsheet, bring the chart with accompanying figures into the
word processor, and create a memo to send to the boss via
electronic mail.   The report can be distributed to other members
of the department on the network, so that co-workers can attach
comments and, if appropriate, make modifications.   Finally, a
publication-quality document can be printed on a shared
high-resolution printer.  
    In short, the salesperson will be able to collect
information more quickly, analyse it more accurately, and then
present it in a way that communicates more effectively.  
    This scenario illustrates three of the most important ways
  1990s'   technology will make computing more accessible and
useful to a broader range of people: a standard graphical user
interface, true multitasking, and data exchange and
integration.  
      Graphical user interface    
    The introductions of the Apple Macintosh in 1984,
Microsoft Windows in 1985 and OS/2 Presentation Manager in 1988
have forever changed the face of personal computing.
  Instead of requiring customers to learn different (often
convoluted) sets of commands for individual programs, these
graphically-oriented environments give people a consistent,
visually-oriented way to interact with computers.   The
graphical interface reduces command numbers in an application
and allows users to see immediately on screen what their documents
or spreadsheets will look like when printed.  
    The benefits of this technology are so overwhelming that I
predict within three years, more than 80% of desktop computer users
will be using a system with a graphical user interface.  
    With a graphically-oriented application, users can perform
functions that are difficult or impossible to do with
character-based applications.  
    Some of these are merging text and graphics on the
screen and in documents&semi; shrinking full pages of text to get a
sense of overall layout&semi; and using sophisticated formatting tools
that allow the creation of quality output.  
    What 's more, Windows and Presentation Manager define a
single consistent set of interface guide-lines that all
applications will share.  
    IBM has made these guidelines a part of its overall System
Application Architecture (SAA).   So more people will be able to
use more and different kinds of applications - once they know one
application, they 'll be well on their way to learning others.  
    Here at Brimaur Microsoft we receive less than half as many
support calls about these consistent graphical applications than
about character-based applications.  
    The ability to learn applications quickly and intuitively
translates into reduced training costs for corporations.  
    Although this new user interface is a change from the
inconsistent interfaces of today 's non-graphical applications, it
represents an inevitable transition.   The sooner companies
switch their users, the more time and money they will save in the
long run.  
      Multitasking    
    As office workers begin to use more applications,
they 'll discover many ways they can benefit by using those
applications together.   In the salesperson scenario above,
for example, electronic mail was running simultaneously with word
processing&semi; the database and the spreadsheet were exchanging
information&semi; and the salesperson was creating a report that
combined charts, data, and text from multiple sources.  
    Until now, the only way to bring together all these
different types of information was with an all-in-one program such
as Microsoft Works, Lotus Symphony, or Ashton-Tate Framework.
  But corporate users recognise that, as powerful as the
integration feature may be, the individual modules within those
programs fall short of what a dedicated word processor,
spreadsheet or database is capable of.  
    The model for the 1990s is integration, not at the program
level but at the system level, through the OS/2 Presentation
Manager and its consistent user interface.   Thanks to OS/2,
users can select products that fully meet their needs and then run
them simultaneously.   Because Presentation Manager
establishes a standard foundation for all applications,
people can work with applications from different vendors, moving
effortlessly from one to another.  
    One other key feature of multitasking in the 1990s is
applications will be able to share the microprocessor
simultaneously.   Which means the user doesn't have to quit one
application to run another.   Electronic mail, for example, can
operate in the background while the user works on a different
application in the foreground, alerting the user whenever mail
arrives.   Similarly, the computer can invisibly perform a
complicated spreadsheet calculation or database sort behind the
scenes while you 're writing a report on a word processor.    
      Data exchange, integration    
    Under MS-Dos without Windows, sharing data between
applications has usually been limited to the exchange of text-only
files carrying data without formatting information.   What 's
more, the way data is exchanged can change drastically from one
combination of applications to another.  
    When users work with a Macintosh system, Microsoft
Windows, or OS/2 Presentation Manager, they can move data from
one application to another by using a common  -  sense "cut and
paste" technique.   For example, they can now prepare a
graphic image in one application and then move it to another -
such as their company logo on to an electronic form used for
database entry.  
    A normal clipboard requires cut-and-paste commands be given
every time data has to be updated.   In Windows and Presentation
Manager, a special kind of paste command called Pastelink is
supported, which uses Dynamic Data Exchange (DDE) protocol to
establish permanent links between applications.   Links can
be established between applications so when any file is updated,
the information is automatically updated throughout the system.
  These links can be used, for example, to put sales data from a
spreadsheet file into a monthly report in a word processing
file, or to retrieve real-time stock quotations over the phone from
a remote source.   Because the data exchange happens at a high
level, with all formatting intact, even highly complex documents
can be constructed quickly and easily.  
      Workplace benefits    
    Behind all three of these features - graphical user
interface, multitasking, and data exchange and integration - lies
a broader vision about how people will be using computers in
the workplace of the future.  
    For example, a team collaborating on a project such as a
contract or a report will benefit greatly.   One person might
draft a document, another annotate it, and a third edit it - all
on-line.   Comments from people all over the company can be
compiled, with revisions automatically marked.   The group can
store a "library" of documents and graphics, and any member of
the group can pull in graphics or charts.  
    Along the way, the group can track who has reviewed the
document, and can even prevent some reviewers from making
particular types of changes.  
    All this can happen without those people needing to know
whether they 're accessing a mainframe, a minicomputer, or a
database residing on an independent file server.  
    Applications running on the personal computer using the
standard graphical interface will translate requests for
information into machine communications standards - particularly
SQL (Structured Query Language) - so networks using advanced
servers like those based on OS/2 departmental servers can
transparently fetch the information.  
    The goal of networking is simply to combine personal
computing benefits (rapid response, simple interface, a wide range
of software) with large-scale computing benefits (corporate
data, reliability, security) by connecting them.  
    A first-generation approach is to allow the personal
computer to be simply a terminal for running large-machine
applications.   This avoids the need for two displays, but it
doesn't simplify the user interface or allow data to migrate on to
departmental and personal machines.  
    The second-generation approach now being implemented with
OS/2 servers, and sometimes called "co-operative processing" or
"client-server architecture", allows the interface portion of
an application to run on the personal computer even though the data
is stored on another machine.  
    These features also open doors for new classes of
software.  
    New-generation programs will accommodate compound
document processing (merging text, graphics, data, and
ultimately, sound).   This will enable, routinely, production of
annual-report-quality output, and will be highly customisable
through macros.   There will be transparent access to many
different information-packed databases - department or company-wide
- without the need to know or care where the database resides.
  It will be easy to integrate information stored on
information-packed CD-Rom discs, as our salesperson did in the
scenario from the 1990s.  
    A new class of "mission-critical" programs will appear,
written internally by companies for their specialised business
needs and working together with standard applications.
  Examples include transaction processing, financial trading,
and resource scheduling.   Many of these applications will be
well-served by operating systems that support multitasking and
transparent data exchange.  
    Right now, running applications that work together is a key
feature of OS/2 and the next logical step to the 1990s.   But
soon, users will see greater movement toward "families" of
applications.   We 'll see the emergence of on-line sales
training, on-line budgets, regular use of electronic mail, word
processors that easily incorporate graphics, and much more.
  Some forward-looking New Zealand corporates will be regularly
using some of these "new look" packages by the end of the year.  
    The information age has just begun where the way a company
deals with data - making it available easily and quickly to its
employees - will be a key factor in competition.  
    This new "architecture of information" ushered in by the
personal computer revolution represents a major challenge and
opportunity for all companies.    
        DTP takes on colourful hue    
    By Eve Sinton  
      A  pplications as diverse as high-class
magazine production and giving notice of a bomb-threat, in the form
of tattered newspaper headlines pasted onto a crumpled page, call
on desktop publishing technology.   What distinguishes the glossy
magazine from the terrorist 's message is the subtle combination of
graphic design skills and appropriate computer
equipment.  
    Desktop publishing concerns the enhancement of written
communications with page design, layout and graphics, added to text
in a computer environment.   Many word processing programs
include desktop publishing capabilities such as multi-column text,
font manipulation and line drawing features, but desktop publishing
software takes the process further, placing typography and
graphic design tools within reach of anyone with access to a
personal computer.   People are the vital component&semi; without them
screens, keyboards, and clever packages achieve nothing.  
    The term desktop publishing (DTP) was coined by Apple
Computer, whose appealingly Wysiwyg Mac led its evolution, but
it is now a vague, umbrella label for a spectrum of electronic
publishing which includes many types of usage.   The technology
can be viewed on three broad levels: professional electronic
publishing, as used by newspaper, magazine and book publishers,
and advertising agencies&semi; in-house corporate communications
including legal documents, contracts, newsletters, forms, brochures
and advertisements&semi; and personal document presentation where
reports, submissions, notices and posters are enhanced with layout
tools.  
    The proliferation of PCs and page make  -  up programs
has caused alarm among professional typesetters.   Initially,
there were fears of losing business and resentment among staff as
unfamiliar computer technology was introduced into their work.
  However, the concern among professionals today is that people
who are ignorant of design skills produce an avalanche of ugly,
gimmicky documents which lack readability.  
    Although small scale desktop publishing bureaus and
in-house facilities have taken business from the professionals,
there is a trend for some of that work to come back, as customers
find amateur document production fails to meet their
requirements.   Alan Morton, managing director of Jacobsons
graphics communications in Parnell, says:     "The major
benefit of DTP for Jacobsons is we now have a greater number of
clients who have access to the technology and can appreciate the
talent and skills of our people."       Morton is
critical of the way desktop publishing has been presented as
  "boxes of promises"   by vendors, but believes
new users who rushed out to buy publishing systems are now coming
to realise the designer and typographer have been undervalued.
  He points out a computer alone will never make a designer out
of a novice.  
    The quality of low-end promotion and advertising has been
much improved by the use of DTP and enabled small-scale users to
make savings on the cost of typography and design.   The hidden
cost includes lack of impact and a failure to generate the intended
response to a promotion.     "In the growing clutter of
advertising material today 'getting noticed' is critical, and
quality always gets noticed,"   Morton says.  
    Apple Computer 's value-added retailer channels manager,
John Halliwell, says there has been a strong trend for corporations
to form in-house communications divisions.   For example, Fay
Richwhite uses Digital equipment for its banking functions and
Macintosh PCs for all in-house document presentation.
  The company has found, when dealing with Japanese
interests, it is useful to have a document processing team in the
room so agreements can be drafted, completed and signed at the
end of the negotiating session.   This procedure is favoured
by Japanese business people.  
    Computer technology was employed by some large newspaper
publishers for years before the advent of DTP, and large-scale
electronic publishing today includes integrated systems which
cover everything from display advertising and editorial layout to
classified columns, accounting, and circulation functions.
  On this scale the most appropriate selection of equipment
is critical, and there are pitfalls for companies taking advice
from computer sales people who have scant knowledge of the
publishing process.  
    According to Peter Harris, managing director of
Imagetext Publishing Systems in Auckland, the country is riddled
with people who claim to have all the answers.     "The
market has been saturated with salespeople touting fifth-grade
solutions,"   he says, and points out the recent collapse of
some PC dealers has left customers struggling without support
for the systems sold to them.  
    Harris says the benefits of integrated publishing systems
for large-scale concerns come from working smarter rather than
attempting to make savings on equipment.   Good management of
newsprint stocks, the extension of advertising deadlines, and
prompt issue of accounts all enhance revenue prospects.   The
pitfalls of DTP, he says, are exposed when users attempt to be
too clever, sacrificing comprehension and readability for pretty
shapes.  
    Professional publishing systems, while integrating
Macintosh and IBM-compatible PCs, are likely to include dedicated
workstations such as Bedford Meteor and Sun.   Each system
has strengths, and Jacobsons has found much advantage in networking
its entire range of Macs, Dos and Bedford machines so customers can
have the appropriate technology applied to different phases of
their work.   The company 's operations manager assesses each
job and directs it to the best machine.   Some processes are
still quicker if done by traditional manual techniques, so staff
are encouraged to be versatile and mix old skills with new to the
customer 's best advantage.  
    There is no disputing Apple Computer initiated DTP in its
present form, and can justifiably claim to be the industry
standard, but IBM compatibles are rapidly catching up.
  ComputerTime business consultant Craig Betts says with
Pagemaker, ScanJet Plus, Scan Gallery, and Omnipage OCR software
all running under a Windows environment, the Dos solution is on a
par with Apple Macintosh, although he concedes   "it 's still
not quite as tidy as Apple"  .   However, the cost is
significantly lower.  
    Apple 's John Halliwell attended the latest Seybold DTP
conference where he felt IBM was showing little direction as to
which of its three platforms - Windows, OS/2, or MS-Dos - it would
support in desktop publishing.     "90% of stands at
Seybold had Macs,"   Halliwell claims,   "and now
Ventura has been ported to the Mac environment we don't feel any
threat from that quarter."    
    The developers of the Next computer, according to
Halliwell, are still 18 months from having a commercial DTP
product.   When it arrives it could be a serious contender in
the field.  
    While some vendors feel the market may be approaching
saturation, Apple perceives large newspapers with annual budgets of
$50 million or more represent big sales potential.  
    Software distributors certainly anticipate continuing
demand for their products, according to Kate Francis, managing
director of Icon Software, distributors of Xerox Ventura.
    "We have seen incredible demand over the last three
months,"   she says.   Ventura has the capability to
handle very large documents, and its add-on - The Professional
Extension - provides features such as vertical justification, table
editing and scientific equations.   It also has a network server
facility.   Ventura has been purchased by large organisations
including Parliament and universities.  
    Francis emphasises the importance of good support and
training in software sales, and the necessity of good design
skills.     "Some companies expect typists to create lovely
documents with desktop publishing when they should bring in
specialists,"   she says.  
    An early convert to DTP technology for small-town newspaper
production was the   Waitomo News  , based in Te Kuiti.
  Two-and-a-half years ago the company installed a Macintosh SE
file server with two SE terminals and four Mac Plus machines.
  Its branch office in the neighbouring town of Otorohanga has an
SE linked to Te Kuiti by modem.   Software included Pagemaker,
Word and Freehand.  
      The   Waitomo News   experienced problems
when two staff who received training in the use of the system both
left six months later, and management was reluctant to pay to train
replacements.   Although the paper receives good telephone
support from its Auckland supplier, according to typesetter Agnes
Paton, it is not easy to get support people to drive down for
on-the-spot assistance.  
    The paper has upgraded its printer to a LaserWriter NTX,
replacing a slower model,   photo    caption   but the
system has never achieved the hoped-for elimination of paste-up.
  The   Waitomo News   had wanted to produce a full page
ready for print, comprising four A4 sheets, but still has to use
manual paste-up to complete the job.  
    A happier story emerges from Dene O'Brien, New Zealand
marketing manager of Foodtown.   Twelve months ago the
company bought a computer graphics system consisting of a
Macintosh IIcx, 21in high-resolution screen and LaserWriter IINTX.
  Pride of the system is a Truvel scanner, which can scan images
from actual products, such as a can of beans placed on its
flat-bed surface.   O'Brien 's staff think the system is
fantastic.  
    Foodtown uses AdMaker software to create press
advertisements, tabloids, and catalogues.   The whole process
is completed to pre-print stage without manual paste-up or layout
boards.   "    It has revolutionised the way we look at
graphics, and the staff feel they have been released from drudgery.
  The system brings out a lot of talent.     Even though
people were at first apprehensive at the introduction of the
technology, they would never go back to a conventional
system,"   he says.   O'Brien 's only regret is they didn't
opt for full colour capability.  
    Choosing the most appropriate system for a particular
function is a complex task, given the increasing number of options.
  Craig Betts of ComputerTime was asked to give an example of a
good Dos environment system for an in-house business
communications department.   He suggested a 386 computer
such as the 25MHz Hewlett-Packard RS25C, or an SX based machine.
  To this he would add a ScanJet Plus scanner and a
Hewlett-Packard 2D laser printer with a range of scalable fonts
under Type Director.   Software would include ScanGallery,
OmniPage, and Pagemaker running under Windows.  
    Imagetext recently designed an electronic publishing system
for the   Nelson Evening Mail   which included two Macintosh
IIcx 's, a Translator II to the Mail 's existing One System, a Truvel
TZ-3 greyscale Scanner, a PC running Master Planner software, a
Dash 30 52MHz 68030 fileserver and Varitype VT600W plain paper
imagesetter.   Software included Multi Ad Creator, AdWriter,
MacWrite, Desk Paint/Draw, Adobe Illustrator, and Tops Network.
  An ethernet network with Local Talk and SCSI interface was
designed to link the system together.   This represents a total
investment of over $200,000, including installation, on-site
training, and on-going support.  
    Trevor Gray of Renaissance, distributors of Aldus Pagemaker
for Dos and Macintosh systems, expects hardware prices will
continue to drop but software will not, as more improvements
and features are developed for existing applications.   While he
perceives prices as a barrier to small business customers, he
thinks cheaper hardware will encourage more purchasers and this
will flow on to maintain a demand for software.   He
    see    sees     the increasing use of colour, with
links to high  -  end colour separation machines, as the main
trend for DTP in the future.  
    The quality and price of output devices have been critical
in the evolution of DTP, and range from the Apple PostScript 300
dpi LaserWriter through to highly expensive Linotronics and
Tabloid Laser typesetters.   This has been accompanied by the
use of large, high-resolution screens.   Now input devices have
been boosted by the addition of scanners and optical character
recognition software.  
    Colour screens have been in use for some time, and now are
being joined by colour scanners and more affordable colour
printers.   The accurate definition and manipulation of
colour from on-screen editing through proof-printing to the final
product offers savings on time and frustration.  
    Paul Newport, joint managing director of Visual Solutions
in Auckland, also predicts colour as the major new thrust of
DTP through the 1990s, along with open platforms.   Open
systems can already be seen in developments such as Apple 's FDHD
drive which recognises either Dos or Macintosh format 3.5in disks
and can translate files between the two formats.  
    Many applications which run in both environments, like
Pagemaker and Microsoft Word, are able to read files of either
type, complete with formatting.  
    Desktop publishing will develop new twists with the advent
of multi-media and compact disk technology.   Reading will become
an experience mixing on-screen video images, with sound and text -
cost is the only stumbling block.   But costs inevitably come
down.   Will creative skills and tasteful production keep pace
with new technological development?      
  

      Magellan Mace Laplink III System
Sleuth    
  by Doug Casement
      R  emember the days before hard drives
became commonplace and 360Kb was a lot of storage?   The
biggest problem then was keeping floppies accurately labelled,
so data files could be found as quickly as possible.  
    Nowadays, not having a hard drive is the exception,
not the rule.   Indeed, lack of a hard drive can disqualify
you from many software packages, which need megabytes of space
to reside in.   However, while the space offered by hard
drive storage is great, keeping track of all the files stored
can be a hassle - you can get an awful lot of letters on
40Mb!  
    As ever, the software industry was not slow to respond
to a perceived demand and file management utilities have been
appearing regularly in recent years.   Such programs range
from public domain offerings with limited functionality
through to high end commercial packages such as PC Tools and
Xtree.   The latest contender in this market is Lotus
Magellan, described by Lotus as an
  "easy-to-use  photo  
data management system."    
    I wouldn't disagree with that description - Magellan
is easy to use from the moment it is installed.   Of course,
taking advantage of all of its features takes a little
practice but the essential functions are mastered in a matter
of minutes - rather than hours.  
    Apart from the software design itself, the other
factor that makes Magellan so easy to use is the superb
documentation.   In addition to the ring bound reference
manual (entitled Explorers Guide) there are two booklets -
Quick Launch and Ideas.  
    The 22 page Quick Launch booklet covers installation
and the basic operations offered by Magellan.   It is
augmented by the software itself - in addition to the
Magellan program, there are two sub-directories.   One
contains a demonstration program illustrating major features.
  The other holds practice files to avoid accidents early in
the learning curve.   There is also a nine page ReadMe file
with updated information and handy tips.  
    Installation is straightforward - the usual insert
floppy, select drive and type Install routine.   At the end
of installation, Magellan asks if you want to index your files
now or later and warns this may take some time.   It isn't
kidding - it took 35 odd minutes to index a 20Mb drive holding
approximately 17Mb of applications and data.   (It was
an elderly XT - faster machines obviously index a lot more
quickly, with 20 minutes for a 286 with 40Mb drive being an
average.)  
    However, complete indexing only has to be done once
and subsequent updates take only a few minutes, unless of
course, the hard drive data has been almost completely altered
- accidental formats do happen...  
    The other trap for young players, is that while the
index itself takes up only five to seven percent of the disk
space occupied by the files, it requires lots more room for a
temporary file while indexing.   I tried updating the index
for a 20Mb drive holding 19.5Mb of data - Magellan informed me
that there was insufficient space for the temporary index
file.   (Floppy disks can also be indexed, allowing Magellan
to perform its magic on files stored on externally.)  
    Unless there is a compelling reason not to, it makes
sense to index the files immediately - then you can get on
with the fun part - using Magellan.  
    Select the directory where Magellan is installed -
usually Magellan if the default offered by Install has been
chosen.   Type MG and Magellan loads, presenting a screen
that may well become home base for many users.  
    The top line of the screen is the Status line, which
displays program name, mode and other current information
and instructions, while the second line shows the Explore
Path.  
    Below the top two lines the screen is split
vertically, with roughly one quarter to the left of the line
and the  photo  
balance to the right.   The left hand window is the List
Window and the right, the View Window.   Underneath the
windows is an information line and below that, a Function Key
Map,  photo  
showing F key numbers and functions.  
    The basic operations of Magellan are controlled from
the 10 function keys, each of which has a dual role via the
ALT key.   F1, surprise, surprise, is the Help key - and
very good it is too!   The others are Copy, Delete, Print,
Gather, Sort, Launch, Zoom, Explore and Quit.   Hold down
the ALT key and these become Compose, Move, Mark, Rename,
Index, Tree, Macro, Options, Path and DOS.  
    In addition to the function keys, a key letter menu
can be activated by typing either the / or &gt; keys.   In the
various dialogue boxes, where Magellan asks for
information and/or instructions, you can move around using
either the cursor keys or by entering the command key letter.
  All of which makes movement fast and easy.  
    The Explorers Guide suggests that there are five
categories of activity when Magellan could be used.   These
are: viewing file contents, searching for information within
files, organising files and directories, customising
Magellan and with other programs.  
    One of Magellan 's strengths is its ability to let you
view files without opening them, so it becomes an easy matter
to scan through files and decide whether to keep them or trash
them.   Magellan has custom viewers for 1-2-3, Manuscript,
Agenda, Symphony, Word, WordPerfect, Wordstar, Multimate,
dBASE III, Displaywrite, XYWrite and ASCII files, so all
formatting commands are retained when the files are
viewed.   There is also a generic viewer for those files
that were created using programs for which a custom viewer is
not available.  
    Files can be sorted for viewing by: filename,
extension, path, size, time/ date, rank or by mark.   The
latter obviously only works after you have first marked
one or more files.   This can be done by the Mark Up or Mark
Down commands, or alternatively, as you scroll through files
in the List window, pressing the space bar
automatically marks the file the cursor was located
on.  
    Both the List window and View window can be zoomed, to
either get more information about a file 's size, directory or
creation time, or to fully view the contents of a file.
  They can  diagram   
also be incrementally adjusted using Control in conjunction
with the arrow keys.  
    Searching for information is a dream - tell Magellan
what you want and it will go find it - Lotus claim three
seconds to find a single word anywhere on a 40Mb hard drive.
  In use, Magellan was at least that fast and often quicker.
  Magellan also does fuzzy searches and will list files in
descending order for closest matches, which allows concepts to
be searched for.   The threshold for this type of search can
be specified by the user.   (Magellan can also search
network drives, RAM disks and CD-ROM disks - in fact, any
logical drive the PC can access.)  
    The Compose function allows international or
special characters to be entered in an Explore path or
dialogue box, so Magellan can search for them.  
    Organising files and directories is almost too easy -
using the Copy, Move, Rename and Delete functions, shifting
whole directories or parts of them requires only a few
keystrokes.  
    Using the Macro facility, regularly used routines can
be initiated with the minimum of key strokes and Magellan
can be customised to suit the individual.   This
customisation includes number of lines displayed - graphics
card/monitor dependent of course.   Macros can be created by
recording an operation as a macro or written directly with
keystrokes and keywords.  
    A Start Up macro can be specified that automatically
runs when Magellan is launched, switching, say, to Tree
mode, selecting a directory or listing applications ready to
open.   Magellan comes with several pre  -  defined
macros for marking files up or down, sorting by time, repeat
last Explore, exit to DOS, redraw the screen or run one of
five tutorial lessons.  
    The ability to launch applications or load a selected
file including the application that created it, makes Magellan
a very user friendly DOS Shell.   Applications can be
ordered according to frequency of use, so that if XYZ program
is used the majority of the time, it can be placed at the top
of the list - anything to cut down on the key strokes - I 'm
lazy at heart!  
    Unless there is a good reason not to, the best way to
use Magellan is to have it load automatically as the last
command in the Autoexec.bat file.   From there, you can
launch whichever program you need to use, while Magellan
retires to the background, using only 5Kb of RAM.   Exit
the application and you are back in Magellan, ready for the
next operation.   Too easy!  
    One handy facility is the Gather function, which
allows marked text to be collected from one or more files and
written to a new file.   For example, a series of
minutes might contain various references to a proposed
project.   Gather allows you to pull all those
references together and create the bare bones of a dedicated
project document file.   Gathered files are stored in ASCII
format that can be easily imported into your favourite word
processor.  
    As I mentioned earlier, the documentation is
superb.   The Explorer 's Guide is divided into Getting
Started, Tutorial, Reference and an Appendix with glossary and
information on Magellan files, program compatibility,
troubleshooting and an index.   It 's almost worth buying
Magellan just to enjoy the documentation and its light hearted
graphics.  
    In case you get stuck for inspiration, the Ideas
booklet has information on organising files, filtering and
exploring by topic, discovering databases you never knew
you had, using Magellan to pilot other programs, using it as a
program manager, hints on macro ideas and using Magellan to
back up files that have been changed since the last backup.
  Say goodbye to the terse DOS Backup command and hello to a
two key painless back up routine.   The biggest problem I
had reviewing Magellan was deciding when to stop - it would be
all too easy to keep going on about it.   Suffice to say,
it 's the best file/data management utility I have ever
used.   It now resides on the editorial computer and picking
the   Editors'   Choice was easy for once.   Magellan
well deserves it.    
            Big Time for
Newport        
      Newport Electronics have moved into the big
time by adding to their existing vast range of panel
instrumentation, two new, large size, 4-digit panel
displays - the L2Q Series and L4Q Series - the largest of
which is visible at 65 metres.    
    The L2Q Series feature digits 57mm high in a 120 x 264
x 117mm case.   The larger L4Q Series have 120mm digits in a
case 18O x 480 x 112mm.   Both have a number of mounting
options and operate using 18 analog input conditioner
cards which can be configured to directly accept the output of
most standard transducers, temperature sensors, voltage and
current inputs.  photo    
          Chemical Resistant pH and ORP
Sensors        
          "Special measurements require
advanced sensor technology.   The capability of any system
is based on the quality and accuracy of the signal
provided by the sensors but, to achieve the benefits of
advanced sensor technology, the most appropriate sensors,
transmitters and controllers must be integrated into an
effective package."        
    The above is part of a policy statement from Great
Lakes Instruments - measurement specialists, Milwaukee,
Wisconsin.   Sensor technology provides the cornerstone of
the special measurement systems developed by Great Lakes
and their latest product releases are illustrative of this,
incorporating the patented, field-proven Differential
Electrode technique for which they are well known in many
parts of the world.  
    The new releases are pH and ORP sensors, fully
encapsulated and enclosed in bodies fashioned from liquid
crystal polymer (LCP), which provides excellent   photo  
chemical resistance and meets the requirements of
every demanding industrial application.  
    They may be used with acids, bases, alcohols,
hydrocarbons, chlorinated hydrocarbons, aromatics, esters
and ketones.   Chemical resistance is also important in the
measurement of oxidation reduction potential.  
    ORP monitoring is of importance in industrial waste
disposal systems, such as chromate waste baths, cyanide wastes
from metal plating and metal treatment processes.  
    The encapsulated design eliminates moisture and
humidity problems and extends the working life of the sensor.
  The low heat distortion of LCP allows these sensors to be
used in metal fittings without risk of the leakage usually
caused by the heating/cooling cycles of other materials.  
    Two standard mounting styles are offered - a
convertible style for submersion or flow-through mounting
and a union-style mount for easy service in flow-through
installations.   Optional mounting hardware assemblies are
offered in varied materials, each including a pipe-mount
or surface mount junction box with terminal strip.  
      The patented Differential Electrode measurement
technique, developed by Great Lakes, uses two glass electrodes
- a process electrode and a standard electrode immersed in
buffer -to make pH measurements differentially.   A third
metal electrode is added to reduce ground loop problems.
  In this way, errors from contamination, such as
precipitate build-up, and electrode drift are virtually
eliminated.  
    A double junction salt bridge electrically links
the standard electrode to the process, making contamination
unlikely and, since ground-loop current bypasses the
standard electrode, the reference signal is not affected
by precipitate build-up on the salt bridge.  
    Compared to other, more conventional techniques,
this patented sensor provides measurements of greater
stability over long periods, with less down  -  time
and maintenance.  
    These pH and ORP sensors are offered with an
integral preamplifier or two-wire transmitter which provides a
4-20 mA output proportional to pH.   Each sensor also
includes a temperature sensor to compensate automatically for
process temperature variations.  
    The sensor body has a distinctive hex shape to
facilitate installation.   Threads are provided on both ends
so that the sensor may be mounted into a pipe tee or attached
to the end of a pipe for submersion applications.  
        PYRO! - Preventing Screen
Burn-in...      
      Screen-Saving Utility Offers User-Configurable
Displays, Password Protection    
    Fifth Generation Systems announced the release of
Pyro! for the PC, a screen-saving utility modelled on the
nearly omnipresent Macintosh Pyro!.   Pyro! prevents screen
burn-in and provides an entertaining display during idle
computing time.  
    Pyro! comes with four modules, including Fireworks and
Bouncing Clock (the two original modules for Pyro! Mac).
  The other modules are the user configurable "Roving
Picture" and "Message".   The program works in colour on
CGA, EGA or VGA systems and also supports monochrome
displays.  
    Pyro! can be set to "pop-up" after a user-specified
period of keyboard or mouse inactivity or through the use of a
"hot-key" combination.   Once the keyboard or mouse is
used, Pyro! will end until the next idle period.   Pyro!
also provides optional password protection.   If a user
chooses to install a password, only that password will end the
display.  
    Screen-burn is the etching in of an image that results
from a monitor being left on for extended periods of time.
  For example, a user that consistently uses one
software package will eventually find that the borders, menu
etc of that screen remain as a "ghost" image when in another
software package or even when the monitor is turned off.
  Many users also favour Pyro! because its mainly dark
display is easier on the eyes during idle computer time than a
bright screen.  
    FGS President and CEO Barry L. Bellue Sr. sees the new
product as an indicator of the convergence of the PC and
Macintosh markets.     "Three years ago, most PC users
would have seen Pyro! as a frivolity, now everyone wants it.
  We stopped trying to count the requests long ago.   The
need for a screen saver on the PC side is certainly great with
all of the paper white and expensive colour monitors but I
think that Pyro! on the PC is also a sign of the shrinking
difference between     Mactinosh    Macintosh    
and PC users."    
      "Pyro! for the PC is our most requested
product ever,"   said Dariel LeBoeuf, FGS Product
Manager.     "I must admit that this is a case where I
had a little 'Mac Envy' myself.   Most people would be
surprised to learn that Pyro! Mac 's biggest area of success is
in the corporate market, where it 's purchased in volume
through traditional channels, just like a spreadsheet or
database.   Of course, we 'll try to keep Pyro! as fun as
possible but it 's wrong to think of screen savers as
toys."    
    The memory-resident driver takes up only 5K of RAM and
works with virtually any graphic display configuration.
  Pyro! PC is available immediately for $96 excluding
GST.  
    Fifth Generation Systems, Inc. is a leading developer
of Macintosh and MS-DOS products incorporated in 1985.
  Their Macintosh products are Fastback II, Suitcase II,
Pyro!, DiskLock, FileDirector, SuperSpool, SuperLaserSpool,
and the Fastback Tape 120, a high-speed Macintosh tape backup
drive.   For the PC, FGS produces Fastback Plus, Brooklyn
Bridge, the line of Mace Utilities, Direct Access, Direct Net,
The Logical Connection and The Logical Connection
Plus.  
        FILTERS PROTECT COMPUTERS      
      An advice and supply service for filters to
protect computers and all micro  -  processor systems from
the increasing incidence of damage from mains borne
interference is announced by Cory-Wright Group Ltd.    
    The Company reports a marked increase in demand for
interference protection from all types of users.  
    Filters range from the popular single, machine
protecting cord set, 6 amp rated and conforming to
international standards, to 200 amp 3 phase and neutral units
for system protection at source.    photo      
  

          Chemophobia      
        The Fear of Chemicals.   Is it
justified?      
      The fear of chemicals, especially pesticides
and fungicides, is growing throughout the world.   But does the
available evidence support the concern?    
      S  ynthetic chemicals play a
vital role in all modern industrialised societies.   These
man-made chemicals are often essential to our quality of life and
have an important influence on our food, health, transport and
leisure activities.   Without them our lifestyles would
change dramatically.   They are an integral part of our lives&semi;
from what we brush our teeth with to how we communicate and how we
combat disease.  
    Most of us take them for granted, even the ones we use
every day.   But chemophobia - the fear or aversion of chemicals
- appears to be on the increase throughout the western world.
  For many people, chemophobia equates to the effects of one
particular group of chemicals&semi; synthetic pesticides.
  'Pesticides' is a general term which includes insecticides,
herbicides, fungicides and other chemicals used to control insect
pests, weeds and diseases and parasites of plants and animals.  
    Pesticides are a cornerstone of modern agricultural
production throughout the world and New Zealand is no exception.
  For the last 40 years they have played a very important part
in allowing New Zealand farmers and orchardists to increase
production to high levels and to maintain profitability.  
    In the absence of pesticides meat, wool and dairy
production would all be significantly lower, and the labour
requirements of agricultural production substantially higher.
  We could not consistently produce apples, kiwifruit and
other horticultural crops to the high quality standards demanded
by our major overseas markets without the judicious use of
pesticides.   Pesticide use in NZ agriculture has unquestionably
provided enormous direct and flow-on benefits to both producers
and society as a whole.  
    There is also a risk with pesticides.  
    Many of them are highly poisonous to non-target organisms,
and if used carelessly pose a threat to livestock, people, and
beneficial insects.   Their use must therefore be carefully
controlled to ensure that any potentially harmful
side-effects, such as pesticide residues on the food we eat and in
the environment, do not exceed acceptable levels.  
    In New Zealand, and many other countries, pesticide use is
regulated by statute.   Use is no longer permitted of most of
the first generation pesticides&semi; those which were highly toxic or
very persistent in the environment such as DDT.  
    Pesticides are often given a bad image, particularly among
the more sensationalist elements of the media.   Urban dwellers
especially tend to be more aware of the potentially harmful
effects of pesticides than of their benefits.   A major USA
study which investigated health hazards in the food eaten by
Americans found that the main hazards, in descending order of
importance, were:
    Microbiological contaminants of food, such as
bacteria.  
    A nutritionally unbalanced diet  
    Environmental contaminants  
  photo  
    Natural poisons in food  
    Pesticide residues  
    Food additives    
    When the researchers compared the media coverage given to
these dietary hazards they found that most media attention was on
pesticide residues, followed by environmental contaminants and
food additives.   Natural poisons had virtually no coverage&semi; but
in fact these poisons are a greater hazard than most man-made
chemicals.  
    Examples of natural poisons which occur in foodstuffs are
mycotoxins produced by fungal contaminants.     Aspergillus
flavus   for example is a widespread fungus which commonly
occurs in many parts of the world as a preharvest contaminant of
peanuts and grains.   This fungus produces a variety of
mycotoxins including aflatoxin B1, a naturally occurring
chemical which is among the most potent carcinogens known.  
    In the USA, aflatoxin testing of peanuts used in food
processing is mandatory, and similar measures are proposed for
maize.   In NZ, the maximum allowable concentration of aflatoxin
in foodstuffs is 15 parts per billion.   (See Table
One).  
    The ergot alkaloids are another example of naturally
occurring poisons in food.   Ergot is associated with the fungus
  Claviceps   which grows on wheat, other cereals, and
grasses.   Consumption of products made from contaminated
cereals, or animals grazing affected pastures, results in ergotism
- which can be fatal to humans and livestock alike.   Use of
fungicides can reduce the incidence of Claviceps and hence the
risk of ergotism - one of many examples where the use of
pesticides can result in a healthier diet.    
    The bad image which pesticides are often given by the
media reflects little more than the fact that newspaper sales and
television ratings are more likely to be boosted by stories
which focus on the spectacular and emotive.   Many people will
respond more readily to the fear engendered by pesticide residues
on their food than to the far greater health risks inherent in a
nutritionally unbalanced diet.  
    We eat more fat than any previous human generation.
  That high-fat diet is strongly linked to excess weight,
coronary heart disease, gall stones, adult-onset diabetes, and
many of the most common forms of cancers.   Yet we often prefer
to ignore the dangers of our own over-indulgence and concentrate
instead on issues for which someone else can be held responsible.
  Pesticide residues are an easy target.  
    Are pesticides a health hazard in New Zealand?  
    Chemophobia arises partly from the belief that the use of
pesticides is taking a heavy toll on our health and environment.
  In addition to pesticide residues on our food, there is a
potential risk from direct exposure to pesticides through spray
drift for example, and from the build-up of pesticide residues in
soil and water.   How much pesticide contamination of our food
and environment is occurring?  
    New Zealand 's existence as a food exporter depends on our
ability to produce high quality, high return products which meet
the needs of consumers in our major overseas markets.   Our
export products are required to meet stringent quality standards
to enter these markets, including pesticide residue tolerance
levels.   Maximum residue limits (MRLs) on food are typically of
the order of one to 25 parts per million for individual
pesticides, in some cases less than one part per million.  
    Pesticide residue MRLs are set by using toxicological data
obtained for animals - divided by a large safety factor of 10 to
10,000 to allow for uncertainties when extrapolating to humans - 
  table one  
to derive an acceptable daily intake&semi; the level of residue which
if ingested by humans daily over an entire lifetime, will not
result in an appreciable health risk.   This is then compared
with the potential daily intake derived from food consumption
data and actual crop residues found in field trials using maximum
application rates and minimum with-holding periods.  
    The MRL is then set based on the worst-case crop residue
as long as the potential daily intake does not exceed the
acceptable daily intake.   No MRLs can be set unless the
toxicological consequences of the residues in the foodstuff are
insignificant.  
    In fact, because MRLs are based on worst-case field data,
actual residues are generally well below the set limits.   Some
pesticides can cause cancer when fed in large doses to animals
such as rats and mice, but small traces found in foods which meet
MRLs do not produce these symptoms and are not known to cause
health problems.  
    Hence New Zealand 's primary foods, grown to export
standards, are not known to pose any significant health risks.
  For most of these foods no distinction is made in pesticide
use between produce which is exported or consumed within NZ.
  In the case of kiwifruit for example, it is not until the
fruit passes over the grading table that some is rejected for
export - on such grounds as shape and size - and diverted to local
consumption.  
    Monitoring by MAF chemists of export fruit crops over the
last decade indicates that pesticide residues very rarely exceed
the tolerance levels set in major markets.   Pesticide use on NZ
fruit crops is high by standards for domestic USA but low by those
of the UK and Japan.   Nevertheless, the non-persistent nature
of most modern pesticides means that there is not a
correspondingly large build-up of residues on fruit.   Only
the last one or two applications before harvest contribute
significantly to total pesticide residues.  
    Pesticide residues have been monitored less intensively in
NZ on produce which is grown solely for domestic consumption, such
as local market fruit and vegetables.   Monitoring which has
been done, by DSIR and the Department of Health, has revealed no
obvious health risks from pesticide residues in New
  Zealanders'   diets.   MAF, DSIR and the Health Department
are currently conducting more detailed monitoring of residues in
domestic foodstuffs.  
    It can be concluded from the extensive export monitoring
and limited domestic monitoring which have been conducted to date
in NZ that pesticide residues in food are not 'a major health
risk'.   This monitoring also shows that, overall, there is
a good degree of compliance with spray recommendations for
particular crops and no evidence of widespread misuse of
pesticides by farmers, orchardists, spray contractors or
other pesticide applicators.  
    These residue data are in accordance with extensive food
surveys conducted in other westernised countries with broadly
similar patterns of pesticide use to our own.   In the USA, UK
and Sweden, for example as in NZ, pesticide residues in the diet
are generally well below current health guidelines with at
least 50% of samples having no residues detectable.  
    There is likewise no scientific evidence that direct
exposure to pesticides in spray drift is a major health risk to
New Zealanders.   It is true that not all spray reaches the
target, which is generally soil or the foliage of a particular
crop or weed.   Most spray drift generated during pesticide
application enters the true vapour phase and is dispersed in
the general atmosphere in extremely dilute form.   Research
conducted by MAF and the Agricultural Engineering Institute showed
that depositable drift from orchard airblast sprayers was very
low.  
    Research is continuing in this area, with financial
support from the New Zealand Kiwifruit Marketing Board, to
document more fully the dispersal of pesticides from orchard
spraying.   The only scientifically documented cases in NZ which
have demonstrated harmful effects of spray drift have involved
herbicides affecting non-target plants, generally from aerial
application.   Crops such as grapes and tomatoes are extremely
sensitive to phenoxy herbicides (hormone sprays) and can be
affected at doses as low as 0.1 to 1.0% of a full spray rate.
  Exposure to the very low concentrations in spray drift is
likely to be well below levels which are known to be significant
threats to human health.  
    New Zealand has an international image as a "clean,
green" producer of top quality agricultural and horticultural
products, but is this image threatened by the build-up of
pesticide residues in our environment?   The available
evidence suggests that the answer is generally no.  
    Extensive monitoring conducted by MAF chemists, for
example, has revealed that pesticide residue levels in the Manukau
Harbour are well below internationally accepted health guidelines
and often non-detectable.   No pesticide residues have been
detected in groundwater from the Auckland or Canterbury regions.
  Research coordinated by the Ministry for the Environment on
the herbicide 2,4,5-T did not find any 2,4,5-T or dioxin (TCDD)
residues in sheep meat from high-use properties.  
    However, the levels of DDT and its breakdown products are
still relatively high in some NZ soils.   This persistent
organochlorine insecticide was widely used on NZ pastures to
control grass grub and other insect pests, from the early
1950 's - when little was known about the environmental persistence
of some pesticides - to the late 1960 's when the use of DDT on
grazed pastureland was banned.   It will take many more years
before DDT residues disappear completely from some South Island
dryland farms, although the levels which now occur are not known
to represent a human health hazard.  
    The fact that there has been no repeat of these problems
in NZ since DDT was banned indicates the advances which have been
made in pesticide registration and use.   Modern pesticides are
biodegradable and do not persist in the environment like the older
organochlorines.  
    They also tend to be more active and can therefore be used
at lower application rates than older pesticides.   Synthetic
pyrethroid insecticides, for example, are generally applied at
rates of grams per hectare, compared to kilograms per hectare
for older insecticides.   There is also a marked movement in
the agrochemical industry towards the production of
"environmentally soft" pesticides such as insecticides which do
not harm the natural predators of insect pests.  
    The available scientific evidence indicates that the
health of New Zealanders is not significantly threatened by
pesticide residues in our food and environment, or by exposure to
spray drift.   It must be emphasised that in these cases,
science deals in probabilities based on the available
knowledge.   The public desire for absolute assurances
cannot be met&semi; but this should not lead to the dismissal of
assurances based on informed judgement of the risks.  
    Regardless of the scientific evidence and its
interpretation by experts, there remain some people with a sincere
belief that any exposure to pesticides, no matter how low, is
dangerous to human health.   The views of these people must be
considered in the political arena, where value judgements are
made in relation to the desired degree of balance between the
benefits and risks of pesticides.  
    It must also be emphasised that NZ has no grounds for
complacency regarding the risks associated with pesticide use.
  Current increases in the monitoring of pesticide residue
levels on the food we eat and in our environment must be
maintained to ensure that any incipient problems are detected
before they become unmanageable.   There is also a need for more
education of pesticide users, and for improvements to
methods of disposal of surplus pesticides and containers on
and off farms, orchards and home gardens.  
    Although there is no scientific evidence that pesticide
use significantly threatens the health of New Zealanders,
chemophobia and pesticide residues have become key issues in
the future of our export food and fibre industries.   Advances
in analytical techniques make it possible to routinely detect and
measure quantities of chemicals such as pesticide residues at
extremely low levels (parts per billion and lower).  
    Most pesticides can now be detected at much lower
concentrations than the levels which are known to be hazardous to
human health.   This has made it possible for importing
countries to set tolerance levels for pesticide residues at
correspondingly low levels, in order to establish a wide safety
margin between tolerance and hazardous levels, and/or in response
to political pressure from the growing environmental awareness
and anti-pesticide lobby in many western countries.  
    Market acceptance has become a separate issue from
health risk.   As environmental concerns continue to grow,
countries such as NZ which export food to the affluent nations
of the northern hemisphere are at risk of losing markets if
they cannot meet extremely low pesticide residue tolerance levels.
  Further reductions in these tolerance levels and in the
range of acceptable pesticides in key markets will heighten
the threat to the access of our primary exports.  
    In recognition of the trend towards lower tolerance level
for pesticide residues, organisations such as MAF and DSIR are
making a substantial investment in research on the
development of control methods for pests, weeds, and plant
diseases which are less reliant on pesticides.   These
approaches include biological control, pest and
disease-resistant plants, integrated pest management and
organic growing.   Considerable progress has been made,
particularly for pasture pests.  
    Alternatives to pesticides and methods of reducing
pesticide use are now well established in NZ pastoral farming.
  Examples of control methods which have been developed by
MAF researchers since 1975 and are now used widely by farmers
include: Biological controls (predatory beetles and wireworms)
to control Australian soldier fly&semi; Ryegrasses containing
endophytic fungi which confer resistance to Argentine stem weevil
and black beetle&semi; Farm management practices (grazing,
avoidance of cultivation) to control grass grub and porina
caterpillar&semi; Selective grazing pressure and reduced insecticide
applications to control lucerne flea&semi; Reduced insecticide
applications and pasture management to control cattle ticks&semi;
Simple population monitoring techniques for black field cricket to
target insecticide applications.  
    These control methods have made an important contribution
to the 63% decline in insecticide use on NZ pastures during the
1980 's, although the economic downturn of farming during this
period also contributed to this trend.   Research is
continuing on the development of alternatives to chemical
pesticides for control of pasture pests&semi; e.g., MAF/Monsanto
research on a biological insecticide for grass grub based on a
naturally occurring bacterium, and MAF/DSIR research on biological
control of Argentine stem weevil.  
      Development of satisfactory alternatives to pesticides
is more difficult in horticulture than in pastures, because of
consumer preference for high quality, unblemished fruit and
vegetables.   This cosmetic factor demands a very high level of
pest control, which is often unattainable by any method other than
the use of insecticides.   The use of insecticides in
horticulture is increasing.   Kiwifruit, for example, is a host
plant for pests such as scale insects and leafroller
caterpillars.  
    Most NZ kiwifruit growers currently apply seven or eight
insecticide sprays on a calendar basis during the growing season,
to prevent serious fruit contamination of these pests.  
    The most immediate prospect for reducing this
relatively high chemical input is the development of a monitoring
system for scale insects, to allow insecticidal sprays to be
applied only when sufficient pests present to warrant treatment.
  This approach is currently being investigated by joint
MAF/DSIR research funded by the New Zealand Kiwifruit Marketing
Board.   It may also be possible to use pheromone traps under
development by DSIR to monitor leafrollers on kiwifruit, as
has been achieved for apples.  
    These approaches may reduce insecticide inputs, but
will not eliminate them.   Substitution of the currently used
insecticides with 'environmentally soft' materials which
affect only pests may also be possible.   One such
insecticide, based on the   Bacillus thuringiensis   which
is active against leafroller caterpillars, is already registered
for use on kiwifruit in NZ.  
    Other research, such as evaluation of other potential
biological control agents, identification of pest-resistant
kiwifruit cultivars, and development of organic growing
methods, may eventually allow the use of insecticides on kiwifruit
to be reduced further, but these are long term possibilities.
  For the next few years at least, production of high
quality kiwifruit for export will continue to rely on the use of
insecticides.  
    Tolerance of a low level of fruit contamination by
pests in major overseas markets, rather than the current nil
tolerance, and/or a high price premium for organically
grown produce, may be necessary before it is possible to
greatly reduce or eliminate insecticide use on kiwifruit and other
NZ horticultural crops grown for export.  
    Most advances in biological control and integrated pest
management have been made with insect
    pest    pests     and mites.   In comparison,
the development of alternatives to pesticides for the control
of weeds and plant diseases is in its infancy.   Some progress
is being made, e.g., MAF research on biological control of
plant diseases with antagonistic micro-organisms, and DSIR
research on biological control of weeds with insects.   At
present, herbicides make up about 60% of the NZ pesticides market,
compared to 24% for fungicides and 12% for insecticides.  
    Research on integrated pest management strategies, and
alternatives to pesticides such as biological control, often
has a high risk of failure.   This type of research requires
large investments with uncertain outcomes.   Despite the
substantial investment in this area of research in NZ and
worldwide it is unrealistic to expect cost-effective alternatives
to pesticides to be available for the control of all major pests,
weeds and diseases, at least within the next 10 to 20 years.  
    It must also be recognised that integrated pest
management usually involves the transfer of more complex
information than simple calendar-based spraying and is therefore
more difficult to implement without specialist consultancy
services.   Growers are often reluctant to move away from
calendar-based spraying, unless they receive significant,
direct economic benefits from integrated pest management.  
    The political impact of chemophobia and other
anti-pesticide views is a major issue in the future of NZ 's
primary export industries.   Market access depends on our
ability to meet pesticide residue tolerance levels.   These
tolerance levels may become more stringent than those which
currently apply in our major markets, even if current levels are
already well below the levels which are known to provide a
significant health risk.  
    The market access issue has given high priority to
government and producer board investment in research on the
development of control methods such as biological control and
integrated pest management.   The success or failure of this
research may decide the long-term future of some of NZ 's export
crops.  
    The trend towards less use of chemical controls should
also relax any environmental pressure - real or imagined -
which may result from the use of pesticides.   Nevertheless, it
is likely that control of many pests, weeds and diseases will
continue to rely largely or entirely on pesticides for at
least the next 10 to 20 years.      

  

        PC waters looking a little
muddier    
      T  EN years ago this month, IBM gave birth
to the personal computer and spawned a billion-dollar
industry.  
    And at the Wellington Show and Sport Centre last week,
one of the side-products of the birth - the personal
computer exhibition - attracted droves of the faithful who
swarmed to pay homage to the PC.  
    IBM was there with its notebook computer - yet to be
officially launched in New Zealand.   But IBM 's delivery is
late.   There seemed to be notebook computers on every
second stand at PC   '91  .  
    IBM was also showing off its RS/6000 reduced set
instruction computing workstation - on the same stand and
sitting right alongside a workstation from Sun
Microsystems.   Workstations?   Sun?   Risc?   Who
would have dreamed of such things 10 years ago?  
    In the tumultuous decade since the arrival of IBM 's
PC, the world has been turned upside down.   But in August
1991, instead of clarity and understanding about where PC
development is now and where it is heading, there is
only confusion.  
    IBM is there in the thick of it all, hitting the
headlines with its alliance with arch-rival Apple Computers,
and its attempts to maintain a stranglehold on the PC
market through the introduction of new architectures, the
development of new operating systems, and liaisons with just
about anybody     one     you care to name.  
    In actual fact, it was not IBM that really gave birth
to the PC, but 
  photo  
  caption  
it was IBM 's involvement that changed the world - because it
saw and seized an opportunity in the business market.
  Today the waters are a little muddier.  
    The evolution of the PC can be traced to 1965 with the
Beginners All Purpose Symbolic Instruction Code (Basic) - one
of the first simple high-level programming languages for
non-programmers.   Microsoft can trace its roots back to
Basic.  
    In 1966 Robert Noyce and Gordon Fairchild formed
Integrated Electronics, or Intel.   They eventually
created the microprocessor destined to be the brain of the IBM
PC.  
    The first PC was called the Kenbak 1 - developed by
John Blankenbaker of Boston in the United States and
launched in 1971.   It offered 256 characters of memory and
cost US$750 (NZ$1300).   Forty were sold.   Later that
year Intel introduced the 4004 chip, the world 's first
microprocessor.    
    By 1975 things were hotting up.   Two thousand Intel
8080-based PCs, called the Altair, were sold.   Bill Gates
and Paul Allen formed Microsoft as a software writing
partnership.   Their first project was to write a
version of Basic for the Altair.   In December the first
retail PC store - the Byte Shop - opened in Mountain View,
California.  
    On April Fool 's Day in 1976, Steve Jobs and Steve
Wozniak incorporated Apple Computer in dual California
headquarters - a garage and a spare bedroom.   First year
sales of the Apple II in 1977 totalled $2.5 million.  
    Intel 's 8086 and 8088 microprocessors emerged during
the next couple of years, as did Wordstar, the first
high-selling PC word processing program, and Visicalc for the
Apple     11    II     - the first user-friendly
PC-based spreadsheet software.  
    IBM was getting worried.   In July 1980 it made an
unpublicised decision to develop a PC.   The endeavour
was codenamed "Project Chess" and the processor called Acorn.
  Not a moment too soon.   A month later Ashton-Tate was
founded and launched Dbase II, the first high-selling
database management software for PCs.  
    April 1981 saw Adam Osborne 's Computer Corporation
introduce the Osborne 1, the first portable computer at 27
pounds.   The machine was bundled with Wordstar and
Supercalc - a precursor of things to come.  
    But in August of that year came the key contribution
from IBM.   Suddenly the PC won instant credibility with
corporate America.   IBM 's first effort ran Microsoft MS-Dos
1.0, Intel made the 8088 chip, and third parties made the
software, the disk drives and the printers.   But it was
IBM 's involvement that mattered.  
    It was a watershed year.   A few months later
Microsoft and IBM signed their first software agreement.
  Apple went public and made many early employees instant
millionaires.  
    From that moment the PC never looked back.   At the
end of 1982 the total value of software shipped that year was
estimated at $1 billion and Time magazine named the computer
its "Man of the Year".  
    It wasn't long before other companies started to
emulate the IBM model, using Intel chips and the Dos operating
system.   Companies such as Compaq were quick to emerge
offering glossy expensive machines, while elsewhere the
invasion of the clone was beginning.   Low-cost lookalikes
flooded out of Asia in the 1980s, eating into IBM 's market
share.   And nowhere did they appear to be more
successful, on a per head of population basis, than in New
Zealand.  
    Over the decade IBM was not generally first with the
latest.   Its first attempts to enter the home market, and
produce a portable computer and local area network, were
disastrous.  
    Frequently reactive rather than proactive, IBM decided
to tackle the clone problem with the introduction in 1987 of
microchannel architecture and the OS/2 operating system.
  It was greeted less than enthusiastically - demanding
a whole new set of applications.   Problems with
traditional partner Microsoft and the rise of the
workstation initiated by Sun Microsystems have created more
headaches for IBM.  
    This year has been another watershed one.   The
emergence of the Ace consortium was the first oddity - an
agreement between Compaq, Digital, Microsoft, Mips, Santa Cruz
and others to develop new hardware and software
standards.  
    IBM was quick to react.   In an agreement that once
would have seemed bizarre, Apple and IBM agreed on a single
software and hardware platform for their next generation
personal computers and workstations, built round an
advanced version of the RS/6000 risc chip.   Apple will
license its Macintosh software technology to IBM, which will
incorporate it into its workstations and mainframes, and
has set up a company to finish development of Apple 's
object-oriented programming-based Pink operating system
running on Intel processors.  
    What does it all mean?   Nobody really knows - apart
from the two clear conclusions that IBM, as usual, is
attempting to dominate the market, and its rosy
relationship with Microsoft is badly tarnished.   The race
is on for the next generation of personal computers.   The
starters in the first race - Intel, Apple, IBM, and Microsoft
- are still there, jostling for position.   What will
the PC market of 2001 look like?   At this stage, it 's
anyone 's guess.    
        Legal worries put imaging on
backburner    
    NEW ZEALAND companies are highly interested in
electronic imaging but are also worried about the inadequacy
of the law for admitting electronically-stored images as
evidence.  
    These are some of the findings of a survey by Deloitte
Ross Tohmatsu of 73 organisations from an original mailout of
112.  
    Fifty six per cent of those surveyed were information
systems managers who were able to identify only six
imaging vendors active in the New Zealand market
place.  
    A weighty 63.6 per cent of the sample raised legal
problems, with the acceptability of imaging systems
evidence a big, while 9.1 per cent were not bothered about
this issue.  
    While 65 per cent of those sampled were either
planning investigations or thinking about electronic imaging,
27.4 per cent were putting the issue off till next year while
6.8 per cent did not see imaging as relevant to their
organisations.  
    The main reasons managers cited for adopting
electronic imaging were better customer service (38 per cent)
and improved access to customer files (20 per cent).  
    The biggest impediment to adopting imaging were seen
to be: high cost (36 per cent) and other higher priority
projects (30 per cent).  
    More than 45 per cent of those
    survey    surveyed     had budgeted
more than $10,000 for investigating imaging next year.  
    Consultant Michael Konnoth says the survey 's high
response rate indicates a lot of interest for imaging.  
    He says while many of the objections to imaging
projects are other higher priority projects this will not
always be the case and he says companies buying personal
computer equipment to support new standards such as
Windows 3.0 will find they are already installing the
capability to handle imaging.  
    He says gradually the perceived high price of imaging
will not reflect the true cost of technology as equipment such
as optical jukeboxes become desktop items.  
    But Mr Konnoth declined to guess how many installed
imaging sites there would be within a year.  
    Mr Konnoth and Norm Hosken, a consultant to Deloitte,
say there will be a considerable amount of work for
consultants from imaging - particularly in workflow
planning.  
    Mr Hosken says workflow planning goes to the heart of
any business and vendors are keen to see consultants do this
work while they concentrate on installation
issues.    
        Wang looks forward to more Mips
sales    
    WANG strategic relations manager John Hawkins says the
company expects to sell 90 Mips computer systems this year
and is looking forward to the emergence of the Advanced
Computing Environment systems for further sales next year.  
    The Ace is a consortium of more than 60 companies
initiated by Mips, Compaq, Digital, Microsoft and the
Santa Cruz Operation to replace the Intel-based personal
computers of the 1980s with a reduced instruction set-based
architecture designed by Mips in the 1990s.  
      Mips'   Australia managing director Rob Byrne
says the environment will offer both Sco Ace/Unix and
Microsoft 's NT operating system on either Mips risc or
Intel 8086 family processors.  
    The Ace/Unix will be a combination of Digital 's
Ultrix, Open Software Foundation 's Motif, Sco 's Unix,
    Mip 's      Mips'       risc/OS and AT &amp;
T 's system V.4.   Both operating systems and processors
will provide backward compatibility to MS-Dos applications
through compatible software or emulations putting up to 45,000
applications at the disposal of the users.  
    Mr Byrne says the new systems - based on the Mips
R4000 processor to be launched next year - range from the
middle of the personal computer price/performance curve -
including laptops - into larger server systems.  
    He says the 64-bit R6000 will easily outperform the
32-bit Intel architecture system and competition in
fabrication between companies including Samsung, Sony and
Seimens will drive down the price.  
    The R4000 chip is designed to be flexible with support
for symmetric multiprocessing and fault tolerance built
in.  
      "  From 1986 to 1987 Mips was
effectively getting its own act together.   From 1987
till April when Ace was announced we were fighting the chip
war with the other risc chips.   Now we feel the chip war
has been won and we have to win the systems war.     For
that we 'll need great partners,  " Mr Byrne says.  
    However, he says Mips will always be a low-profile
company, concentrating on design work and also providing
chips and systems through resellers such as Wang.  
    Mr Hawkins says in the United States, Wang has
rebadged Mips product.   It has decided against this
option in New Zealand because of the credibility the Mips name
carries in a market that shows more interest in open systems
than the United States.  
    Mr Byrne says partners with high market share in New
Zealand such as Wang are attractive sales and support agents
though this is shared with Eagle Technology via Prime.  
    Mr     Byrnes    Byrne     says he does
not expect much to come from the Apple-IBM alliance if it
is signed later this year and he says any products that do
emerge will not begin to enter the market till 1993.  
    Mr Hawkins says     IBM    IBM 's    
alliance with Wang is to use IBM 's risc technology for its
  "open office"   imaging and
office automation development, not for
  "commercial"   or database
engine systems.  
    He says Wang 's original VS family will be
progressively   "opened"   with
non-proprietary technology such as memory and will attract
customers where specific applications are required and
  "openness"   is not so highly
prized.   Examples include the financial and legal
markets.  
    Mr     Byrnes    Byrne     says Mips enjoys
an advantage over other risc
    systems    system     manufacturers in
that it has never targeted the technical workstations market
and therefore concentrates on the
  "commercial"   market.
  He says while some companies offer low-cost risc they
cannot match Mips on service support and
performance.    
        Rising Sun comfortable with workstation
trends    
    SUN Microsystems, buoyed by strong regional and
international financial results, is unperturbed by the
prospect of the workstation becoming a commodity item.  
    Regional marketing director Dr Tony West says low
prices will stimulate the market and allow Sun to continue
dramatic growth in workstation shipments.  
    He says Sun is a cash-rich company with US$834 million
(NZ$1.45 billion) in hand and, unlike some competitors, does
not have financial problems.   Revenues for Australia and
New Zealand were up 61.5 per cent during the past last year to
A$96 million (NZ$131 million), with New Zealand accounting for
$25 million.  
    Resellers sold $45 million worth of products last year
compared with $15 million the previous year.   Compounded
growth in the five years since Sun began operations in
Australia and New Zealand is more than 100 per cent.  
    According to IDC Research figures, Sun has doubled its
Australian Unix market share to 40 per cent.   Unix
workstation market share has increased from 38.4 to 53.2 per
cent and unit growth rose from 1700 to more than 4000.
  Staffing levels have increased 57 per cent to 172,
including 25 in New Zealand.  
    The parent company 's revenues shot up 31 per cent
to US$3.22 billion and net income increased more than 70 per
cent to $190.3 million.   Fourth-quarter net income and
revenues increased 35 per cent on the previous year.  
    The 2 1/2-year-old New Zealand subsidiary, under
managing director Don Sykes, still reports into Australia, and
Dr West says a change is unlikely because of the support
Australia can give.  
    Commenting on the IBM-Apple alliance, Dr West says it
will be the death-knell of the Motorola 88K and the Intel
i860 reduced instruction set computing chips.
      "Mips and Sparc are the only games in
town."      
    The Ace consortium is   "too little, too late.
    They 're not decisive enough about stating what
  photo  
  caption   
they 're going to do  "  .   There is still confusion
about the future of OS/2 and the IBM-Microsoft relationships
and   "the more confusion there is, the more we 'll forge
ahead"  , Dr West says.  
    Sun expects to retain a dominant position in the
Sparc market through its low manufacturing costs and
value-added components.   Two out of three risc
processors shipped last year were Sparc.  
    Dr West says Sun expects to ship 200,000 units this
year and he believes there will be a volume explosion during
the next two or three years.  
    Only the IBM-Apple alliance stands a chance of
catching Sun, he says, dismissing other competitors for being
overpriced or poorly marketed.   Key sales factors will
be the low-cost and ease of use - where Dr West admits there
is still work to be done - value-added applications, and low
manufacturing costs.   The Sun hardware has become far
less complex and the move to Unix V.4 later this year should
help sales.  
    Multiprocessor capacity is not far off, enhancing the
efficient I/O throughput architecture.  
    Superscalar Sparc chips of around 80-100 million
instructions a second are arriving soon from Texas
Instruments and LSI Logic.  
    New colour low-end machines, replacing the IPC and
SLC, are now available in New Zealand.    
          Crystal driver a winner for Lower Hutt
maker    
  photo    caption  
    LOWER Hutt company Solid State Equipment has won
orders to supply NASA, Atomic Energy of Canada and Texas
University with a device evocatively described as a
"crystal driver".  
    The high-technology tool is used to determine the
viscosity of liquids (including metallic glasses) when
they undergo chemical or temperature changes.  
    Costing US$20,000 (NZ$35,000), the device is used in
conjunction with a personal computer for programmed
experiments over periods of time.  
    Originally conceived by the Department of Scientific
and Industrial Research 's physical engineering laboratory
in the 1970s, the machine won the affection of visiting United
States scientist Steve Carpenter.   He was impressed with
the system 's ingenuity and cheap price.  
    Redesigned by Solid State co-managing director
Neil Poletti, the early versions featured dials and knobs
requiring the device to be constantly monitored during
an experiment.  
    However, the latest version of the machine is now
based around a Motorola MC68H11 microprocessor
programmed in the Forth Language and is controlled by a PC
application programme written in C++.  
    Mr Poletti says the machine constitutes something
of a niche market in that nobody else in the world makes it
and so far sales have been through word of mouth in the
scientific community.  
    The potential market, while inherently limited, is
not necessarily small and could include every university and
relevant research institution in the world.   A smaller and
cheaper crystal driver is planned for industrial
scientists.      
  

          THE CLYDE RESERVOIR      
    The worrisome aspect of the Clyde dam reservoir is the
fact that there had been so little seismic activity in the
plate block area in recent times.   This had led project
engineers and Electricorp to continue to describe the River
Channel fault as   "inactive."     This lack of
activity might mean that any "stored energy" that is there had
not been relieved naturally for some time.   Whereas Benmore
was not built directly on top of an active fault, the Clyde dam
was.   Any large increase in seismicity might increase the
dangers of dam failure a hundred-fold or more because of
this.  
    Being some 64 metres high, the Clyde dam would hold
back a considerable amount of water - up to three times the
amount held in Wellington harbour.  
    Due to the extensive area of the lake catchment a large
quantity of silt and bedload might reduce the life of the
reservoir and dam to under 60 years, which was the average life
of gravity dams.   The power station would then have to
operate as a "run of river" station due to the loss of storage.
  This was one of the factors looked at when the water rights
were being considered.   That was, would it be the best use
for the water?   The Roxburgh dam was moving swiftly towards
being a "run of river" station due to serious silting in the
gorge and would operate only so long as the intakes were clear
of this silt and debris.  
    One wonders if the bottom sluice (used to take out silt
and floodwaters) in the Clyde dam would block up as quickly as
Roxburgh 's did after only 15 years in operation.   The finer
(flocculated) cohesive material had a tendency to form density
currents which would move generally through the reservoir
towards the dam.   Larger rock particles formed deltas at the
head of the reservoir or in arms around the lake edges.  
  cartoon  
    Any thoughts Electricorp engineers might have of
dredging out the reservoir behind the Roxburgh dam might well
be futile because the very fine silt was some 6 - 8 metres deep
there and would simply flow from upstream to replace that which
was excavated near the dam.   The silt was so fine that, near
the shores, it was like walking in cotton wool.   There was
also the problem of where to put it, if it was successfully
excavated.  
    One of the problems with dams of any type was not so
much with the finer, or even coarse material entering turbines
and causing damage but the air bubbles that went in with it,
affecting water densities and pressure down the draught tubes.
  This could be an indirect cause of "water-hammer," as
anyone with faulty plumbing in their house would know.   On
the scale that a dam experienced it, despite the operation of
surge tanks, it could be very damaging and would shorten the
effective life of the dam.  
    There was a large amount of the finer material in the
sides of the Clyde reservoir because it had been subjected to a
grinding movement due to landslides and glacier action over a
period of many thousands of years.   Also, the schist, when
ground down, was much finer than the sand that made up
sandstone, greywacke or most other types of rock.   Due to a
fairly dry climate now and earlier afforestation and also to a
relative lack of wind, much of this finer material remained in
place.  
    Around the lake sides and bottom there was a fear of
water percolating down through the crushed schist to
tectonically stressed rock at depth following the filling of
the reservoir.   However, despite inadequate data, the risk
of this happening to any devastating effect might not be
large.  
    But since the dam itself was positioned right over the
faults the effects could be much more devastating because it
simply wasn't known what stresses had built up in the ground
and fractured rock, especially further down in that fault.
  These stresses could be suddenly released if a large
earthquake occurred in the region of the reservoir.   Quite
often, the fact that an active fault hadn't moved could make it
more hazardous than if it just recently had.  
    For this reason, the Rodgers Creek Fault just north
west of Oakland, San Francisco was feared as a site where
violent movement might take place - about a 90 percent chance
within the next 30 years.   This probability had been
determined from some 300 seismometers placed along the fault.
  As mentioned  earlier, there were no seismometers in the
Clyde/Cromwell area.    
        DAM SAFETY      
    Geophysicist Dr W. Smith had said that occasionally
deep earthquakes have been known to cause damage in New
Zealand, but in estimating earthquake risk the main attention
was drawn to the shallow activity (such as near Clyde).   In
its submission to the Wheao canal Committee of Enquiry in 1983,
(the canal collapsed soon after it became operational) the
Geological Society of New Zealand listed as its most important
recommendation the fact that planning teams (for proposed dams,
canals etc) include an engineering geologist from the very
onset of developing the project.   This was not done at
Clyde.  
    Some "lay people" and residents pointed out some of the
potential effects of the Clyde Scheme F. project on the terrain
and the potential effects of the countryside on the dam, but
were ignored because they were not expert engineers.   There
was only one thing worse than not taking due note of what
concerned non-experts had to say and that was having too few
experts of your own and ignoring them anyway or relying on
not-so-expert experts such as had been employed on the Clyde
dam scheme over the years - and still are.  
    When Gerald Lensen spoke about the insecurity of the
site while still employed by the DSIR, he said that the DSIR
wouldn't guarantee the site for a high dam and that the
Government would have to accept full responsibility.  
    The NZ Geological Society noted that the geological
complexity of New Zealand created a high potential for
difficulties with ground engineering works and it was concerned
that major capital works were being undertaken without any
requirement for adequate geological advice.   This was
certainly the case with Clyde.  
    Dam failure occurs to one dam for every 100
constructed.   About 300-400 dams are completed each year
throughout the world.  
    Dams were much more vulnerable to failure immediately
after or even during construction than after they had been
operating for a time.   The most frequent failures occurred
within seven years after construction.   After the useful
life of the dam had ended (averaging 60 years or more of
operation) the risk of failure 
  cartoon  
increased again dramatically.   Forty of the world 's 15,000
dams in existence (outside China) failed in the period from
1945 to 1980.  
    One of the Waipori dams showed a crack during an earth
slide during a flood in 1984&semi; displaying "disaster by degrees,"
not from a particular earthquake although the area around
Waipori was faulted from the Maungatuas through to Waitati.
  Worldwide, the safety record put Waipori (an arch dam) at a
lesser risk than a gravity dam such as at Clyde.   Lower
gravity dams were at greater risk than higher ones, mainly
because they were more susceptible to "tumbling  "  .  
    One of the risks of any sort of gravity dam was from
water percolating through at the sides or underneath.   Or,
in the case of the Clyde dam, water possibly entering
faulted rock near the mobile joint.   If there was
insufficient water pressure to keep the mobile joint closed
if the reservoir was only half filled, it might be like your
bathroom tap, left dribbling - sooner or later the water would
work its way round the washer and the tap would never turn
completely off.  
    In 1987 problems with movement of the Clyde dam
powerhouse construction had been encountered.   This was
because of settlement due to the large weight of concrete
poured for the foundations and the movement of rock occurred
with the excavation of the adjacent diversion channel.   With
grouting, the sections of the powerhouse were secured to the
rock beneath.   In unstable ground like this the alignment of
turbines could be badly affected by movement within the rock if
such movement continued after construction or was aggravated by
the earthquake swarms that would accompany the filling of the
reservoir.   Already the Ohau A powerhouse and nearby Pukaki
had been affected by Plate Block tilting and this could be a
warning of larger earthquakes to come there.  
    Already, even though there was no water yet behind the
dam, serious cracking has begun in the power  house and
dam itself.   At the 136 level (metres above sea level)
painters who were painting the ceilings and floor complained
that the paint wasn't drying and some of it was peeling.
  They had to get the workforce to drain the drainage tunnels
(the powerhouse goes far below the tail water level).   There
was a real fear that the enormous pressure of water when or if
the dam was filled, would make this seepage considerably
worse.  
    There were cracks opening up in the lower generator
floor near the No 1 and 3 generators.   These had been filled
with expandable epoxy cement after the edges of the cracks had
been "cosmetically" straightened.   These gaps were about
20mm in places and represented the divisions of the highly
reinforced blocks of the powerhouse as it sat on the schist
"pad" partly over the main river fault and directly on "fault
21." There was movement there originally when the foundations
were laid (see earlier).  
    There were cracks in the walls of the drainage
galleries (lower generator floor, No.3) and the drainage
gallery which drained the draught tubes area.   One was a
particularly bad crack several millimetres wide and with
considerable amount of calcification forming around it.   It
was being monitored carefully as were most of these recent
cracks.  
    One of the problems encountered was the cracking of the
electrical conduits running throughout the structure, through
the concrete.   Water had permeated into these pipes and
caused electrical shorts and malfunctions.   It 's presence
showed that the cracks forming in the blocks might be serious.
  It was a problem which is very rare in concrete dam
constructions - particularly one, like the Clyde dam, which
wasn't even filled and operating!  
    There would be a problem of alignment for the
generators, draught tubes and penstocks if these cracks widened
even without the additional heavy pressure of water from a
completely filled reservoir.   One shudders to think what
might happen when water from a filled reservoir came thundering
down the spillways.   It might be like jumping up and down on
a jelly!  
    There was a very serious and recent (June 1990)
compression crack to the right of the penstocks and on the
generator pad near the stilling basin (below the spillways).
  It was near the shaft built early on to investigate the
shears beneath the spillway blocks.   Concrete was cracking
and breaking away as the two huge blocks were forced towards
each other.   This crack had to be seen in context of the
crest to toe cracks on   the   reservoir side of the dam
(mentioned elsewhere), to the left (facing it) of the mobile
joint.  
    It appeared that the entire block or blocks in between
these two major cracking episodes might be sinking and tilting
left towards the river channel fault hole.   It had to be
remembered that the amount of concrete "pulling" the structure
down at that point was over 300,000 cubic metres.   Again, to
the extent that this problem might be aggravated after the dam
was filled could only be conjectured but was bound to cause
critical problems for leakage generally and possible calamity
even if carefully monitoring and evasive engineering steps were
taken.  
    The compression crack was right on the junction of the
River Channel Fault and the Road 6 fault which branched off 30
degrees to the right in an upstream direction.   The upstream
rock section had been inching downstream long before the dam
was even mooted.   This section was directly upstream of the
mobile joint and included the fault hole itself.  
  cartoon  
    Although clever engineers had devised a compressed rock
"blanket" and grout curtain on the upstream side of the dam
right up to the mobile joint to keep water from seeping into
the concrete-plugged fault hole they relied on an inflexible
concrete pad under the stilling basin on the other side.   It
appeared that water may have seeped down into the fault hole
through cracks in or around it when water was released into the
stilling basin.   The fault material (pug) may have been
re-watered since the river channel was diverted and the hole
dug out and re-filled with concrete in 1984/5.  
  cartoon  
    If the dam is commissioned water would soon be
pressuring all sections of the structure and what this would do
to the existing, recent serious cracks hardly bears thinking
about.   As Gerald Lensen said about the problem with the
slides in the gorge,   "they 're just a smokescreen - the
dam is the real problem."    
    There was a problem being monitored (June 1990) with
pump correction - to keep the level of the stilling basin low
in case it backed up against the concrete plug in the mobile
joint and pushed it out through the other side.  
    There were various other design and engineering bungles
including the one concerning the roof of the powerhouse.   It
was covered with aluminium roofing but unfortunately, due to
the extremes of temperature in Central Otago, began to shear
and open up at the bolt holes.   This let in water to the
powerhouse below causing damage to fixtures and equipment.
  The aluminium was dismantled and steel roofing put in place
instead.   This caused problems because the steel beams and
struts weren't strong enough to carry the load.   Thus
another correction to a correction was made.  
    Because of the design changes, the dam had been moving
towards the powerhouse very slightly since it was completed and
this problem was largely countered in 1984 and 1985 by using
huge jacks at the bottom blocks of the dam.   That stopped
the movement apart from the upper blocks which continued to
move for a time.   No further trouble was anticipated with
this problem.   This was a different problem to the
compression cracks near the stilling basin mentioned
earlier.  
    A near disaster was nearly caused when water began
flooding through the lower coffer dam - nearly leading to its
collapse, loss of life and millions of dollars of damage.
  It was caused by project engineers directing a dredging
contractor to remove material from the river bottom close to
the bottom of the lower coffer dam.   The resultant flood
swamped the pumps and only through strenuous and desperate work
with the heaviest equipment on the site - blocking the flow of
the river, was a calamity averted.  
    Another calamity was averted when the engineer, who had
been trying to straighten the drop bars on the gate on the
river side of the diversion sluice let water     in
to    into     the sluice.   Unfortunately, he didn't
check to see if workers were in there (there were only two -
the others had left earlier) working in a special bulkhead,
cleaning out and repairing the tunnel which had suffered damage
and scouring over a period of time because of the delay in
filling the reservoir.   The workers escaped somehow after
water threatened to rise quickly over their heads.  
    Another example which did not involve safety but
resulted in sheer waste was the $380,000 refit of the Koe-Ring
digger and excavator especially to take out the coffer dam
after the completion of the diversion sluices.   It was never
used for that purpose anyway and was used only for one hour for
other purposes before being abandoned.   It reminded one of
the days of the old National Party 1950 's and 1960 's when the
farmers   "had it so good"   that you would hear of
a farmer buying an expensive post hole digger, digging half a
dozen holes with it then parking it to rust away at the corner
of a paddock.  
    Another equipment extravagance was the concrete shed
erected near the Electricorp offices near the dam, costing
several hundreds of thousands of dollars, especially to house a
small pickup truck and a lawnmower.   The plush Electricorp
offices on the site had undergone a continual metamorphosis as
planners couldn't make up their minds what to use them for&semi; a
South Island HQ, mere site office or whatever.  
    Safety problems in dams or other large works are
generally met with unquestionable reliance on the
  builders    "    '       abilities to
engineer workable solutions, says Gerald Lensen.   An
alternative description we suggest is "Bureaurocracy Under
Momentum" ('BUM') - treating all present or future problems as
ultimately solvable in order to ensure a continuity of
employment and spending in the area and face-saving for
engineers and decision-making politicians.   Construction
problems in the past had always been dealt with, having a
virtually unlimited use of public money, by "MESS" (Ministry
Engineering Silly Solutions).  
  cartoon  
      As each new method of dealing with disasters on
site was announced the public sighed and said, ah, they must
know what they are doing.   However, looking back over the
years at the activities of these "sticking-plaster" engineers,
especially in relation to the Clyde dam, it left one with the
sinking feeling that indeed if they didn't seem to know what
they were doing, they were going to let as few people know
about that as possible and treat all objections as ill-informed
and emotional.  
    Even the possible commissioning of the dam posed
several problems.   Normally, planning, design construction
and commissioning was undertaken in a fairly close sequence
(such as at, for instance, the Manapouri project).   The idea
was to install the generators, draught tubes and penstocks (and
all other equipment) and while the installation engineers were
there on the spot - fill the reservoir and make whatever
construction and engineering adjustments that were required so
that when the first water came into the generators, all was
well - there would be no mal-alignments (even one millimetre
out is serious) and there would be no chance of equipment
failure or any blow-ups or blow-outs.  
    Unfortunately in the case of Clyde the manufacturer 's
installing engineers were now spread all around the world on
other jobs and would have to be tracked down and returned at a
time convenient to them and certainly, at very great cost!
  Commissioning electricity generators is a very specialised
job and not just any engineer can do it!  
    Early in 1990 a campaign by the State Owned Enterprises
was launched advocating a greater freedom for them from public
scrutiny - especially by means of the Official Information Act.
  It was   "intrusive"   they said.
  Submissions from Government agencies urged, instead, for a
much tougher scrutiny - especially to overcome the
  "commercial sensitivity"   clauses which were
used so often to block requests for information as to what the
S.O.E 's were doing.   It is hoped that requests for
information in the future on the safety of the Clyde dam
through the Official Information Act would   be   granted
without any resistance, even to "lay people."  
    Rather unforgettable, as a "disaster on site," was the
huge reinforced concrete canopy Ministry engineers designed and
built to fit over the eastern entrance to the Homer tunnel at
Milford to deal with the frequent avalanches of snow and rubble
that continually blocked the road.   One day in 1951 a huge
avalanche of snow swept it away so completely engineers could
not even find traces of its existence.  
    In 1981 the Ruahihi canal in the North Island was swept
away - said to be caused by an engineering fault.   In 1983
the Wheao canal was devastated by a torrent of water, silt and
boulders that - after breaching a canal wall, poured down the
hillside, destroying the power station and other buildings.
  The project had been declared officially open the day
before by the then Prime Minister, Robert Muldoon with Bill
Birch - both prime movers of the "think big" strategy of the
former National Government which "masterminded" the hasty
choice of site and construction (without proper investigation)
of the Clyde dam in the vain hope that some big user from
overseas would come along and buy the power generated from
it.  
    The "mega-dam" is well on the way out these days.
  All but seven of the world 's hundred largest dams were
built since World War Two.   Dams, such as the Aswan dam in
Egypt and the Ataturk dam which was nearing completion in
Turkey was, or would be, environmentally destructive.   Also,
because of their size (like the Yacyreta dam in Argentina) they
mounted huge problems with cost over-runs&semi; the Yacyreta dam
costing $US6 billion - some three times the amount budgeted.
  The World Bank now looks very closely at large dam projects
for these sorts of cost problems and also for the environmental
damage they cause.  
    One of the worst cases of inexcusable ecological damage
was the construction of the Balbina dam in Brazil in 1988 -
flooding huge tracts of virgin forest&semi; over 2,000 square
kilometres!   The World Bank 's project to dam the Narmada
river in India is being fiercely resisted by the 100,000 people
it will displace.   They are succeeding.   The other dam on
the river&semi; the Sardar Sarovar was commenced in 1987 and will
displace 67,000 people.   Local studies of both projects
declare them to be "dodgy," economically and likely to grind to
a halt through lack of finance, leaving behind vast
environmental damage.  
    Periodic reviews of the safety of dams occur from time
to time.   One of the more successful dams in NZ 's history
was the Waitaki dam, designed and built in the 1920 's.
  Alterations proposed include enlarging the ability of the
dam to take away flood waters (a 25 percent increase).   The
significance of uplift pressures underneath the dam are
beginning to be realised and are now being
monitored.      
  

      Augmenting the eye    
      EVER SINCE 1609  , when Galileo pointed his
rickety "spy glass" at the heavens and made discoveries that turned
the scientific world upside down, people have been star-gazing with
the help of ever bigger and better instruments.  
    To see more, the most obvious solution is to increase the
size of the telescope.   By doing so, we collect more light
(each doubling of the diameter produces a fourfold increase in
the amount of light collected), and stars that were previously
below the threshold of visibility can now be seen.   These may be
inherently faint objects quite close to us, such as asteroids or
the so-called "brown dwarf" stars, or they may be very bright
objects out on the edge of the observable universe - the
quasars.  
    As the size of telescopes is increased, so is their
resolution - their ability to distinguish fine detail or to see as
separate two objects close together.   By eye, we can just
separate two 
  photo  
  caption  
objects one arc minute - (1/60') apart, whereas the giant Hale
telescope in Mt Palomar, with its 5m (200-inch) diameter mirror,
reduces this to almost one arc second - a sixtyfold
improvement.  
    Recently completed or planned are the "new technology
telescopes", their mirrors made up of between 10 and 20
computer-controlled segments which can be independently
oriented to keep the mirror true to shape, whatever its angle of
tilt.   At the European Southern Observatory in Chile an array of
four 8m telescopes is planned.   These will work in unison, with
the light-collecting ability of a single 16m diameter instrument,
or be able to function as four independent telescopes when
needed.  
    But size is not the only consideration.   Getting a
better view also means getting a clearer view, and that means
reducing the depth of atmosphere we have to look through.   The
components of the atmosphere, particularly water vapour, cause
the atmosphere to act as a filter, and wavelengths like the
ultra-
  diagram  
  caption  
violet and infra-red are almost completely blocked.   Worse is
the fact that the atmosphere is not steady or uniform in
composition, so that light from the stars is being refracted first
one way and then another.   This is why stars appear to twinkle,
their images dancing back and forth as if seen through the surface
of a swimming pool.  
    The best viewing sites are as high as possible, with clean,
steady air and wind streams.   The greatest collection of major
telescopes on earth is at 4150m on the summit of Mauna Kea,
Hawaii, well above sea fogs and low-level cloud, and in
relatively non-turbulent oceanic air.  
    Clearly, the ultimate viewing conditions are to
   box  
 be found in space, and the solution is to lift the telescope into
orbit.   This was accomplished with the successful launching of
the Hubble Space Telescope from the space shuttle Discovery in
April, and now we have a medium-size telescope in the ideal optical
environment of space.  
    Unfortunately, a design flaw in the Hubble 's primary mirror
(discovered after the launch) means that the telescope 's
observations in the visible part of the spectrum will be
severely affected.   If working properly, Hubble would have
been able to see objects 10 times smaller than the largest
telescope on earth could.   For example, while giant
telescopes like the Hale 200-inch can resolve to roughly the
stellar equivalent of reading the headlines of a newspaper at 1km,
the Hubble is theoretically capable of reading the fine print as
well.  
    Now such improvements in resolution will have to wait until
the mirror system can be repaired and that may mean bringing
Hubble back to earth.  
    No telescope can "see" anything without a light detector of
some sort.   Originally this was the astronomer peering through
the eyepiece, but in the Hubble he is replaced by the Charged
Coupled Device, the "retina" of the video camera.   By
converting light into electrical signals which are then
transmitted to the astronomers on earth, the CCD has made
observing from space possible.   Like photographic emulsion, the
CCD can be given long exposures to faint objects, thus building up
a detectable image.   So, in time, the Hubble will let us see
more and fainter detail than ever before.   We will also be able
to see at all wavelengths of the ultra-violet, which are filtered
out by water vapour at ground level.  
    From nineteenth century   astronomers'   pencil
drawings, we have moved through monochromatic and three colour
photographic reproductions into the garish world of
colour-coded CCD images with their arbitrary fluorescent blues,
lurid greens and searing magentas.   One result of these
developments is that the art of peering at the night sky
through a telescope appears to have fallen into disrepute - an
activity more suited to Boy Scouts in pursuit of badges.  
    But no one who has spent time looking through even a modest
telescope in a light-polluted urban sky would agree with this
attitude.   The mid-winter view of the Sagittarius star cloud is
addictive.   Once seen, its reappearance is impatiently awaited
each autumn.   This is also true of the Orion nebula in
summer.  
    Eighty years ago astronomy, like sketching, was one of
the polite accomplishments, and modest refractors were in
popular demand.   During the 1920s, amateur telescope-making
became the core of many astronomical societies, and the
manufacturing of parabolic mirrors one of their major functions.
  By the late 1950s, the availability of the commercially
produced Schmidt-Cassegrain telescopes seduced many amateurs
away from the work bench.   However, a handful of dedicated
amateur telescope-makers have kept the craft alive and abreast of
developments.  
    Such people belong to one of two schools: there are the
craftsmen who produce instruments of a quality, ingenuity and
performance that would put them in the five-figure bracket if
priced commercially&semi; on the other hand, the photon-grabbers
place a premium on seeing the stars and cheerfully accept that
their telescopes may look like something left over in a builder 's
yard.   But these assemblages of second-hand plywood,
  plumbers'   fittings and scroungings from car wreckers work,
and with care will continue to do so for years at a price
affordable by anyone.    
        DNA research opens door for disease
identification    
  photo    caption  
      D  R PETER GEORGE and his medical colleagues
are hearing the drum-beat of scientific revolution in their
research laboratory at Christchurch Hospital.  
    The revolutionaries may be infinitely small, but their
invisibility to the naked eye is no indication of their central
role in shaping life itself.  
    Every cell in the human body, except red blood cells,
contains identical genetic material in the shape of 23 pairs of
chromosomes.  
    Each chromosome is a long strand of deoxyribonucleic acid
(DNA) composed of two long, twisted ropes, each containing a
sequence of four chemicals - guanine, cytosine, thymine and
adenine.  
    The exact order of this chemical quartet determines an
individual 's genetic make-up.   It is a tune which can be
played in seemingly endless variations.  
    DNA is found in the white blood cells, semen, bone marrow,
dental pulp and hair roots.  
    The significance of DNA in genetics emerged in 1953
when two scientists at Cambridge University, James Watson and
Rosalind Franklin, identified DNA 's spiral-like structure 82 years
after DNA was first discovered in trout sperm taken from fish
caught in the River Rhine.  
      "I see an extraordinary potential for human
betterment ahead of us,"   Watson said later.
      "We have at our disposal the ultimate tool for
understanding ourselves."      
    For more than three decades, DNA has continued to mesmerise
the scientific world.   Its central role in shaping life and
importance in transmitting both physical characteristics and
genetic disorders, have opened new windows and transformed our
understanding of biology.   DNA will continue to play a major
part in medicine, law, and ethical issues.  
    For Peter George and a small research team at Christchurch
Hospital, research includes studies into the use of DNA in
identifying genetically determined diseases such as cystic
fibrosis, thrombosis and muscular dystrophy.  
    The revolution dawned two years ago when a new technique -
the polymerase chain reaction - opened up opportunities for fast,
inexpensive and quick identification of individual cases.  
    Using the new diagnostic method, researchers can analyse
individual DNA structures from a single cell, reading its unique
codes to decipher abnormalities with precise sensitivity.  
    The technique allows for portions of any human gene to be
amplified using enzymes.   Its sensitivity allows tissue as
small as a single hair root to be used.  
      "During the past 10 to 15 years, we have moved from
the stage of examining these questions from the angle of proteins
to studies involving DNA,"   says Dr George.  
      "  Until 1988, this was fairly difficult.
  The polymerase chain reaction has now made the difficult
easy and has revolutionised applications in medical research
and testing.  
    "    We can now work on cases which take days to
complete instead of weeks or even months.   We can work for
considerably less expense.   Previously, this work was so
expensive that you simply didn't do it.  
    "    The new technique promises such a revolution in
laboratory practice that every clinician should attempt to
understand its implications for medicine."      
    Previous methods required sophisticated laboratory
facilities often unavailable in New Zealand hospitals.  
    They were also time-consuming, taking up to two or three
weeks to complete, and were expensive, costing up to $2000 to
produce a result for an individual case or family.  
    The polymerase chain reaction allows results to be obtained
in 48 hours and costs less than $100 for an individual test.  
      "When the gene responsible for any particular
disease has been identified and sequenced, it is a simple job to
analyse its pathological mutations,"   Peter George
says.  
    Cystic fibrosis has attracted increasing attention
since the gene, which causes the condition, was isolated.   The
situation leading to a genetic disease involves the gene dictating
the way in which a protein should be made.  
    But sometimes an error occurs.   The genetic morse code
misses a dot or a dash and cannot be interpreted correctly.  
    The altered message has also changed the lives of between
250 to 300 cystic fibrosis cases in New Zealand, which cause
retardation and chronic lung and digestive disorders.   The
disease is genetically transferred from one generation to the
next.  
    The polymerase chain reaction now allows researchers to
detect the disease in families with a history of cystic
fibrosis.  
      "It is an enormously powerful tool to have at our
disposal, one which can help families immensely,"   Dr
George says.  
    The work at Christchurch Hospital is one link in a long
chain of international research, each link exploring DNA 's role in
genetics.  
    "    DNA is the stuff of life - its basic
structure.   But it cannot work by itself.     It depends on
other factors,"   Peter George says.  
      "  Conceptually, it contains all the vital
information, transmitting a code which contains information on
how many, how to  'make' an individual.   It tells us why
individuals differ, why one person is male and the other female.  
      "  Obviously, the code has to work in context with
other factors, but it remains the stuff of life in the sense
that it contains the message on how characteristics are transmitted
from one generation to another."      
    Forensic scientists are also continuing to examine the
uses of DNA finger-printing.   But this potentially powerful tool
is also highly controversial, increasingly surrounded by an
international legal and medical debate about its uses.  
    In Auckland recently the
    finger-orinting    finger-printing     technique
was used during a court case, a move strongly criticised by the
defendant 's counsel.  
      "There have been other cases in the United States
which have been thrown out,"   Peter George says.  
        "The problem with genetic fingerprinting is
that it is a tremendously powerful technique in theory, but in
practice it would be difficult to apply at a regular standard for
use as evidence in a court hearing."  
    "In the United States the standard to which it
is applied has not always been sufficiently high for admission as
evidence.  
    "    It is a powerful technique, one with applications
within the medical environment to decide questions of
paternity."      
    According to a recent report by the Scottish Crown Law
Office, DNA profiling offers the opportunity for positive
identification to be made not only to a point beyond reasonable
doubt, but to one of scientific certainty.  
      "Despite concerns expressed about DNA following
recent criticisms of the techniques used by laboratories in the
United States, there is no reason to doubt that DNA profiling can,
in appropriate cases, provide conclusive evidence to link a
suspect to a place, a victim or, in civil cases, to establish a
familial relationship,"   the report to this year 's
Commonwealth Law Minister 's Conference in Christchurch
said.  
        "The value of such evidence strengthens the
case for ensuring that samples for testing should be readily
obtainable.  
      "  Such concerns as there are about DNA profiling can
be met by imposing stringent quality controls on those carrying
out the tests."        
        Through The Looking Glass    
    "      T  oday 's fascination with
"user interfaces" is an artifact of how we currently
operate computers - with screens, keyboards, and pointing devices,
just as job control languages grew from punched card batch
systems.     Near term technological developments promise
to replace user interfaces with something very
different,"   writes Autodesk 's John Walker, in his paper
whose title I 've stolen.  
    And at Anzgraph   '89   Autodesk were showing an
amazing movie about an emerging technology.   It 's the
computer game to end them all&semi; cyberspace.  
    "    If video games are movies that involve the
player, cyberspace is an amusement park where anything that can be
imagined and programmed can be experienced.     The richness of
the experience that will be available in cyberspace can barely be
imagined today,"   writes Walker.  
    Virtual reality, virtuality,
    artifical    artificial     reality - these are
some of the other terms that have been used to denote a
computer-simulated world that provides stereoscopic imagery of
three-dimensional objects, senses the user 's head position and
rapidly updates the perceived scene, and offers some means of
interacting with simulated objects.   Once through the
looking glass, the user can move about inside his model and see
objects from this side or that, can reach for and move objects.
  And so far we 're only talking about visual and kinesthetic
feedback.   The more sensory feedback a cyberspace system could
provide - sounds, temperature, smell, wind in the face - the
richer and more "real" would be the simulated world.  
      It 's not a new idea.   Many science fiction
writers have based stories around the notion, and in 1968 Ivan
Sutherland built a helmet with two CRTs attached to the
ceiling and mechanisms to detect head movement and position.
  But in those days the computer power to generate real-time 3D
images   drawing    blurb   just wasn't available.
  Today, with fast CPUs and special purpose graphics hardware not
only available but at decreasing cost, cyberspace is a
practical direction for development.  
    The cyberspace system starring in the Autodesk movie at
Anzgraph   '89   had two small video monitors mounted on a
helmet worn by the user.   Attached to the helmet was a
tracking device like the Polhemus Navigator which tells the
position without attached wires.   Each monitor was
attached to a separate graphics controller which rendered the
view of the three dimensional model of the world from that eyes
viewpoint, updating the display as the head moved.   Then
there was a kind of glove with a navigator attached, with which
the user could point to or grasp objects in cyberspace.  
    The obvious application of such a system is to 3D
engineering and architectural design, but cyberspace is a
general purpose technology which could be used for as yet
unanticipated applications.  
    One application already discovered, though, is
    entertainmet    entertainment    : the NASA Ames
Group, which developed a cyberspace system and demonstrated it to
members of the US Congress, reports that one of the problems in
their demonstrations was that people liked it so much that they
would spend all day in the cyberspace lab if not forcibly
removed!  
    And already there 's a whole rash of other fascinating
applications.   A US company called, of all things, Cyberspace,
has brought out a laptop computer with no screen.   Using a
lightweight eyepiece worn on a headband, the display appears to
float in mid-air about two feet from the user - a crisp, full-sized
image that no-one else can see.   The eye-piece, developed by
Reflection Technologies Inc., uses light-emitting elements, lenses
and a tiny oscillating mirror to make the image appear in front
of the user.   It 's being designed into dozens of products
from a variety of companies.   Then there 's a line of portable
information equipment being developed by Hughes Aircraft Co, and an
electronic book from Selectronics Inc that will allow a
bookshelf of information to be carried in a shirt pocket, or
clipped to a belt.  
    What next?      

  

        SECTION TWO    
        New Zealand      
        INTRODUCTION TO THE NEW ZEALAND
SECTION      
    In this section the aim has been to provide an historical
record of the many and varied models produced by this country 's
manufacturers over the years by illustrating as many as possible
of the total.   Some readers may be surprised to learn how many
different models there were, and they may also be surprised to
learn how many different brandnames there were.   When the very
large numbers of imported sets are included, the total number of
brandnames which appeared on the New Zealand market at one time
or another is quite staggering.   Furthermore, it is surprising
to find how many of these sets have survived to become
present-day collectibles, and the end is not in sight.  
    Since publication of the previous book  *  ,
  *  reference     more N.Z. brandnames
have come to light, so rather than list them as an addendum a
completely new list has been included at the end of this section.
  Many of these names have been provided by eagle-eyed members
of the N.Z. Vintage Radio Society who have taken the trouble to
send them in.  
     A not infrequent complaint voiced by present-day radio
collectors, often in respect to N.Z. radios, is - "Why didn't
they put the model number on the chassis?"   Why, indeed!
  Not that lack of such identification was peculiar to locally
made sets.   The answer, at least in the case of many small
pre-war manufacturers, is simply that there weren't any.
  Model numbers, that is.   But even if there had been, it
wouldn't have done any good because said manufacturer never
issued such a thing as a circuit diagram.   Perhaps it is even
more frustrating to find a set which does have a model number
but, for reasons best known to himself, the manufacturer did not
see fit to mark the number on the chassis.   And to add insult
to injury, some   manufacturers'   service sheets contained
the words - "When ordering spare parts always quote the model
number".   Ho, Hum!  
        AKRAD RADIO CORP. and PYE LTD      
    The brandname originally used on radios made by Akrad was
"Futura", but after 1940 the name "Pacific" was substituted.
  Although the Pacific name had previously been owned by
another company, Pacific Radio Ltd, it was then out of use due
to the demise of the former owner.   Upon recommencement of
radio manufacture after the war, Akrad continued to use the name
Pacific, and in addition introduced the name "Regent".   These
two names continued to be used side by side for the same models
until 1953 when Pye came on the scene.  
    With the launching of Pye radios on the N.Z. market
Akrad 's two earlier brandnames were   photo   discontinued
and the name "Clipper" was introduced, mainly for use on low
priced unfranchised sets, though in some cases Clipper sets were
directly equivalent to certain Pye models.   A third brandname,
used for a short period after 1959, was "Astor".   This name
was of Australian origin and its use by Akrad came about as a
result of the takeover of Radio Corporation of N.Z. by Pye/Akrad
at this time.    
  photo  
        BELL RADIO-TELEVISION CORPORATION      
    Bell commenced business in 1950, taking over from the
earlier Antone Radio Co.   For the next twenty years small
plastic cased radios continued to be churned out.   The number
of different models which used the same cabinet were the 5-valve
"Colt", the 3-valve "Champ", the 3-valve "Cadet" and a 4-valve
version of the Colt using a solid-state rectifier.  
    Then there were wooden cased versions of the Colt with
cabinets of solid oak, oak veneer and walnut veneer.   A
5-valve dual wave model was also available and was normally
supplied in a walnut veneer cabinet, when it was known as "The
Planet".   All in all quite a remarkable production record for
any small radio, which ended in 1967 when a transistorised
version known as the "Solid State Colt" was marketed.  
    The cabinet die used for locally moulding the plastic
cabinets came from Australia where it had originally been used
to produce the same cabinets for Airzone.   Because the Airzone
model did not have a tone control it needed only two controls,
but as the Colts had three controls it was necessary to provide
an extra hold in the cabinet front in the space originally
provided for the Airzone decal.  
  photo  
        COLLIER &amp; BEALE LTD      
    At the end of World War II Collier &amp; Beale Ltd, in common
with others in the industry, picked up the threads of peacetime
production and had their first post-war sets leaving the factory
in 1946.   Subsequently, quite a large range of models  was
marketed under the (unfranchised) Pacemaker name.  
    In the early post-war C &amp; B continued their policy of
supplying private-brand sets to the few remaining customers, the
Electric Lamphouse (Ensign) and His Masters Voice (HMV) being two
of them.   The existing names Cromwell and Gulbransen were
carried on as brandnames for franchised dealers for a few years
but both were discontinued after 1958.  
    In 1955 an important change in the company fortunes
occurred when the General Electric Co. of England (GEC) acquired
a majority holding.   One noticeable effect of this change was
reflected in the different manufacturing policy introduced.
  For example, where C &amp; B had always used heavy gauge cadmium
plated steel chassis they now used thin tinplate.   This was
obviously a cost cutting procedure which extended to such things
as using 100-volt working capacitors in place of the formerly
used 400-volt types.   Needless to say, reliability dropped
sharply as a result.  
  photo  
    Another change occurring after 1956 was the introduction
of model names in place of model numbers.   Prior to this any
names used, for example "Petit" (model 5155), were simply
telegraphic codewords, the only exception being the 1949 "Little
Jewell" (model 518N).   It is true that in pre-war days there
were a few model names used, for example the Radion "Little
Aristocrat", "Parliament", "Elstree" and "Rugby", but these were
exceptions.  
  photo  
        DOMINION RADIO &amp; ELECTRICAL CORP.      
    This firm was set up in 1939 expressly for the purpose
of manufacturing Philco radios in New Zealand, but the outbreak
of World War II soon halted production.   Philco radios
produced in the early post-war years were in any cases almost
"carbon copies" of pre-war models, an example being the 155, an
8-valve 3-band set of 1945 which was almost identical to the 1941
model 157.   As in pre-war days, the use of chromium-plated
chassis was initially continued, but within a year or two had
ceased.  
    The use of American-style model numbering which
incorporated a year of issue indicator had been introduced in
1941, as in the case of models 41-710 and 41-722 for example, but
was not continued with after the war&semi; in fact the American
influence on local production waned steadily from this time
onwards and by the early 1950s had almost vanished.  
      PHILCO RADIOS    
    Up to 1956 only Philco radios had been made by
  photo   Dreco, but at this time a new unfranchised
brandname, "La Gloria", was introduced, initially for the purpose
of marketing a line of low priced radiograms.   A limited range
of radios was also sold under   photo   the La Gloria name
over the next few years.   On the other hand, the name Philco
had, by 1962, been withdrawn from the market-place and to counter
this loss a "new" name "Majestic", was introduced in its place.
  Although also seen on television sets, this name did not
remain in use for long and had disappeared by 1965.   Another
brandname, used only on portable record players, tape recorders
and transistor radios, was "DRECO", but this name also soon
disappeared.  
  photo  
        HIS MASTER'S VOICE N.Z. LTD      
    Before World War II HMV had imported nearly all the sets
sold by them, only a very few being of local manufacture.   In
the post-war years they continued to make radios and radiograms
in their Wellington factory until 1957, during which period
approximately 62 different models were produced.   In addition,
there were 10 models made by other manufacturers, mainly Collier
&amp; Beale.  
    An unusual pre-war model of 1939 vintage was a 2-valve
TRF which had the distinction of being the smallest set made in
New Zealand in those days.   A set to have what must have been
the most unusual cabinet styling seen in this country was a 1946
5-valve model housed in a mirror-glass cabinet, or rather it had
a wooden cabinet overlaid with mirror glass.   It remains the
only locally made example of its type.  
    Commencing in 1945, a system of model identification
using three numerals followed by one or two letters was
introduced.   In this system the first two numerals indicated
the year of design, rather than the year of issue, though these
two dates often coincided, of course, while the third numeral
indicated the number of valves employed.   In addition various
letters were used as suffixes to distinguish different models
having the same number of valves but made in the same year.
  These suffixes were:  
    A = used to distinguish between two models  ,   AW
= all wave, D = dual-wave, S/B = band spread, P = portable, R =
car radio, R/G = radiogram  ,   RP = record player (without
radio), C/RG = radio with auto changer, T/RG = table model
radiogram.   Examples of the complete coding are: 465 = 1946
5 valves, 465D = 1946 5-valve dual-wave, 467SB = 1946 7-valve
bandspread, 526 D/CRG = 1952 6-valve dual-wave gram with auto
changer.  
    Commencing in 1947, model names were assigned in addition
to model numbers and this practice was continued right through
to the end of receiver production.   Unfortunately for those
concerned, then as now, HMV did not mark model numbers on chassis
made prior to 1948 which made     indentification  
  identification     difficult unless one had a set of
service manuals.   These manuals were particularly useful as,
in addition to the usual technical information, they contained
pictures of each model.  
  photo  
    The need for an improved system of coding was apparent
by the end of 1952 when there were no less than five different
5-valve models issued in that year.   Commencing towards the
end of 1954 a new model-coding system was introduced which
consisted of four numerals only without any suffixes.   In this
system the first two numerals indicated the year of issue and the
remaining two indicated serially the number of models issued that
year, with subsequent issues being numbered 5402, 5403 and so
on.  
    After 1957 HMV ceased manufacture and had sets supplied
by other firms such as Akrad, Green &amp; Hall and Philips.  
        JOHNS LTD - WELLMADE LTD      
    Although Johns Ltd had established a separate factory in
1928 they did not commence using the brandname "Well-Mayde" until
1931.   Even then this name never appeared on the front of any
sets&semi; it was to be found only on a nameplate on the chassis.
  In 1933 the name "Companion" was introduced in its place and
remained in use thereafter.  
    A simple system of chassis model coding was introduced
in 1931 which indicated the number of valves used and the year
of issue:  
    WM31 3 valves (plus metal rectifier) 1931
  WM61 6 valves (inc. rectifier) 1931
  WM81 8 valves (inc. rectifier) 1931  
    Cabinet styles were given model names and it is
interesting to note, that for the years 1931 and 1932 Maori names
were chosen, these being Ariki, Kiwi, Rangatira and Tui.  
    A slight alteration to the coding occurred in 1932 when
the letters "WM" were changed to "SG" to indicate Screen Grid,
an important sales feature in at the time.   As yet all models
were still of the TRF variety, superhets not appearing until 1933
when the coding was changed once again by dropping the first two
letters.   By 1935, with the advent of shortwave coverage, the
suffixes BC, DW and AW were added to the model numbers thus:
65BC, 65DW and so on&semi; later "AW" was changed to "TW" to indicate
Triple Wave.   For some reason that year 's system applied only
to AC models, battery sets being assigned model names
only.  
  photo  
        PHILIPS IN NEW ZEALAND      
    Although most of the Philips and Mullard receivers
illustrated here are examples of N.Z. made sets&semi; a few British
and Dutch models have been included for the sake of convenience
in presentation.  
    Philips commenced radio manufacture in this country on
a small scale in 1939 but the factory output was soon affected
by wartime conditions and, in common with other local
manufacturers, they   photo   were compelled to cease
production in 1942.   Prior to opening their own factory a few
models had been "made on behalf" by the Radio Corporation of N.Z.
between the years 1934-36, such sets being required to fill gaps
in the imported range.   Likewise, a few sets were imported
from Australia at much the same time.   Following the end of
the war, all Philips and Mullard radios sold in this country were
locally produced&semi; the last valve-operated model being marketed
in 1969.  
        RADIO CORPORATION OF N.Z. LTD -
COLUMBUS      
    The Columbus brandname was introduced in 1937 as a
"house" brand following a change in company policy which saw all
previous "private" brand production, with the exception of
Courtenay, phased out.   From then on nearly all models
produced by Radio Corp. were marketed under both brandnames,
though Courtenay was not so widely distributed as Columbus and
never became as well known.   In pre-war days the cabinet
styles of the two brands differed considerably but in later years
the differences were generally not so pronounced.   Nearly all
models were marketed under both names, though there were one or
two exceptions.  
    An interesting Columbus set made in 1939 was
  photo   the 10-valve model 88 all-wave console which
featured a special Philips low-noise RF valve, EF8G, and had
variable IF selectivity together with motorised push-button
tuning which could be remotely controlled.   It was one of only
three models which had a push-pull output stage.   Altogether
a most impressive, not to say unique, combination of
features.  
    Two top-of-the-line models, marketed in both brands, were
the 90 and 91.   The former was a swept up version of the
earlier 75, while the 91 was a 9-valve model having push-pull
output.   This latter feature was rarely used by Radio Corp.,
in fact apart from the above mentioned 88, only one other set,
the 13-valve high fidelity model 99 of 1946, had a push-pull
output stage.  
    Although small numbers of transistor radios were produced
from 1957 on, by 1960 radio production at the Wellington factory
had ceased.   One of the last valve-equipped sets to carry the
Columbus name was the model RG11 radiogram of 1960, but this was
made at Akrad 's Waihi factory.  
  photo  
        RADIO CORPORATION OF N.Z. LTD -
COURTENAY      
    The brandname Courtenay was originally introduced in 1930
on receivers made by W. Marks, and this name was carried on after
the Columbus name was introduced in 1937.   Courtenay radios
were originally distributed by the Stewart Hardware Co. but in
1935 distribution was taken over by Turnbull &amp; Jones Ltd.   In
1956 the Courtenay name was withdrawn from the market when
Turnbull &amp; Jones gave up handling radios.  
  photo  
        RADIO CORPORATION OF NEW ZEALAND
LTD      
    Because at one time there were so many different private
brandnames appearing on sets made by Radio Corporation they will
be given only the briefest mention.   Of the names appearing
between 1933 and 1937 only two ever became well known - "Pacific"
and "Stella".   Amongst the lesser known brands were Acme,
Audiola and CQ.  
    Not surprisingly, most of the brandowners wanted their
sets to look as different as possible from Courtenay or Columbus
and, as can be seen from the accompanying pictures, many of them
went to some trouble to accomplish this.   Particularly in the
case of Pacific was this endeavour noticeable where Art Deco
styling was favoured.  
    On the other hand the cabinets used by Stella were in
most cases quite similar to those used by Columbus and Courtenay,
particularly in the 1937 models which were the last private brand
sets issued.  
  photo  
          RADIO LTD - RADIO 1936 LTD - ULTIMATE EKCO
LTD      
    The first mains-operated Ultimate radio, produced in
1929, was an electrified version of the earlier battery-operated
Screen Grid Four.   It used exactly the same sized metal
cabinet and the same circuitry.   The main difference was the
use of 5-pin sockets for the first three valves, which were not
indirectly heated AC types.   A separate power pack was
used.  
    It is interesting to note that, following the tradition
established in 1927 when the all-wave   photo   battery sets
were launched, for the first three years AC receivers were made
only in all-wave form.   At this early date shortwave reception
was still very much of a novelty and very few manufacturers in
any countries had produced all-wave receivers for general use
before 1933.   Even though these Ultimate sets could boast
single-dial tuning they were still regenerative TRFs using
plug-in coils, which meant that they were sets for the enthusiast
rather than the ordinary listener.  
    However, by 1932 production of all-wave receivers had
been almost completely overshadowed by the arrival of
superheterodynes having BC band coverage only.   This state of
affairs continued for the next few years even though Radio Ltd
carried on their pioneering role by becoming the first N.Z.
manufacturer to make all-wave superhets.   By this time public
interest in shortwave reception had waned and not until several
years had elapsed did multi-wave receivers become popular.
  For the record, it should be mentioned that in 1932 Radio Ltd
produced a self-powered shortwave converter suitable for use on
any make or model of broadcast receiver.  
    A form of model identification for sales purposes was
introduced in 1931, using a three-letter code which indicated the
number of valves and the price of each model, thus: 856 indicated
8 valves &pound;56, 527 indicated 5 valves
&pound;27, and so on.   This system was carried on
through 1933 and was also used with the Courier brandline
marketed during this period.   In the case of Courier the same
actual numerals were used but their relative positions were
interchanged, thus 514 Ultimate became 145 Courier, and so on.
  Chassis identification was by means of an alpha-numeric code
die-stamped into the rear chassis apron or flange, the figures
preceding the serial number.  
    During 1934 cabinet styles became identified by model
names and the former three-digit system was discontinued.
  Chassis models were henceforth identified by figures
die-stamped on a small metal nameplate fastened to the rear
chassis apron.   Examples of the new coding are 4K, 5N, NS, but
there was no complete uniformity.   During 1935 the letters C,
D, E, F, L or X were used singly or in combinations such as CAU,
CES, LR, XC and so on.  
    Not until 1936 was a standardised system adopted wherein
the first numeral indicated the year of issue, thus A = 1936, B
= 1937, C = 1937-38, C and D = 1939, E = 1940, F = 1941.   The
fact that "C" had also been used in 1935 seems to have been an
unfortunate oversight.  
    So far, so good, but for some unfathomable reason Radio
1936 Ltd did not often make use of this coding when preparing
circuit diagrams, here it was (and is!) most frustrating to find
descriptions only being used, thus "6-valve broadcast 1938 model"
or "5-valve dual wave 1937 model".  
    By 1939, however, gradual change was taking place whereby
the actual chassis models came to be   photo   included on
the circuit diagrams.   But even as late as 1940 some circuit
diagrams were still being printed without accompanying model
numbers.  
  photo  
    Upon recommencement of production after the war, a new
system of chassis coding came into use.   All radios were
allocated model identification commencing with the letter "R"
(for radio), while all electrical appliances used "E" (for
electrical).   Initially just two letters sufficed but after
about three years it became necessary to use three letters.
  To summarise: RA to RY from 1946 to 1948, RAA to RAZ from
1948 to 1951, RBA to RBZ from 1951 to 1953, RCA to RCX from 1953
to 1959, RDA to RDZ from 1956 to 1959, REC to REU from 1959 to
1964.   Note: there is some overlap in the later periods.  
    Manufacture of transistorised radios commenced in 1957
and these used the same coding system.   After the closure of
the Quay Street factory in 1967 any radios bearing the Ultimate
name were made by Akrad and used a different
system.     