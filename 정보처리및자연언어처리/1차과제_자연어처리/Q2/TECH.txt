   Knowledge-based systems and operational hydrology    

  Slobodan P. Simonovic

   Knowledge-based systems were brought to the attention of hydrologists almost a decade ago.  The application of knowledge-based systems technology is natural and appropriate for the field of hydrology because it contains numerous procedures developed from theory, actual practice, and experience.  The emphasis of the present paper is on demystifying knowledge-based systems of artificial intelligence.  After a detailed review of the most important applications to the field of hydrology, the original concept for applying knowledge-based technology is presented.  The discussion ends with the list of possible benefits from the application of knowledge-based technology.  An expert system for the selection of a suitable method for flow measurement in open channels is used as a case study to illustrate the discussion in the paper.  The system has been designed for potential use in Environment Canada.
 
 

   Les hydrologistes ont pris conscience de l'existence de systemes a base de connaissances il y a pres d'une dizaine d'annees  L'application de cette technologie est naturelle et convient au domaine de l'hydrologie parce qu'elle comporte de nornbreuses procedures elaborees a partir de la theorie, de la pratique et de l'experience  Cet article a pour principal objet de demystifier les systemes a base de connaissances  Apres un examen detaille des plus importantes applications dans le domaine de l'hydrologie, le concept original d'application de la technologie a base de connaissances est presente  La discussion se termine par une liste des avantages possibles de l'application de cette technologie A titre d'exemple, un systeme expert pour selectionner une methode appropriee de mesure de l'ecoulement a surface libre est utilisee comme etude de cas  Le systeme a ete concu en vue d'une utilisation possible par Environnement Canada.  
   Mots cles: systeme expert, ressources hydraulique, mesures de l'ecoulement.  

   Introduction   

   Hydrology is the study of water in all its forms and from all its origins to all its destinations on the earth (Bras 1990).  The segments of the hydrology field this paper refers to are those pertinent to planning, design, and operation of engineering projects for the control and use of water, later called operational hydrology.  Some professional discussions indicate that a gap still exists between the basic scientific facts in hydrology and their application for solving water management problems.  A pertinent reason for this is the "scale difference" (Klemes 1983).  The hydrologic scale is largely outside the human direct sensory comprehension, making us incapable of creating meaningful conceptualization.  Another major reason is the very strong perception that hydrology is an appendage to hydraulics and hydraulic engineering (Yevjevich 1968). 
   The major objective of this paper is to bring to the attention of hydrologists the research within the field of artificial intelligence (AI).  This is not because of the lack of "natural" intelligence, but with the honest belief that some of the principles of artificial intelligence may help in the application of existing hydrological concepts and act as an inspiration for development and new discoveries. 
   When an engineering problem is complex with much scientific uncertainty and high demand for judgement, AI seems to have something to offer.  Knowledge-based engineering, called also expert systems or production systems, is a way to successfully build human expertise and some degree of intelligent judgement into decision-supporting software.  Knowledge-based engineering is concerned with the representation of knowledge and with symbolic reasoning (Rolston 1988).  One of the most distinguished characteristics of expert systems is their potential to deal with challenging real-world problems through the application of processes that try to mimic human judgement and scientific intuition.  The most general definition of an expert system is that an expert system is a computer application for solving problems that would require extensive human expertise (Rolston 1988).  To perform this task, expert system simulates the reasoning process by combining knowledge and search techniques (usually referred to as inferences).  Rolston (1988) characterizes an ideal expert system as one that includes the following: (i) extensive specific knowledge from the field of interest; (ii) the application of search techniques; (iii) support for heuristic analysis; (iv) a limited capacity to infer "new knowledge" from existing knowledge; (v) symbolic processing; and (vi) an ability to explain its own reasoning. 
   Knowledge-based systems are finding their place in the field of water resources engineering with all the dangers of being oversold or misused.  Recent publications by Ortolano and Steinemann (1987) and Simonovic and Savic (1989) present a survey of expert systems in environmental and water resources engineering, respectively.  The following paragraph briefly summarizes the review of Simonovic and Savic (1989) because of its relevance to this paper. 
   The roles of water resources engineering and the science of hydrology have expanded beyond the traditional concepts of design and synthesis to a large multidisciplinary function serving a broad social environment.  Development, in time, of these fields follows three basic phases: (i) construction (emphasis on the design and construction); (ii) planning (emphasis on the examination of wider range of alternatives); and (iii) operation and maintenance (emphasis on the careful management of existing projects).  This has created a pressing need for an overall review of engineering education with the main accent on its increased multidisciplinary character, supported by the available knowledge base and experience (Simonovic 1989b). 
   Since their first introduction to the field of water resources in the early 1980's, expert systems have been used in design, planning, and operation.  The following contributions have been made to water resources design.  HYSIZE and its simple modification HYSTOR are expert systems for determining the optimum layout for a particular hydroelectric site.  These systems are able to rank alternatives in order of economic priority and to test the sensitivity of assumed variables (Dotan and Willer 1986).  SISES is an expert system used for selecting an appropriate site for a specific use (Findikaki 1986).  DMWW is an expert system for designing a municipal water well (K. Strzepek, University of Colorado, personal communication, 1988).  The design process can be very complex and require much information on procedures and related knowledge (Russell 1989).  The design is created in the first phase, and then modified in the second phase until the user feels comfortable.  Experience and judgement play important roles in both phases, and this is why expert systems by exploiting experience may help, thereby, enhancing the design process. 
   Planning a water resources system is another field of expert systems application.  RAISON is a system developed for the analysis of acid rain data.  It is designed to examine the relationship between the terrain sensitivity index, which assesses susceptibility to acid rain deposition and possible deposition levels (Swayne and Fraser 1986).  WATQUAS is an expert system for extracting knowledge from a large quantity of available historical water quality data and interpreting it in a useful form (Allen 1986).  ARIANE is an intelligent decision-support tool for guiding the user through the multi-annual operation planning process in Hydro-Quebec (M. Hanscom, Hydro-Quebec, personal communication, 1988).  RRA is an expert system for the administration of the acreage-limitation provision of the US Reclamation Reform Act of 1982.  It provides a means for determining the status of the landholder, as well as the number of acres on which subsidized reclamation water can be received (K. Strzepek, University of Colorado, personal communication, 1988).  SID, Seattle Water Department's integrated drought management expert system, is an expert system designed to evaluate and display information for drought-management planning.  A linear programming model is used to generate optimal operating policies as a function of numerous past drought experiences.  These policies are incorporated into an expert system and the user is required to identify the degree to which the current drought situation is similar to past events (Palmer and Tull 1987; Palmer and Holmes 1988). 
   For the operation of water resources systems, expert systems are slowly taking their place in practice.  SID, an expert system already mentioned, is used for planning and operation of Seattle water distribution system during the drought.  JOE is an expert system designed to aid in operations of the Jenpeg generating station in Manitoba.  Manitoba Hydro's Jenpeg generating station is located near the outlet of Lake Winnipeg into the Nelson River system.  The operation of Jenpeg during the freeze-up period is very complex, involving many judgemental calls, and has a major impact on the hydro power generation downstream (Raban 1989).  EMMAES is an expert system built around the EMMA model used within Manitoba Hydro to plan the integrated operation of hydro and thermal power generation and tielines, as well as with maintenance considerations.  The system is being designed for three purposes: (i) preparation of an annual budget; (ii) preparation of weekly schedules for releases, thermal and hydro power generation, and imports and exports of energy; and (iii) long-term planning that includes such tasks as evaluation of benefits from installing additional capacity and examining particular operational conditions that may occur in the system (Nagy et al.  1989; Grahovac and Simonovic 1990). 
   The relative importance of expert systems to improvements in water resources projects has not yet been established, but some practical experience has already been documented (Nagy et al.  1989; Raban 1989; Palmer and Holmes 1988). 
   The following sections present definitions and approaches appropriate for the application of expert systems in the field of hydrology.  These are followed by an review of present research.  To illustrate their development, a case study of the use of an expert system for the selection of a suitable method for flow measurement in open channels is presented. 

   Knowledge-based systems and operational hydrology  

   Introduction to expert systems  
   Expert systems have been identified, by a number of authors, as a way to successfully apply AI techniques  Through the application of AI techniques, expert systems capture the basic knowledge required to assist an individual dealing with problems of varying complexity (Rolston 1988).  Expert systems function as an assistant to an expert; a partner to an expert, or a replacement for part of an expert's knowledge.  The following definition, derived by the author, seems appropriate for the field of water resources.  A water resources expert system is a computer application that assists in solving complicated water resources problems by incorporating engineering knowledge, principles of systems analysis, and experience, to provide aid in making engineering judgements and including intuition in the solution procedure (Simonovic 1990; Simonovic and Savic 1989).
 
   An expert system is a computer model composed of the following components: user interface; explanation subsystem; knowledge acquisition subsystem; knowledge base; and inference engine.  Figure 1 illustrates the basic structure of an expert system.  The user interface is responsible for requesting and translating user input, and presenting generated results to the user.  The explanation subsystem is a very important part of an expert system, as it is responsible for explaining the reasoning behind any conclusion the system reaches.  The knowledge acquisition subsystem is used to perform modifications to the knowledge base.  The knowledge base contains the facts and rules associated with the application field.  These rules can vary from being strictly procedural (well-defined and invariant) to heuristic (practices and procedures that are valuable but are incapable of proof and are gathered through experience).  The inference engine controls the execution of the system and determines how to solve a particular problem.  It uses the knowledge base to modify and expand the contents of working memory.  In simple words, the inference engine is a search mechanism.  Most expert systems are based on backward or forward search techniques.  In backward chaining, the system begins with the desired goal and works towards the requisite conditions to satisfy this goal; whereas forward chaining uses the known conditions and works towards the desired goal of the consultation. 
   Knowledge is the main source of an expert's ability to perform.  Therefore, expert systems use a collection of the rules and facts to mimic expert behaviour related to problem solving.  In an expert system, knowledge can be represented in the form of the rules, semantic nets, or frames.  Rules are the simplest and most popular knowledge representation scheme.  They are most appropriate when the domain knowledge results from associations between facts that have evolved through years of problem solving.  Another approach is to represent domain knowledge through a network of nodes and arcs, known as a semantic net.  The nodes represent the objects, concepts, or events; and the arcs represent the relationships between the nodes.  Finally, the term frame refers to a special way of representing concepts and situations  Essentially, it is the same as a semantic net, in that it consists of a system of nodes and arcs.  However, in the case of a frame representation all the properties of an object or concept are collected together at a node in a package.  Frames and semantic nets are most helpful in grouping and structuring a large number of rules. 
   The tools for developing an expert system can be divided into three main classes; (i) general purpose languages; (ii) representational languages; and (iii) expert system building shells and environments.  At the base level, an expert system can be written in any program language, such as FORTRAN, C, or PASCAL  Using these languages, the developer has complete flexibility, but the entire expert system structure must be developed and this is very costly in time and required resources.  General purpose representation languages, such as PROLOG, SRL, or OPS5, require only organization and expression of the domain knowledge.  As with the programming languages, a significant portion of the code necessary to produce an expert system must be written by the developer.  Expert system building shells and environments are packages that aid in the rapid prototyping of application expert systems.  They usually provide one or more knowledge representations and reference mechanisms.  Using these tools, the level of effort that must be applied to developing expert systems is greatly reduced, allowing the developer to focus on acquiring knowledge and refining the system behaviour.     Seismic response of concentrically braced steel frames  
  Richard G. Redwood AND Feng Lu
  Gilles Bouchard and Patrick Paultre

   Braced frame structures designed according to the 1990 edition of the National Building Code of Canada and the CSA standard for steel structures (CAN/CSA-S16.1-M89) are analyzed under a number of different earthquake motions.  The nonlinear response is studied in the light of the design philosophy, and the validity of a number of design assumptions is examined.  The study is limited to a group of eight-storey frames, located either in Victoria, British Columbia, or Montreal, Quebec, all with the same bracing configuration.  A 20-storey frame in Montreal is also considered.  The results suggest a number of areas in which improved design provisions could be made. 
 

   Les constructions avec charpente a elements entretoises concues selon les exigences de l'edition 1990 du Code national du batiment du Canada et de la norme CAN/CSA-S16.1-M89 sont analysees en fonction de differents mouvements du sol  Le comportement non-lineaire est etudie a la lumiere de la philosophie de conception et la validite d'un certain nombre d'hypotheses de conception est examinee  L'etude est limitee a un groupe de charpentes de huit etages erigees soit a Victoria, Colombie-Britannique, ou a Montreal, Quebec, presentant la meme configuration de contreventement  Une charpente de 20 etages a egalement ete prise en consideration a Montreal  Les resultats permettent d'identifier un certain nombre de domaines ou des ameliorations pourraient etre apportees.  
    Mots cles: analyse, conception, genie des structures, acier, tremblements de terre, charpente a elements entretoises.   

   Introduction  

   Detailing requirements for steel concentrically braced frames subjected to seismic design loads are incorporated in the 1989 edition of the CSA standard for limit states design of steel structures, CAN/CSA-S16.1-M89 (CSA 1989).  These provisions are coupled to those of the 1990 edition of the National Building Code of Canada (NRC 1990), which identifies three categories of concentrically braced frame: ductile braced frames, braced frames with nominal ductility, and frames for which no special provision is made to ensure ductile behaviour. 
   The anticipated behaviour of these three categories is as follows: 
  (i) Ductile braced frame (DBF): inelasticity will be largely confined to the braces, with beams also capable of some inelastic action  Large ductile deformations in the braces are provided for.  Columns are expected to remain essentially elastic under the loads induced by yielding braces.  The braces may yield in tension and compression.  Design provisions are directed at (a) limiting framing configurations to those that can maintain stability when some inelasticity occurs in the braces, (b) providing some redundancy, (c) ensuring brace ductility by controlling overall and local buckling, and by providing for adequate connection resistance, and (d) ensuring adequate resistances of beams and columns and their connections when braces yield. 
  (ii) Braced frames with nominal ductility (NDBF): yielding of braces may occur, but large ductility demands need not be provided for  No significant yield is anticipated elsewhere in the frame.  Ductility design requirements are directed at provision of brace sections that can undergo limited amounts of inelastic straining in compression or develop the yield load in tension, and at corresponding connection design.  Beams and columns and their connections must provide resistances adequate to support the brace-induced loads. 
  (iii) Braced frames with no special provision for ductility (SBF): principally elastic response is anticipated.  Local inelasticity may occur, but complete cross-section yielding is unlikely.  The design of these frames is based on traditional requirements for strength and stiffness.  Since these requirements do not exclude brittle details, for example, in brace connections or in column splices, the specified loading (NRC 1990) is quite severe.  By eliminating the possibility of such details by complying with case (ii), a significantly lower design load is decreed. 
   Three eight-storey braced frames, each corresponding to one of these categories, have been designed according to S16.1-M89 and the 1990 National Building Code, and are described in some detail by Redwood and Channagiri (1991).  These and other designs are examined in this paper.  The structures are subjected to a number of earthquake ground motion records, and the resulting responses are analyzed and compared with the expected behaviour. 

   Design procedures  

   The example building structure comprises two exterior moment resisting frames in one direction, and two braced bays in the other, these being located in the core area.  The building layout is shown in Figure 1 and is based on a study by Chien (1987) of a building not subjected to earthquake loads.  The design procedures for seismic loading are outlined in detail by Redwood and Channagiri (1991). 
   The ductile braced frame design, corresponding to category (i), will be considered first and is shown in Figure 2a.  Seismic loading corresponds to that for Victoria, B.C.  The frame was first designed for strength and stiffness in the usual way and was then modified to satisfy the requirements for ductile braced frames given in Clause 27.2 of CSA (1989).  In carrying out the design, a number of assumptions were made which will be examined in the light of the analytical results presented herein.  These assumptions were as follows: 
  (a) Brace connections will usually be designed to carry the full tensile yield load of a brace; however, CSA (1989) permits lower loads in some cases.  These are intended to deal with overstrength members, selected to satisfy stiffness requirements, for example.  Thus, for a ductile braced frame, when twice the seismic load, plus the specified gravity load, is less than the brace yield load, the lower load may be used for design.  This load is approximately equal to the load that the brace would carry if the frame was being designed as the third category of structure, that is, without special provisions for ductility.  For the braced frame with nominal ductility, the load is 1.33 times the seismic load, plus the gravity load, which also brings the load to approximately the same level as for the third category of structure. 
   In view of the importance of these connections for the structure to perform as assumed, there may be some reluctance to design the connections for less than the brace yield load.  The brace connections for the frame shown in Figure 2 a  were assumed to resist the full brace yield load.  The effect of using loads less than the tensile yield load will be examined in the light of the analytical results for another braced frame in which connection resistance less than the full yield load was assumed. 
  (b) Axial loads induced in the columns by the yielding of overloaded braces must be added to the specified gravity loads to obtain the design loads for columns under the severe seismic load condition.  Because of the low probability of all braces being overloaded simultaneously, a column in a lower floor need not be designed to carry the sum of the maximum brace loads induced at each higher floor.  For the subject designs, the loads in any column due to braces were taken as the maximum loads induced at any level above the column considered, plus the square root of the sum of the squares of all other brace-induced loads above that level.  The validity of this combination rule will be examined herein. 
  (c) Yielding of braces leads to redistribution of the horizontal shear carried by the braces, which then leads to a change in load carried by beams forming part of the lateral load resisting system.  Depending on the bracing configuration, some beams have no axial forces induced under lateral load until brace yielding occurs.  The assumptions used to estimate these loads for the beam design will also be examined.  These assumptions significantly influence the design of the short beams, i.e., those supported at their centre by the intersecting braces, and are described in detail by Redwood and Channagiri (1991).  
 

 

   For comparative purposes, in addition to the DBF design shown in Figure 2a, other designs were produced for the same structural configuration and also for the Victoria, B.C., location (Channagiri 1990).  A braced frame with nominal ductility, NDBF, corresponding to category (ii) and designed according to Clause 27.5 of CSA (1989) is shown in Figure 2b.  Again, it is assumed that brace yield loads are transmitted to the other members. 
   Figure 2 c  shows a design based on category (iii), that is, one for which no consideration of ductility has been given.  It should be recognized that, by virtue of Sentence 4.1.9.3(1) of NRC (1990), the latter design would not in fact be permitted in Victoria, B.C.; nevertheless, it provides a useful comparison, since it should correspond to very limited ductile behaviour. 
   Parallel to these designs, three braced frame designs were also generated for the location of Montreal, thus incorporating the effects of a significantly lower seismic-to-gravity load ratio.  These designs are shown in Figure 3. 

   Analysis  

   Program and modelling  
   The dynamic analysis was performed using the program DRAIN-2D (Kannan and Powell 1973), modified to include bracing elements (Jain and Goel 1978).  Assumptions made for the analysis corresponded to those on which the design was based, and may be summarized as follows: - columns and braces are pin-ended, and are without lateral support between floors;
- effective length factors of 1.0 are used for all columns and braces;
- beams are simply connected to the columns, and in those floors where they intersect the braces, the beam is continuous through the intersection point; beams are laterally supported. 

   Gravity loading equal to the specified dead and live loads was applied to the structures throughout the imposed earthquake excitation histories.  These loads impose initial compressions in columns and braces and tensions in some beams.  Specifics of the gravity loads are given by Redwood and Channagiri (1991). 
   The DRAIN-2D program carries out nonlinear time-history analysis of two-dimensional frame structures.  Mass is lumped at nodes; viscous damping with both mass and stiffness dependence was incorporated, and was assumed to be 5% of critical for the highest and lowest modes.  Constant acceleration is assumed in each time-step, and equilibrium corrections are made by applying corrective loads in the succeeding time-step.  Fundamental periods of the structures analyzed lie in the range 1-1.7s (the structure of Figure 2a  had periods of 1.47, 0.51 0.37, 0.28, and 0.22s for the first five modes), and an integration time-step of 0.02s was therefore assumed.  The total integration time used in the analysis was 30 s for each earthquake record.  P-effects were approximated by including a geometric stiffness based on the element axial force under static load (Kannan and Powell 1973). 
   A primary purpose of the analysis was to determine if the structures could be expected to perform as assumed in their design.  For the DBF, the primary interest was to determine if the beams and columns would indeed remain elastic if the braces yielded.  Thus for this structure, beams and columns were modelled as elastic elements, and only braces were modelled as yielding elements.  If beams or columns then proved to be overloaded, the extent of overload would provide a guide to selection of a section with the necessary increased capacity.  The same modelling is appropriate for the NDBF, whereas for the SBF, a fully elastic model would be adequate if the basic design assumptions were valid.  However, the braces were modelled to exhibit nonlinear hysteretic behaviour for this structure also. 
   The resistance of yielding elements used in the analysis and the resistances of members used to compare with the analytical results were based on their most probable values, i.e., the nominal values with the resistance factor  = 1.0. 
   The brace members were modelled as "buckling elements," denoted EL9 by Jain and Goel (1978).  This is a multi-linear hysteretic model which may yield in tension or buckle in compression.  An example of the hysteretic behaviour modelled by this element is illustrated in Figure 4.  The buckling load in the first cycle, C, is equal to the unfactored axial compressive resistance of CSA (1989).  In subsequent cycles, a reduced compression resistance, C|u, applies, the amount of reduction being based on empirical data (Jain 1978).  The load versus deflection path followed as the brace compression is reduced and loading becomes tensile is a function of loading history, and is based on empirical results.  The point A (Fig. 4) is determined by C|u and the compressive displacement occurring since the last load reversal.  Point B indicates the degree of pinching and is related to the slenderness ratio, and point C is a function of residual displacements.  The element does not model local buckling, and thus the deformations predicted by the analysis must be critically examined in relation to the section width-to-thickness ratios provided. 
   Columns were modelled as elastic truss elements, and beams as elastic beam - column elements, in view of the large axial forces expected when braces yield  Because beams were considered to be laterally supported by floor slabs, only their yield behaviour, and not buckling, is modelled.  Contribution of the slab in resisting axial or bending loads was ignored. 

   Earthquake records  
   For the structure located in Victoria, earthquake records from the western U.S.A. were used as input to the analysis Ten components of six recorded earthquakes were considered, as listed in Table 1.  Some of these same records were used for the Montreal structures, in view of the lack of strong motion records for eastern Canada, and, in addition, the Saguenay earthquake of 1988 was also considered.  

    CHANGE DETECTION USING PRINCIPAL COMPONENT ANALYSIS AND FUZZY SET THEORY   

  by 
 P. GONG

     RESUME    

    Cet article presente deux nouvelles methodes permettant de faire un meilleur usage de l'information multibande obtenue a partir de donnees de teledetection pour la detection des changements  Au lieu d'effectuer une analyse des composantes principales sur une combinaison d'images multibandes initiales, celle-ci a ete effectuee sur des donnees resultant de differences d'images  Ainsi, la plupart des informations relatives aux changements ont ete conservees sur les premieres composantes principales  Des operations fondees sur la theorie des ensembles flous ont ete proposees en vue de combiner les informations sur les changements provenant des differents canaux en une seule image  Les zones ayant subi des changements peuvent alors etre extraites de cette derniere image  Des images de Kitchener-Waterloo, en Ontario, acquises par le capteur thematique de Landsat pendant deux annees successives sont utilisees pour illustrer ces methodes  Un certain nombre de strategies sur l'utilisation des operations fondees sur la theorie des ensembles flous sont egalement presentees.    

    SUMMARY   

   In this paper two procedures that were developed to make better use of multispectral information from remotely sensed data for change detection are discussed.  Instead of applying principal component analysis (PCA) to a combined data set of original multispectral images, PCA was applied to difference images.  Thus, most change information was preserved in the first few principal component images.  Operations based on fuzzy set theory were proposed to combine change information from different image channels into a single-image channel.  Changed areas could then be extracted from this single image.  Landsat Thematic Mapper (TM) images acquired in two successive year over Kitchener-Waterloo, Ontario, are used to illustrate these methods.  Some strategies on the use of fuzzy set operations are discussed.  

   INTRODUCTION  

   In remote sensing, changes can be determined by comparing the spectral response differences at the same spatial location among a set of two or more multispectral images acquired at different times.  These images are first spatially registered.  A commonly used change detection procedure then follows in which changes are identified via thresholding a difference image that has been obtained by subtracting one band of image on one date from the same band of image on another date.  However, it is usually not possible to detect changes occurring in a region using only one spectral band because different types of changes may be captured in different bands.  Therefore, changes have to be enhanced and extracted from multispectral imagery. 
   Two types of procedures for using multispectral data in change component enhancement are commonly used.  The first type involves simple image arithmetic between images of the same spectral band for two dates.  For convenience, we define the two images obtained for two different dates of the same spectral band as band-pair images.  Image arithmetic includes rationing and differencing band-pair images.  Images so generated are referred to as change component images.  To locate and identify change automatically, change component images are thesholded or classified (Jensen, 1986; Fung and LeDrew, 1988; Pilon et al., 1988; Singh, 1989).  To detect changes visually, rationed or differenced band-pair images can be analyzed based on their colour displays on a video monitor or photographic products (Howarth and Wickware, 1981; Howarth and Boassan, 1983).  Although logically straightforward due to increased data redundancy and display difficulties, this type of procedure becomes inefficient to use when the image dimension (that is, the number of spectral bands) exceeds three. 
   To overcome these difficulties, a second type of change component enhancement procedure employs image transformation methods.  Transformation methods include vegetation indexing (VI) (Tucker, 1979), tasseled cap analysis (KT-Transform) (Kauth and Thomas, 1976), change vector analysis (CVA) (Malila, 1980), and principal component analysis (PCA) (Lodwick, 1979; Byrne et al., 1980; Ingebristen et al., 1985; Fung and LeDrew, 1987).  By image transformation, the change information recorded in original multispectral data can be preserved in a relatively small number of components.  Rather than serving as general enhancement tools, the VI and KT-Transform methods were developed specifically for such purposes as enhancing the vegetation or the soil component.  While CVA has the potential of summarizing various types of change components and their magnitudes into separate image channels, it has rarely been applied since its introduction.  Among these transformation techniques, the PCA method has been most commonly used.  Researchers using PCA for change detection have reported that the minor component images are likely to contain most of the change information when multispectral images that have been obtained on two dates are applied as an integrated data set.  The amount of change information contained in each principal component image, however, may vary from image to image.  It may not be an easy task to determine which principal component image to work with.  In addition, the use of PCA in such a manner is subject to the condition that the areas of changes have to be a small proportion of the entire study area (Richards, 1984; Fung and LeDrew, 1987). 
   It would be desirable to deal with only one image channel and to extract most, if not all, change information from this channel of image.  In this paper, a method is presented for transforming change information into one image channel from images of different spectral bands.  The main objectives are: 
 	to introduce an alternative method on the use of PCA for change component enhancement with which change information is guaranteed to be preserved in the major component images regardless the proportion of changed area in a study area; and

 	to demonstrate the effectiveness of fuzzy set theory in combining change information from different image channels into a single-image channel. 

   METHOD  

   The change detection procedure proposed in this study can be divided into six steps: 
 	spatially register images from two different dates;

 	undertake a band-pair image differencing for each spectral band and reduce registration noise;

 	apply PCA transformation to the multispectral difference image;

 	determine change membership functions for a number of selected change component images;

 	apply fuzzy operations to combine change information in different change component images into a single image;

 	determine changed areas based on the image generated at step five. 

   At step one, two images from different dates can be registered using a geometric correction program.  After the image-to-image registration, two images with the same coordinate system are obtained: X = {x i| i = 1, 2, , n}  of date one and Y = {y i| i = 1, 2, , n} of date two, where x i ' = [x i1, x i2, , x ip] and y i' = [y i1, y i2, , y ip].  The parameter - denotes the number of pixels in an image and p the number of spectral bands. 
   At the second step, a difference image, DIF j, for each spectral band j, can be created, where DIF j = { ij   =(x ij-y ij) | i = 1, 2, , -} and j = 1, 2, p.  A grey-scale mapping can then be applied to every difference image, DIF j, j = 1, 2, p.  This further reduces registration noise in each difference image (Gong et al., 1992). 
   In traditional change detection, one difference image of a specific spectral band is selected among these difference images.  This difference image should contain more change information than the other difference images for a particular application.  An image thresholding technique is then applied to detect changes using the selected difference image (Jensen, 1986; Fung and LeDrew, 1988).  For example, DIF 2, the red spectral band difference image of Landsat multispectral scanner (MSS) data is usually considered to contain more information on rural to urban land-cover changes than the other three MSS bands.  Thresholds T 1 and T 2 can be determined on the histogram of DIF 2 using the mean (ave) and standard deviation (std) (Figure 1).  Deciding whether a pixel has changed is simply a matter of testing whether  ij   falls outside of range [T 1, T 2]. 

 

 

   Two problems are associated with the above-mentioned traditional method, and they will be overcome by subsequent steps three to five.  As mentioned in the introduction, the first problem is that different types of change information are contained in different spectral bands; thus, the use of one spectral band usually does not allow every type of changes to be detected.  The second problem is that once thresholding is applied to a difference image, change information occurring at smaller magnitudes (that is, within range [T 1, T 2]) will be lost.  Also, noise could be included as change if its magnitude falls outside range [T 1, T 2] . 
   At step three, the difference images are used to generate a variance-covariance matrix.  This is used to find a new set of axes according to the eigen structure of the feature space.  With the variance-covariance matrix, PCA can be applied to the difference images.  The resultant principal component images are called principal component difference images, denoted by PCD j = {  eij | i = 1, 2, , -}  where j = 1, 2, , p ;  eij is a pixel value for pixel i in the jth principal component which results from a linear transformation of the difference images with the transformation coefficients determined with PCA.  PCD images are obtained from a p-dimensional data set {x i-y i | i = 1, 2, , -} instead of the traditional application of PCA in change detection where a combined two-date data set,{(x i ' y  i'  ')'| i = 1, 2, , -} of two p dimensions, is used.  Because the variance in a difference image represents primarily change information and the purpose of PCA is to preserve most variances into the first few principal components, the application of PCA to difference images will result in most change information preserved in the first few PCD images. 
   At step four, the first two or three PCD images containing change information are selected.  The exact number of PCDs is determined according to the eigenvalues and the correlation matrix of difference images.  Each selected PCD image and its histogram are analyzed, and a fuzzy membership function of change is empirically defined based on the analysis results.  From Figure 1, it is reasonable to assume that the more distant a pixel value is from the average, ave, the more likely is that pixel to fall into the change class.  Based on the shape of the histogram of a PCD image, parameters of a fuzzy membership function of change can be determined.  While a fuzzy membership function may take a variety of forms (Zadeh, 1978), in the case of Figure 1 an inverse triangular-shaped function may be suitable.  A more sophisticated change membership function may be determined based on the knowledge of the various types of change in the study area.  For example, statistics on various change types can be estimated from selected training samples. 
   A fuzzy membership function of change, cj(e), can be defined as: 
  

where   cj(e) represents the degree of pixel value   e in image PCD j belonging to a fuzzy set of change, C.  L  , ave, and H are the three parameters defining the inverse triangular-shaped function.  L and H can be determined empirically by examining the histogram distribution of image PCD j, and ave is the average pixel value in image PCD j.  A graphical form of   cj(e) is shown in Figure 2.  After applying a fuzzy membership function, cj(e), to an image, PCD j a change membership (CM) image, CM j = {  cj(ei) | i = 1, 2, , -}, is obtained. 

   At the fifth step, various change information from different CM images can be combined into one image, CCM, by applying the fuzzy set theory (Zadeh, 1965).  Most operations based on the fuzzy set theory can be realized by using three basic types of fuzzy set operator: fuzzy union (square-root), fuzzy intersection (), and fuzzy complement ().  While there are a number of definitions for fuzzy union and fuzzy intersection, the maximum and minimum rule are used in this study.  Therefore, fuzzy union of   1 and   2 is equivalent to max (1 , 2), and their fuzzy intersection is min(1, 2).  A fuzzy complement of   1 is 1 -   1.  For example, if one wishes to combine change information in three CM images in such a manner that both the subtle changes in CM 3 and changes in either CM 1 or CM 2 are included in the final resultant image, CCM = { (i) | i = 1, 2, , -}, the following operation can be used: 
 
where 1 -  c 3(i) represents the complement of   c3(i) because subtle changes have lower degrees of change membership in image CM 3.  Fuzzy set operations should be defined according to the characteristics of changes. 

 In most cases, successful change information extraction requires that knowledge about the study area and expert knowledge on various change types be properly represented with fuzzy membership functions and fuzzy set operations. 

   Once the desirable change information from different image channels has been combined into one image CCM, at the final step, the CCM itself can be stored in a database to represent change information.  One can also apply the thresholding or classification technique to determine and identify areas of change and to make a change map. 

   TEST AND RESULTS  

   In this section, steps one to five on the use of PCA and fuzzy set theory in change detection in the previous section are illustrated using an example.  However, no attempt was made for step six to undertake a thorough change detection of the study area.  Instead, the purpose is to combine change information from different image channels into a newly created image channel. 
   The principal component analysis module, PCA, image arithmetic, ARI, and image display programs in the EASI/PACE image analysis software package (PCI Inc., 1991) were used in this study.  Programs implementing the grey-scale mapping, the fuzzy membership functions, and the fuzzy set operations have been developed by the author as additional EASI/PACE modules. 

  The Study Area and Data Preparation  </I
   The study area consists of a large sector of the twin cities of Kitchener-Waterloo, Ontario, and a small part of their surrounding rural area.  A number of change detection studies have been carried out for this area with both Landsat Multispectral Scanner and Thematic Mapper data (Fung and LeDrew, 1987, 1988; Fung, 1990).  
   WIND TUNNEL INVESTIGATION OF A WING-PROPELLER MODEL PERFORMANCE DEGRADATION DUE TO DISTRIBUTED UPPER-SURFACE ROUGHNESS AND LEADING EDGE SHAPE MODIFICATION  

  R.H. Wickens Guest worker Applied Aerodynamics Laboratory

   ABSTRACT  

   A wind tunnel investigation has assessed the effects of distributed upper-surface roughness, and leading-edge ice formation on a powered wing propeller model.  

   In the unpowered state, it was found that roughness reduces the lift slope, and maximum lift by 30 to 50 percent, depending upon particle size and Reynolds number.  The leading edge region is especially sensitive to these disturbances; however, removal of the roughness over a small portion of the nose restored the wing to close to its original performance.  

   The application of power to the wing, with an increase of slipstream dynamic pressure, increases the lift slope and maximum lift; however, this benefit is lost if the wing is roughened.  Subtraction of the propeller reactions indicated that the slipstream interaction accounted for half the lift increase and also resulted in reduced drag for the clean surface.  This drag reduction was removed when the wing was roughened, indicating that the degradation of wing performance due to roughening is relatively greater when a slipstream is present, compared with the unpowered wing.  

   Leading-edge ice accretion causes similar large losses in lift and increases of form drag although a comparison of the two types of contamination showed that leading-edge ice produces a smaller reduction of lift slope prior to flow separation.  In both types of contamination, Reynolds number is important, and emphasizes the necessity of testing under near full-scale conditions.  

   NOMENCLATURE  

   INTRODUCTION  

   Recent flying accidents resulting from adverse weather conditions in the form of freezing rain or snow, have focused attention, on the degradation of aerodynamic surfaces.  One of the most recent accidents, involving a Fokker F-28, mk 1000 jet aircraft, and the subject of a Commission of Inquiry in Canada, dealt specifically with the degradation of such surfaces due to ice and snow contaminants on the wings.  The information contained in this paper stems in part from the investigation conducted for the Commission of Inquiry into the Air Ontario Crash at Dryden, Ontario, March 10, 1989.  (Reference 10)  Investigations of the effects of uniform roughness on airfoils shows clearly that stalling is premature, loss of maximum lift can range from 30 to 50%, (depending on Reynolds number) and form drag reaches very high levels at angles of attack below normal clean wing stall. 

   The effect of upper surface roughness on complete aircraft configurations is less well known; however, there is a long history of aircraft accidents related to flight in icing conditions and several recent accidents, including the Air Ontario F-28 accident, involving swept-wing jet aircraft have highlighted the problem.  In these situations it has been observed that early flow separation and stalling was a characteristic result of ice and snow contaminants on the wing   Flow breakdown was accompanied not only by a loss of lift and an increase of drag, but also wing-dropping as a result of outer panel flow separation and wing tip stall prior to inboard wing stall.  Experimental data on simulated upper surface contamination on a swept-wing model of a typical jet-commuter aircraft have confirmed what was suspected from flight experience and have also demonstrated that large changes of trim will occur on the full-scale aircraft. 

   Propeller-driven aircraft, where the slipstream passes over the wing surface, are thought to be less sensitive to the effects of upper surface contamination compared with the typical swept-wing configuration.  This is attributable in part to the effects of sweep that reduce the wing lift-slope, compared with a straight wing; and the effects of slipstream interaction that augment span loading locally, increase wing lift slope, and also delay flow separation at high angles of attack.  Thus the rotation angle on take off of a straight wing propeller-driven aircraft is likely to be less than that for an equivalent swept-wing aircraft with no slipstream interaction, and the likelihood of a premature stall may not arise. 

   Notwithstanding this apparent beneficial comparison, the propeller-driven aircraft may still experience significant losses of lift and large increases of drag if premature flow separation occurs when the wing upper surface is contaminated.  Figure 1b from Reference 1 for the Fokker F-27 turboprop transport wind tunnel model indicates, however, that smaller losses in maximum lift may be expected from a contaminated wing, compared with the airfoil test results of Figure 1a.  The corresponding reduction in critical angle of attack is also small and, in some cases, positive and was attributed to a significant change in the wing-slipstream stall pattern.  The extent to which the slipstream may remain attached to the wing surface is unknown but its influence may affect the overall stall pattern even when roughened by ice. 

   In view of the unknown nature of the complex interactions of wing boundary layer, propeller slipstream and distributed roughness, and the lack of experimental data, it was decided to use the half-wing propeller model of Reference 3 to obtain some preliminary data on the effects of upper surface roughness in a slipstream and also the effects of typical in-flight ice accretion shapes on the leading edge. 

 The utility of the data to aircraft design or performance estimation will be limited; the model configuration is not typical of current propeller transport configurations and the test Reynolds number was low (Re = 1.3 million). 

   MODEL  

   The general arrangement of the rectangular, unswept half-wing model is shown in Figure 2.  The wing, having a NACA 4415 airfoil section, was untwisted and was equipped with a 30 percent chord plain flap extending along the semi-span.  The aspect ratio was 4.85.  A nacelle containing a 20hp water-cooled induction motor was underslung on the wing approximately one chord length above the floor.  The four-bladed propeller was located at 70% chord in front on the leading edge and was equipped with an adjustable pitch-setting mechanism.  The propeller/wing chord ratio was 1.33. 

   EXPERIMENTAL PROCEDURE  

   The wing was pitched through an angle-of-attack range up to and beyond stall.  A complete stall and flow breakdown was not achieved with this model due probably to the effects of the low aspect ratio, Reynolds number and the half-model configuration. 
 

 

  Maximum lift was achieved however, and this was used as a basis of comparison for the effects of roughness.  Model lift, drag and pitching moment were measured on the wind tunnel balance.  Pitching moment was taken about the 30% chord location.  The measured forces include the propeller reaction, comprised of thrust, normal force and pitching moment.  The test Reynolds number was 1.3 million (2.3 million for the unpowered wing only). 

   At the desired test conditions, thrust coefficient CTp was varied by adjusting the blade pitch settings to a value of 0.115.  This corresponded approximately to the take-off thrust coefficient of a typical turbo-prop aircraft  Thrust coefficient was estimated from the isolated propeller data of Reference 5. 

   SIMULATED ROUGHNESS  

   Roughness, in the form of a uniform distribution of carborundum grit was applied over various portions of the chord.  Three grades of standard grit were used: 150 (.0041"), 80 (.0083"), 46 (.0165").  These correspond approximately to average roughness heights of .03", .06", and .11" respectively on a full-scale wing of 10 feet chord giving roughness height/chord ratios of 0.000227, .000461 and .000916 respectively.  In addition, a heavy grade (50 grit) of commercial sandpaper was applied to the wing surface.  The roughness height and concentration of this application was considered to be significantly greater than the standard grit particles applied manually to the wing surface. 

   The Carborundum grit was applied initially to the upper surface from the lower leading edge stagnation region to the flap hinge line.  Since only the forward portion of the chord was found to be sensitive however, most of the investigation was performed with only the first 25-30% of the chord roughened and the results presented in this report are for 30% coverage.

  The density of application was not varied or determined precisely, and in the case of sandpaper application, the transition was abrupt. 

   In addition to distributed roughness application, shapes representing rime and glaze ice accretions were applied to the wing leading edge.  The shapes were similar to those of Reference 6 and are shown in Figure 2c. 

   PRESENTATION OF RESULTS  

   Effects of Roughness - Powered Wing  

   With the application of power on the clean wing, the resulting slipstream interaction produces an increase in both lift slope and maximum lift by about 25%, and an increase of stalling angle of about 4 degrees.  Roughly half of this lift increment comes from the propeller thrust and normal force reactions; the other half is attributed to the interaction of the slipstream with the wing (Figure 3a). 

   The wing drag polar, as seen in Figure 3b, is shifted by an amount that corresponds to the thrust force plus a leading edge thrust on the wing due to increased leading edge suction.  The drag equivalent of the estimated propeller thrust has a value of about 0.085, which, when subtracted from the total wing force at zero lift apparently produces a negative drag or thrust on the wing.  This effect, known as the'squire Effect', has been alluded to before and is attributed the effects of increased pressure and flow rotation in the slipstream.  

   With roughness applied to the wing upper surface there appears to be a loss of lift slope and maximum lift of about 25 to 35% depending upon roughness element size (Figure 4a). In effect, the benefits of powered lift, resulting from slipstream interaction, is significantly reduced.  Drag also increases as the flow separates prematurely, and there also is an increase in the parasite drag at zero lift due to roughness, and increased dynamic pressure in the slipstream.  The effect of roughness on wing pitching moment is small at angles of attack below stall, (

   The application of the heavy sandpaper roughness further deteriorated the wing performance under power at the Reynolds number of 1.3 million.  Maximum lift decreased slightly, as did the lift slope; although the stall was not sharply defined.  Drag also increased near zero lift but the pitching moment did not change significantly, although the tendency continued to be nose-down. 

   A comparison between the powered and unpowered wing drag polars shows the relative effects of roughness with and without power (Figure 6).  It is clear from these graphs that roughness, especially when it reaches the heavy proportions of sandpaper coverage, has a much more adverse effect on drag of the powered wing near maximum lift than for the unpowered wing in uniform flow.  At lift coefficients below stall however, drag increases between powered and unpowered wings are similar.  The lift curves exhibit about the same degree of degradation of performance between powered and unpowered configurations.  The pitching moment change appears to be smaller when the wing is powered and is accompanied by an increase in slope (Cm versus to alpha) and a small change in the nose-up direction (Figure 5c). 

   In order to simulate the scrubbing action of the slipstream, a portion of the roughness was removed at the propeller location.  This resulted in a modest improvement of performance (Figure 5a). 

   Wing-Slipstream Characteristics  

   In order to separate the propeller reactions from the total wing forces, and to compare unpowered wing characteristics with those with the wing immersed in the slipstream, the isolated propeller thrust, normal force and pitching moment were estimated from Reference 5 and were removed from the measured wing force and moment data.  No attempt was made to correct the propeller data for the blockage and upwash events of the wing; however the comments of Reference 8 and the experimental data of Reference 4 suggest that these interactions may be small.  See Figure A. 

   Typical wing characteristics with the propeller reactions removed are shown in Figure 6.  The increase in CLmax for the clean wing (Figure 6a) is roughly half the total powered lift increments.  The loss of lift due to roughness increases with roughness scale and is apparently greater for the unpowered wing than for the powered wing.  Lift slope diminishes with roughness size in both cases.  
  Pattern classification and recognition based on morphology and neural networks 
   Classification et reconnaissance de formes basees sur la morphologie et les reseaux neuroniques   

  P. Yu, V. Anastassopoulos and A.N. Venetsanopoulos

   Morphological transformations are an efficient method for shape analysis and representation.  In this work the pecstrum (pattern spectrum), which is a morphological shape descriptor, is used for object representation.  Neural networks are then employed, instead of conventional classification techniques, for object recognition and classification.  Various coding schemes and training procedures have been examined in order to achieve a high classification performance.  A complete classification and recognition scheme is proposed, which is shown to work satisfactorily even for small objects, where the quantization noise has significantly distorted their shape.  The classification results are compared with those obtained using conventional methods, as well as with the results obtained using other shape descriptors.

   Les transformations morphologiques sont des methodes efficaces pour l'analyse et la representation de formes.  Dans cette etude, le pecstrum (pattern spectrum), un descripteur morphologique, est utilise pour representer un objet.  Au lieu de techniques de classification conventionnelles, on utilise les reseaux neuroniques pour la reconnaissance et la classification des objets.  Plusieurs strategies de codage et d'apprentissage ont ete etudiees afin d'assurer une performance superieure pour la classification.  On propose ici une strategie complete de classification et de reconnaissance fonctionnant meme pour de petits objets pour lesquels la forme est distorsionnee de facon appreciable par le bruit de quantification.  Les resultats de classification obtenus sont compares a ceux issus des methodes.   

   I. Introduction    

   Pattern classification and recognition techniques constitute the basis of computer-based decision systems and find applications in many important areas, such as medicine, space exploration, geophysics and defense.  Pattern recognition and classification are the links between low-level and high-level computer vision techniques.  They can also be characterized as internal or external, depending on whether they describe information of the whole area of the object or just of its boundary.  The shape descriptor considered in this work is an information-nonpreserving, internal, morphological vector descriptor called the pecstrum (pattern spectrum). 

   Morphological transformations [4]-[5] have been proven to be powerful in extracting shape information  These transformations are defined as combinations of set operations, the simplest of which are the morphological operations of erosion and dilation.  Their realization requires the binary object (set), which will be transformed, and a probing element used to extract the shape information, called the structuring element (SE).  The pecstrum is the simplest morphological shape descriptor which has been analyzed extensively [6]-[9].  The statistics of the pecstrum are unknown and, in addition, the way that these statistics change with the size (area) of the object is difficult to determine.  These two observations are the main reasons that conventional classification methods were not considered.  Neural networks were used instead to achieve the same objective.  In this work these difficulties were overcome using neural networks. 

   Artificial neural networks have been studied in an attempt to achieve biological-like performance.  Even though the best performance of the neural networks studied so far is still far from that of biological creatures, neural networks have already shown a great potential in areas where many hypotheses are pursued in parallel, and where high computation rates are required.  Studies on neural networks began more than 40 years ago with an attempt to develop detailed mathematical models, and a variety of neural networks were introduced for different classification purposes [11]-[12].  Various learning algorithms related to the previous network structures and network behaviours have been considered, and many successful applications have been developed. 

   Neural networks provide a great degree of robustness and fault tolerance to local damages, and high computation rates because of massive parallelism.  Neural networks with hidden layers can achieve better classification results. 

   The neural networks adopted in this paper are the feedforward networks trained with recursive least square learning algorithms [13]-[14].  A group of templates, considered to be the key input patterns, and their associative output patterns are employed to train the neural networks to obtain a set of optimal synaptic weights by successive iteration.  If the training is successful, when the input is a noisy or incomplete version of a key pattern, the network can recall the associated pattern.  If the network employed is on a binary basis, a suitable coding scheme has to be found to convert the morphological shape descriptors, which are represented by a set of continuous vectors, to a set of binary codes. 

   In this paper, the basic morphological operation, the pecstrum, is presented in section II.  Section III gives a description of neural networks and their application in morphological pattern classification and recognition.  Experiments and the results are shown in section IV, which also includes comparisons with the results obtained through conventional classification techniques.  Finally, conclusions are drawn in section V. 

   II. Mathematical morphology and the pecstrum  

   In this section we describe the pecstrum, which is a means of quantifying the geometrical structure of a continuous or discrete multidimensional signal.  Its origins and foundations lie in the principles of mathematical morphology [4]   Erosion () is a shrinking morphological operation which can be defined as

 ,(1)  

where X  is translated by every element of Bs  (B s  = { -b : b    B }), and then the intersection is taken.  Dilation () is an expanding morphological operation: 

 ,(2)  

where X  is translated by every element of B , and then the union is taken.  The erosion and dilation on a set X  by an SEB  are illustrated in Figure 1. 

   Opening () is defined as an erosion followed by a dilation with the same SEB ; i.e., 
 ,(3)  
	
where B  is the SE  The opening of a set X  by an SEB  can be seen as the union of all translations of B  which fit well in X : 
 (4)  .

  As a result, the difference between X B and X consists of all those parts of X  which are smaller than B  or, in other words, into which B  does not fit.  It is obvious that opening has low-pass filtering characteristics, because it removes from the object X , details which are smaller than B  and can be considered as high-frequency components.  The difference X  - (XB) denotes all the shape information which was extracted by the structuring element B . 

   We can continue the opening process on the opened set (XB), increasing each time the radius of the SEB , until the whole object X  disappears, as shown in Figure 2.  The differences which can be formed from two successive openings, i.e., 
 ,  where , 
denote the shape information that can be extracted by the structuring element ( - +1) B.  These differences are used to form the vector called the pecstrum in the following way  Each component, p(n), of the pecstrum, p, is defined as ,(5)  
where Mes (.) denotes the size of the set included in pixels.  If X it nB is empty for all -  It is obvious that each pecstral component, p (-), contains the shape information extracted by the SE (- + 1)  B .  Its value is equal to the size rejected normalized to the whole size of the object  Accordingly, all these components add up to unity: 
 . (6)  

   A shape descriptor must be translation-, rotation- and scale-invariant in order to be appropriate for many recognition and classification tasks.  The vector describing the pecstrum is translation-invariant, since its formation is based on morphological opening, which is a translation-invariant transformation [4]-[5] results in a pecstrum which is scale-invariant, while the maximum dimension, k, of the pecstral space is fixed and expressed from the relation between As and Mes (B) as the minimum integer given by

 (7)    

  Therefore, the number of the nonzero components in each pecstrum is less than k.  On the other hand, the pecstral space can be seen as a k-D orthonormal space in which all vectors, p, have components satisfying (6).  This equation determines a plane in the pecstral space with dimensionality k  - 1. 

   Extensive work on the properties of the pecstrum has been discussed so far, which provides an efficient tool for pattern classification and recognition.  The use of the pecstrum in classification and recognition tasks is attractive, mainly due to the high-speed execution of morphological operations and the fact that it is represented by a vector with only a few components.  It was shown in [7] that the number of the vector components extracted from a fixed standard size, A, is directly related to the size of the basic SE.  The size has to be properly selected to obtain a balance between simplicity of the vector and description of the shape characteristics  In this case, the classification procedure can be considered to consist of two modes.  In the learning mode, the pecstra p(i) (i is an index characterizing the i-th of L different binary objects) of the binary objects are evaluated, given that the objects are known and their size equals the standard size A s, and are stored.  In the recognition mode, the pecstrum of an observed image is evaluated and and the presence of the ith object is declared when the Euclidean distance, 

 ,(8)  
is minimum.  The coefficients c - were proposed to stress more or less the degree of participation of some of the components of the pecstrum in the decision making.  A good selection could be the one which gives small values of c - for the components of the pecstrum that are rather sensitive to various kinds of quantization noise.  In [7], the value of 1 was used for all c -.  

   In [9], the classification properties of the pecstrum and the pecstral space were examined extensively  

   III. Classification using neural networks  

   The structure of neural networks is based on our present understanding of biological nervous systems which are composed of neurons and synapses.  The neurons are considered to be processing elements, and the synapses to be variable resistors carrying weighted inputs that represent data or the sums of weights of still other processing elements.  Neurons can interact in many ways by virtue of the manner in which they are interconnected.  They can be either only feedforward, or they may have feedback loops.  Figure 3 is a typical single-layer feedforward neural system which can be described as

 ,(9)  

where y (j), j  = 1, 2, , - 0 , is the output neuron, and x (i), i  = 1, 2, , - i , is the input neuron of this layer; w ij is the synaptic weight of the network connecting nodes x (i) and y (j); (j), j  = 1, 2, , - 0 , is the threshold; and (.) is a nonlinear function.  More layers can be added, and each output neuron in the lower layer can be the input of the adjacent higher-layer structure. 

   The whole procedure of classification using neural networks can be seen as consisting of the following four steps:  
 1) Choose an architecture of the network structure, which means choosing a suitable number of bits for the input and output signals, hidden layer nodes, and the number of hidden layers involved in this network.
  2) Determine a proper coding scheme to convert the pecstra, which are analogue figures, to a series of binary codes.
  3) Use the key patterns and their associative patterns to train the neural network to obtain a set of optimal synaptic weights of the network.
  4) Having designed a special network, test it with all vector codes available as input patterns to evaluate its performance.  </I
   A heuristic algorithm for power-network clustering 
   Un algorithme heuristique applique au probleme de l'ilotage des reseaux de puissance   

  Hesham K. Temraz and Victor H. Quintana 

   An efficient heuristic algorithm for solving cluster problems associated with partitioning of power networks is presented in this paper.  The algorithm is divided into two stages.  The first stage creates an initial partition based on the electrical distance between system buses.  The second stage involves interchanging pairs of buses among the various clusters of the initial partition.  The first stage solves the placement problem of - connected buses in an r -dimensional Euclidean space; such a problem is reduced to finding r eigenvectors of a connectivity matrix, defined as the bus admittance matrix.  The second stage is based on a node interchange technique.  The node interchange is an iterative heuristic method that can be used to improve an initial partition.  The method moves one bus at a time, from one cluster of the initial partition to another, in an attempt to maximize the total electrical distance between clusters of the final partition.  Applications of the proposed algorithm to both small and medium-size power systems are illustrated in this paper. 

   Cet article presente un algorithme heuristique efficace pour solutionner les problemes d'ilotage associes au fractionnement des reseaux de puissance  L'algorithme se divise en deux etapes  La premiere genere un fractionnement initial base sur la distance electrique entre les bus du reseau  La seconde etape implique l'echange de paires de bus entre les divers ilots obtenus au cours du fractionnement initial  La premiere etape solutionne le probleme de la disposition de - bus interconnectees dans un espace Euclidien a r  dimensions; un tel probleme se reduit a trouver r  vecteurs propres d'une matrice d'interconnexion definie comme la matrice admittance du bus  La seconde etape s'appuie une technique d'echange de noeuds  L'echange de noeuds est une methode heuristique iterative qui peut etre utilisee pour ameliorer le fractionnement initial  La methode se deplace sur un bus a la fois, d'un ilot du fractionnement initial a l'autre, de facon a maximiser la distance electrique totale entre les ilots du fractionnement final  Des applications de l'algorithme propose a des reseaux de puissance de petite et moyenne dimensions sont presentees dans l'articles.   

   I. Introduction  

   In dealing with large-scale networks, several decomposition algorithms have been proposed in various fields.  In circuit theory, for example, we have the Gomory-Hu cut tree representation, the graph-decomposition by-linear-transportation problem, the node-tearing nodal analysis, and the multistack layout placement. 

   Partitioning of large power networks is important because, in general, only small portions of these networks are affected by a contingency, such as an outage of a transmission branch, failure of a reactive power compensator, etc.  During such contingencies, there are parts of the network that are left almost undisturbed.  The efficiency of many application programs in an energy control centre, such as load flow, optimal power flow and others, can be significantly improved if only the areas affected by the contingency are considered for the purpose of correcting the state of operation, as the calculation is then performed on smaller networks.  Smaller networks imply matrices of smaller dimension and, consequently, a smaller number of computer operations to be performed.  The use of smaller control networks can only be accomplished by partitioning a power network into clusters of buses, such that buses belonging to the same cluster are strongly connected electrically, while buses belonging to different clusters are weakly connected electrically. 

   To date, all proposed algorithms in power network partitioning are based on either the minimization of the number edge cut or the total wire length, and only the physical connections are considered. 

   The cluster partition problem belongs to a class of hard problems, the so-called NP-complete class, where no polynomial-bounded global solutions are likely to exist   In this method, each bus of a system is assigned a set of coordinates in a subspace r, where r is the number of clusters into which a system is to be partitioned.  The location of the buses on r is determined so as to minimize the electrical distance between system buses. 

   This paper is divided into six sections.  In section II, an eigenvector technique for separating buses into local clusters is described.  The node interchange approach for an initial partition improvement is described in section III.  In section IV, a summary of the proposed algorithm is presented.  Numerical results on the application of the proposed algorithm to both small and medium-size systems are presented in section V.  In section VI, concluding remarks are given. 

   II. Eigenvector partitioning  

   Let N  = {  N , M } define an undirected graph having a set of nodes N  = {1, , - } and a set of links M.  The problem of placing the - nodes of the graph into a subspace r , where r- , is not new; it is called the node placement problem.  In such problems there are two basic objectives: 1) the minimization of total wire length connecting the nodes; and, 2) the minimization of the circuit board area. 

   In power systems, however, the geographical placement of a bus cannot be changed by an electric utility company, since this is defined by the distribution of population and placement of energy sources.  Moreover, the wire length cannot be modified since the transmission lines only connect the various energy sources to the loads.  Nevertheless, it is extremely important to mathematically define a position on r  for each bus of the system.  By doing so, according to an electrical criterion, we make the bus position a part of the data required by this approach to partitioning of the power network. 

   Let us assume that we are given a system with - buses and an - - symmetric connection matrix C  =  C We want to find the location of the - buses that minimizes the weighted sum of the squared distances between the buses.  If x i denotes the x coordinates of bus i, and S denotes the weighted sum of squared distances between the buses, then the one-dimensional problem is to find the row vector x  T  = (x 1, , x  -) which minimizes

 ,(2.1)  

where T  denotes vector transposition  To avoid the trivial solution x  i 1  = 0, for all i, the following quadratic constraint is imposed: 
 .(2.2)   

   Equation (2.1) can be rewritten as follows: 
 (2.3)  

  Since C  is symmetrical (i.e., C i  =  C j), 
 (2.4)  .

  Define a diagonal matrix D  = (d ij) such that 

 (2.5)  .

  Now define the following matrix: 
 (2.6)   

  In other words, the i th diagonal entry, b  Substituting (2.6) into (2.4) yields

 .(2.7)   

   The Lagrangian of the nonlinear optimization problem described by (2.7) and (2.2) is defined as
 , 
from which the necessary condition is given by

 .(2.8)   

   Equation (2.8) leads to a nontrivial solution of x  if and only if is an eigenvalue of B, and x  is the corresponding eigenvector.  Premultiplying (2.8) by x T and imposing (2.2), we obtain

  Consequently, the value of the objective function described by (2.7) is the value of the eigenvalue of B.  The minimum of (2.7) is given by the smallest eigenvalue of B, and the solution, x, is the eigenvector associated with the smallest eigenvalue of B . 

   For the r -dimensional problem, S is simply the sum of r  quadratic terms, one for each dimension, and the minimum of S  is given by the sum of the r smallest eigenvalues of B.  To illustrate the application of the node placement algorithm [9], consider a four-node graph; its connection matrix, C, and the weighted connectivity matrix, B, are described in Figure 1.  It is required that all four nodes be placed in one dimension in a certain order, such that the total wire length connecting the four nodes is a minimum.  The elements of the connectivity matrix C are formulated as follows: 
 if nodes i  and j  are directly connected otherwise 

  The four eigenvalues of matrix B and their associated eigenvectors El, E2, E3 and E4 are given in Figure 2.  A plot of the four nodes is also shown in Figure 2, where E2 has been used as the y  coordinate.  Any other combination of node placement ordering gives a larger total wire length. 

   If the same placement procedures are used in power-network partitioning, only the physical connections of the network will be considered in a resulting partition; therefore, strongly connected buses may be assigned to different clusters.  A modification in the placement algorithm is required so that the technique can be sufficiently utilized in power-network partitioning; i.e., so that buses belonging to the same cluster are strongly connected electrically. 

   Matrix B, as described above, is a weighted connectivity matrix that measures the distance among buses.  By defining each line connecting two buses of a system as a link, and the electrical distance between two such buses as the value of the link's transfer impedance, we can replace matrix B by the bus impedance matrix Z  bus  However, it is computationally impractical to calculate the smallest eigenvalue of Z bus and the corresponding eigenvector because, in general, Z bus is nonsparse.  Defining the weighted connectivity matrix B  as the bus admittance Y bus, which is very sparse, and maximizing S  in (2.7) is equivalent to minimizing the transfer impedance between buses within the same cluster.  The maximum of (2.7) is given by the largest eigenvalue of Y bus, and the solution is the eigenvector associated with the largest eigenvalue of Y bus  In most practical power systems the ratio X/R is large  Consequently, in many cases, it is possible to neglect the resistances.  By doing so, we can represent Y bus by its purely imaginary part, hereafter denoted as B  bus. 

   The components of the eigenvectors associated with the r largest eigenvalues of B Now let us define the following transformation: 
 ,(2.9)  

where x  *   i   is the largest value of (-log 10|  x i |), for all i  = 1, 2, , - .  Applying (2.9) to each component of each eigenvector associated with the largest r  eigenvalues transfers the coordinates of the buses such that distances are measured with respect to the order of magnitude.  Moreover, the negative sign always keeps the data for these buses in a positive coordinate.  A power-network initial partition into r clusters is obtained by applying the leader algorithm [10] to the transformed coordinates of the buses. 

   The leader algorithm constructs a number of clusters of buses, a leading bus (centroid), and a distance TH for each cluster, such that every bus in a cluster is within a distance TH from the leading bus.  The algorithm makes one pass through the transformed buses-coordinates, assigning each bus to the first cluster whose leader is close enough, and making a new cluster and a new leader for buses that are not close to any existing leaders.  The distance between bus i  and the leader of any cluster is defined as , 
where x , y and   are the leader coordinates. 

   The distance TH is selected such that it creates no more than the required number of clusters  As an example, suppose a three-dimensional placement of seven buses whose transformed coordinates are as follows: 

  It is required that the previous seven-bus system be partitioned into three clusters.  Starting at bus 1 as the centroid of cluster 1, if TH=2, then three clusters are created.  If TH=4, only two clusters are created  By adjusting TH to 3, we ensure that three clusters are created, such that buses 1, 3 and 5 are placed in the first cluster, buses 2 and 7 are assigned to the second cluster, and buses 4 and 6 are located in the third cluster. 

   III. Node interchange partitioning  

   A popular class of algorithms for partitioning the nodes of a graph begins with a random partition and tries to improve it by interchanging nodes, one at a time, between pairs of clusters in the partition [11]-[12].  These types of algorithms are called node interchange algorithms. 

   The main idea of the node interchange algorithm is as follows:  Given a partition (A,B) of a power network, the algorithm moves one bus at a time from one cluster of the partition to another cluster in an attempt to minimize the electrical distance and the number of link cuts between pairs of clusters in the final partition.  The bus to be moved (call it the base bus) is chosen on the basis of both the balance criterion (the cluster size) and the effect on the weight associated with the number of link cuts.  Let us define the gain, g(i), of bus i as the value by which the weight associated with the link cuts would decrease, were bus i moved from its current cluster to the complementary cluster.  During each move, we must keep in mind the balance criterion to prevent all buses from migrating to one cluster of the partition, for that would surely be the best partition, were balance to be ignored.  Thus the balance criterion is used to select the cluster from which a bus of highest gain is to be moved.  After all moves have been made, the best partition encountered during the pass is taken as the output of the pass.  
  THEORY AND COMPUTATION OF TWO-METAL AND HIGHER ORDER PREDOMINANCE AREA DIAGRAMS 

  CHRISTOPHER W. BALE 

   Abstract  - Algorithms which calculate the classical one-metal predominance area or phase stability diagram are briefly reviewed.  The "Constrained Chemical Potential Method", which has been used to calculate one-metal predominance area diagrams, is adapted to calculate two-metal and higher order systems.  The topologies of the resulting diagrams are analyzed and general guidelines are proposed for constructing multi-metal predominance area diagrams.  Examples are presented for systems containing one metal, Fe-S-O, Ca-C-O-S-H, Cu-S-O, Zn-S-O, Al-S-O; two metals, Al-Zn-S-O, Al-Si-C-O, Cu-Fe-S-O ; and three metals, Al-Fe-Zn-S-O.  

    Resume - Les algorithmes calculant les diagrammes de stabilite ou d'aires de predominance d'un metal sont presentes brievement.  La methode des potentials chimiques imposes, qui a ete utilisee pour calculer de tels diagrammes, a ete modifiee pour permettre le calcul de systemes ou plus d'un metal est present.  La topologie des diagrammes resultant est analysee et les principes de construction de tels diagrammes sont presentes.  Des exemples de diagrammes sont donnes pour des systemes contenant un metal, Fe-S-O, Ca-C-O-S-H, Cu-S-O, Zn-S-O, Al-S-O; deux metaux, Al-Zn-S-O, Al-Si-C-O, Cu-Fe-S-O; et trois metaux, Al-Fe-Zn-S-O.   

   1. INTRODUCTION  

   The predominance area or phase stability diagram provides a convenient means of assembling heterogeneous equilibria in various gaseous atmospheres.  For example, Figure 1 shows the classical one-metal predominance area diagram for the Fe-S-O system where the axes are log 10 P (SO 2) and log 10 P (O 2).  All the species are in their pure standard states and the diagram is calculated from the standard Gibbs energies of formation of these species.  Algorithms which calculate and plot this type of one-metal predominance diagram are well documented in the literature [1-19]. 
   The diagram provides a researcher with a wealth of information on the stability and reactivity of the iron-bearing species in oxygen and sulphur atmospheres.  Figure 1 shows the effect of temperature in the range 800 to 1000 K  The diagram shows which species coexist and which species react.  For example, at 900 K the mixture of FeS-FeO-Fe 3 O 4 forms a stable invariant point.  On the other hand, the mixture Fe-FeS 2 is unstable and reacts to form FeS. 
   The phases are assumed to have fixed stoichiometry.  Variable compositions due to solution phases (oxygen and sulfur dissolved in iron) or non-stoichiometry (Fe 1- x O) are generally not taken into account.  For those systems where there are non-stoichiometric compounds or stable solution phases, the predominance area diagram is not the true phase diagram but rather a map of the relative stabilities or "domains of predominance" of the one-component species. 
   The predominance area diagram is not limited to one metal with two elements.  For example, Figure 2 shows a Ca-based system at 1100, 1150 and 1200 K with four elements: C, O, S and H.  The axes are log Each domain in Figure 2 contains one Ca-bearing species. 
   If the system contains two metals, for example Cu and Fe, the result is a two-metal predominance area diagram (Figures 7b to 7e).  The chemical interactions become more complicated due to the formation of multi-metal compounds of the type Cu 5 FeS 4, Cu 2 O&dot;Fe 2 O 3 and CuO&dot;Fe 2 O 3.  Certain domains in the two-metal diagram do not obey the same rules of construction as the one-metal diagram and so the complete diagram can not be calculated in the same manner.  The two-metal predominance area diagram is different because each domain contains two species and the relative stabilities of these species are determined by the overall metal composition ratio Fe/Cu.  That is, a given two-metal system may have several predominance area diagrams, each corresponding to a particular metal composition range. 

   In the literature, two-metal and higher order predominance area diagrams are not well documented.  Unlike the one-metal diagram which, in principle, can be produced with the aid of a hand calculator, the two-metal predominance diagram is generally not as easy to calculate.  Diagrams which are published have often been tediously generated and they may be limited to a single multi-metal compound.  It is possible to map all equilibria by means of sophisticated Gibbs energy minimization algorithms, but this is not a simple approach and requires access to powerful software.  There appears to be no general simple methodology to calculate the multi-metal diagram and explain the rules of construction. 
   In this article, the "Constrained Chemical Potential Method"  for calculating one-metal predominance area diagrams is reviewed.  The algorithm is then extended to calculate two-metal and higher order predominance area diagrams   It will be shown that the algorithm offers a systematic method of computing multi-metal predominance area diagrams containing two or more ligands with the axes represented as logarithms of partial pressures (or activities) or ratios thereof.
 
  The geometries of the resulting diagrams are analyzed and general guidelines are proposed for constructing multi-metal predominance area diagrams. 

   2. CLASSICAL ONE-METAL PREDOMINANCE AREA DIAGRAM  

  2.1.  Computing the one-metal predominance area diagram- standard algorithm  
   Algorithms which calculate the one-metal predominance area diagrams are well documented in the literature [1-19]  
   The standard algorithms for calculating a predominance area diagram generally consist of deriving expressions for the univariant lines of the diagram.  For example, in Figure 1 the FeS-Fe 3-O 4 univariant line is defined by the following equilibrium: 
 (1)  

 (2)  

where K 1, is the equilibrium constant.  An invariant point among the three pure phases FeS-FeS 2-Fe 3 O 4, in Figure 1 is determined by writing a chemical reaction involving the three species: 
 

 (3)  .

  An analogous expression can be derived to express log 10 P (O 2).  
   By systematically computing the univariant lines or invariant points, the complete predominance area diagram can be calculated.  If all the phases in the diagram are stable, the methodology is simple and in principle a diagram such as Figure 1 is within the reach of a hand-calculator. 
   In cases where there are metastable equilibria (for example, in Figure 1 FeO is metastable at low temperatures), it is necessary to verify that each calculated equilibrium is the most stable.  In complex systems, where there may be several elements, this not a simple task and it may require the use of a mass-constrained Gibbs energy minimization algorithm such as SOLGASMIX.  

  2.2.  Computing the one-metal predominance area diagram - the Constrained Chemical Potential Method  

   In the previous publication, a new methodology was proposed which provides an efficient means of computing one-metal predominance area diagrams   The algorithm will be briefly presented here since the equations become the basis to treat two-metal and higher order systems. 
   Fe is the "one metal" present in all the phases and we refer to Fe as the "base element" of the predominance area diagram.  The other elements (S and 0) may be referred to as ligands . The Gibbs energy of formation of each iron-bearing species from the standard state elements is expressed in terms of one mole of the pure base element (iron).  The general reaction is written as: 
  4)  .

  The stoichiometric factors j and k are greater than or equal to zero and z is always greater than zero.  Noting that the reactant Fe(s) is in its standard state, the Gibbs energy of reaction (4) is given by: 
 (5)  

where G 0 is the standard Gibbs energy change of equation (4) and where a(Fe z S j O k) is the activity (generally unity) of the iron-bearing species. 
   The equilibrium partial pressures of the standard state elements S 2 and O 2 at any point (x , y) in Figure 1 are deduced from the equilibrium constant of the reaction:.(6)   

   The Gibbs energy of any Fe-bearing species (equation (5)) can now be calculated at that point.  Since ln P (O 2) and ln P (SO 2) vary linearly across Figure 1, equation (5) can be simplified to: 
 (7)  

where X  and Y  are the coordinates (log 10 P (O 2), log 10 P (SO 2)) on the diagram and the constant coefficients a i , b i and c i are calculated from the linear dependence of the Gibbs energy.  For example, G (equation (5)) is calculated at any three arbitrary points on the diagram such as (X min, Y min), (X max, Y min) and (X max, Y max).  The coefficients a i , b i and c i are then calculated from the three simultaneous equations. 
   Setting up the equations in this way enables us to directly identify the most stable species in any region of the diagram as well as to define the stable univariant lines and invariant points. 
   The most stable species at any point (x, y) on the diagram is simply the one with the most negative Gibbs energy given by equation (7) at that point. 
   The univariant equilibrium between the two iron-bearing species p and q occurs when G p  =  G q.  From equation (7) the equation of this univariant line in Figure 1 is given by:  .(8)   

   An invariant point (x, y) exists among the three species p, q and r when G p=    G q=  G r.  Resolving the three equations leads to the following coordinates for the invariant point: 
  9) . 10)    

   The invariant point is stable if there are no other species with a more negative Gibbs energy (equation (7)) at that point.  A univariant line is drawn between two stable invariant points if at both points all the species but one are common.  This is the basis of the construction of Figure 1. 
   The calculation of the equilibrium partial pressures of the elements S 2 and O 2 at three points on the diagram and the resulting linear equation (equation (7)) is an essential feature of the algorithm and can be generalized.  The axes can be ratios of activities and/or partial pressures of any elements or compounds with S and / or O. 
   Additional (non-metallic) elements can be added to the system.  For example, Figure 2 shows the quinary system Ca-C-O-S-H calculated at 1100, 1150, and 1200 K.  The axes are log  In this system, the equilibrium partial pressures (or activities) of the elements C(s), O 2(g), S 2(g) and H 2(g) are calculated at three (corner) points on the diagram in order to determine the general equation (7). 
   The algorithm offers a systematic method of computing one-metal predominance area diagrams containing two or more ligands with the axes represented as logarithms of partial pressures (or activities) or ratios thereof. 

 It is a general method and is directly amenable to computer treatment and automatic data retrieval [20] .  
  Let us now extend the algorithm to systems containing more than one metallic (base element) component.  We will first consider the situation where there are two metals in the predominance area diagram. 

   3. TWO-METAL PREDOMINANCE AREA DIAGRAM  

  3.1.  Algorithm for computing two-metal predominance area diagrams  

   Figure 3 shows the Fe-Al-S-0 system at 800 K.  The axes are log 10 P (SO 2) and log 10 P (O 2).  The diagram was calculated by superimposing Figure 1 with the Al-S-O one-metal predominance area diagram.  Any possible two-metal compounds formed between Fe- and Al-bearing species do not appear.  In Figure 3 there are two "base elements", Fe and Al.  Each domain contains two species, one Fe-bearing and one Al-bearing. 
   As with the one-metal diagram, there are " Y -type" invariant points formed by three intersecting univariant lines (for example point 'a' in Figure 3).  The invariant point consists of four coexisting species, three of which are interacting with respect to one of the metals (Fe:- FeS, FeS 2 and Fe 3 O 4) and the fourth non-reacting species from the other metal (Al:- Al 2 O 3). 
   In Figure 3, there is a new type of invariant point formed by four domains (point 'b')  This represents a reaction involving two Fe-bearing (FeSO 4 and Fe 2(SO 4) 3) and two Al-bearing (Al 2(SO 4) 3 and Al 2 O 3).  The invariant point is produced by two intersecting univariant lines from each one-metal diagram.  The domains at the point form the shape of a letter 'X' with two straight lines. 
   Figure 3 is a convenient way of combining and comparing two one-metal systems but it provides little new information.  
   Let us introduce a two-metal compound into the system.  Figure 4 shows the FeO-Al 2 O 3 binary phase diagram    In the solid phases, FeO and Al 2 O 3 react to form an intermediate (two-metal) phase FeO Al 2 2 O 3: .(11)   

   According to Figure 4, Al 2 O 3+FeO Al 2 2 O 3 coexist when the overall metallic fraction is 0.0 The range of R  is simply given by the metallic stoichiometry of the coexisting compounds.  Hence it follows that a two-metal predominance area diagram is a function of the atomic ratio of the two metals, the limits of which can be deduced from the chemical formulae of the species containing the base elements. 
 

   In order to compute the true two-metal predominance area diagram, it is necessary to modify the strategy described in section 2.2 to include two base elements.  Each domain contains two stable species and from equation (5) general chemical reactions are formulated to produce the species from one mole of the pure base elements (Fe and Al): 
 (12)  .(13)   

   As before, the stoichiometric factors  n, m, j and k are greater than or equal to zero and z is always greater than zero.  Equations (12) and (13) represent the reactions to form Fe-bearing and Al-bearing species respectively, but now they include both metals.  For simple systems without intermediate multi-metal compounds, n and m are zero and the reactions reduce to two one-metal systems.  </I
   A procedure for estimating the overall system worth associated with generating unit refurbishment  

    Une methode pour estimer la valeur globale pour un systeme de la remise en service d'unites generatrices    

  R. Billinton and L. Goel 

   Quantitative reliability evaluation is an important element in power system planning and operation and the indices generated can be used to make a wide variety of planning decisions.  Quantitative reliability evaluation methodologies are now firmly established in the area of generating capacity adequacy assessment and are routinely used in planning new capacity additions.  Generating system adequacy is primarily governed by the unavailabilities of the units in the system and therefore one alternative to adding additional capacity is to consider improving the availabilities of existing units in the system. 

   Generating unit availability can be improved by unit and plant refurbishment, i.e. by investing capital to reduce the unit forced outage rate (FOR).  Many electric power utilities are doing this rather than investing in new generation equipment.  Generating unit unavailability has a major impact in a number of overall system areas, most of which are not normally considered by power station designers, plant operators and managers responsible for actual power plant modifications.  This paper presents a comprehensive and basic approach to recognizing the impacts of generating unit unavailability on expected energy production costs, expected failure costs and capacity costs.  The procedure is energy-based and utilizes an interrupted energy assessment rate applicable to the consumers served by the system to incorporate the expected load curtailment costs.  The worth of generating unit refurbishment is assessed in terms of overall system costs.  The cost of any specific unit refurbishment can be compared with the resulting net benefits and used to make decisions.   

   Une evaluation quantitative de la fiabilite est un element important dans la planification et l'operation d'un reseau electrique et les indicateurs obtenus peuvent etre utilises dans un large eventail de decisions associees a la planification.  Les approches quantitatives pour l'evaluation de la fiabilite de la capacite de production sont maintenant solidement etablies et sont utilisees de facon reguliere pour la planification des capacites additionnelles de production.  La capacite de production est regie principalement par la non-disponibilite d'unites dans le reseau et une facon alternative pour l'ajout d'une capacite additionnelle est d'ameliorer la disponibilite d'unites existantes dans le reseau.  
   La capacite de production peut etre amelioree par la remise en service des unites generatrices et du reseau associe, i.e. en investissant les sommes requises pour reduire la mise hors-service forcee (MHSF) d'unites et plusieurs compagnies distributrices d'electricite font ceci plutot que d'investir dans des unites neuves.  La non-disponibilite d'unites generatrices a un impact majeur sur plusieurs secteurs du reseau et la majorite de ceux-ci ne sont normalement pas consideres par les concepteurs des sous-stations, les operateurs de ces stations et par les administrateurs responsables pour les modifications en cours dans les reseaux.  Cet article presente une approche simple et pratique pour determiner l'impact de la non-disponibilite d'une unite generatrice sur les couts de production esperes de l'energie, les couts des bris anticipes et les couts en capacite  La procedure, axee sur l'energie, utilise un taux intermittent d'allocation de l'energie applicable au consommateur desservi par le reseau qui incorpore les cout de repartitions de la charge.  La valeur associee a la remise en service d'unite generatrice est etablie en termes des couts globaux.  Le cout de remise en service d'une unite donnee peut etre compare avec les benefices nets escomptes et permet la prise de decision.   

   Introduction  

   An electrical power system serves the basic function of supplying all types of customers with electrical energy as economically as possible and with an acceptable degree of continuity and quality  Reliability has, in recent years, become an important utility design criterion.  It is a highly complex quantity which is dependent on a wide range of factors.  Power system reliability.  System adequacy is governed by the existence of sufficient facilities within the system to satisfy the load requirements.  Such facilities include generation, transmission and distribution.  Adequacy assessment is, therefore, concerned with static system conditions and does not include disturbances in the system.  System security is concerned with the perturbations arising within the system, such as local and/or widespread disturbances, loss of any major facility, etc.  This paper is restricted to adequacy assessment. 

   Generating capacity adequacy assessment is an area in which extensive research has been done.  The basic system adequacy indices are the loss of load expectation (LOLE) and the loss of energy expectation (LOEE).
 The calculated values are very dependent on the individual generating unit forced outage rate (FOR) values.  The system peak load carrying capability (PLCC), which is the peak load that a system can carry at a specified system risk index, is also very dependent on the generating unit FOR values.  In addition to the impacts on system reliability, generating unit forced outages have a substantial effect on system costs and therefore on the price consumers have to pay for electricity   

   Many power utilities have recognized the implications of low unit availabilities and are actively engaged in unit refurbishment to improve these conditions.  The economic cost associated with a given unit FOR can be defined as the variation in the total system costs resulting from changing the unit FOR by an increment, while maintaining the same level of system reliability.  The total system costs include the three components of capacity cost, failure cost and production cost.  In this calculation, it is necessary to analyze the impact on the adequacy performance of the system of varying the FOR and to recognizing the variation in adequacy in terms of extra capacity. 

   Additional capacity which is required to compensate for the decreased level of system adequacy resulting from increased unit FOR can be obtained by adding peaking units in the form of gas turbines.  The simple addition of gas turbines may not, however, be the most effective method of adding capacity to a system over the long term.  It may be more appropriate in a particular system study to consider the actual long range plans of the utility which include specific unit sizes, types and locations.  The objective is to determine the average cost associated with the addition of an increment of generating capacity.  In order to do this, a present-worth analysis was conducted for an expansion period of thirty years.  The average cost associated with a unit increase in peak load carrying capability (PLCC) was determined from these analyses.  Several sensitivity studies were conducted to study the impact on this capacity cost figure of load forecast uncertainty, unit size and type and annual load growth etc.  The worth of unit refurbishment is assessed using the capacity cost benefits due to increased system PLCC and the benefits/losses in the system production costs and failure costs.  The cost of refurbishment can be compared to its worth in order to make objective decisions.  The ability to accurately estimate the refurbishment worth associated with a given unit FOR is an important factor in deciding when to retire an old unit and to replace it with a new one.  The segmentation method has been used in these studies to estimate the refurbishment worth associated with a unit FOR in terms of the capacity costs, energy production costs and the system failure costs.  

   Basic system parameters  

   An adequacy index can be defined as a parameter that denotes a particular aspect of system reliability.  The main objective in generating capacity adequacy assessment is to estimate the necessary generating capacity to satisfy the system demand and to have sufficient capacity to perform corrective and preventive maintenance on the generation facilities.  The LOEE, which is a measure of the expected unsupplied energy due to the occasions when load demand exceeds the available generating capacity, is an intuitively appealing index as it relates directly to the system function of providing customers with electrical energy.  The LOEE is also known as the expected energy not supplied (EENS). 

   Expected energy not supplied has been considered as the primary adequacy index in the studies described in this paper.  The determination of individual generating unit energy production is complicated by unit forced outages, maintenance requirements, the variability of hydro inflows, etc.  The earliest probabilistic production cost simulation which evaluated the expected energy generated by each unit and the EENS of the system was reported by Booth in the same year .  The evaluation of expected production costs, given that the expected energy output of each unit in the system has been predetermined, is a relatively straightforward task.  Under normal circumstances, a preferred synchronizing sequence or generating unit loading order is used to meet a specific load level.  The loading order of the units does not affect the system EENS provided that there are no energy-limited units in the system.  The loading order, however, can have a considerable impact on the system production costs.  In order to minimize the production costs, the units should be loaded in the order of their incremental variable costs, i.e. the generating unit with the lowest incremental variable cost should be loaded first as a base unit and the one with the highest variable cost loaded last or used as a peaking unit. 

   A major aspect in the justification and optimization of a specific system expansion is the cost-benefit assessment of the resulting power system adequacy  Conceptually, the benefits of having increased levels of electric service reliability can be related to the costs of providing that service.  Actual or perceived costs of interruption can be used to determine the benefits of increased system reliability.  The most practical method to establish reliability worth is to survey electrical consumers, sector by sector, to determine the losses incurred by each customer class due to power interruptions.  The data produced by this method can in turn be used to generate a composite customer damage function for a particular service area.  The customer damage function is an attempt to define the total customer interruption costs for that service area as a function of the interruption duration.  In order to create a practical tool for assessing reliability worth, these customer interruption costs must be related to the calculated adequacy indices such as the LOLE and EENS used in system planning and operation.  The EENS has been used in conjunction with the customer cost function to obtain a factor relating customer losses to the worth of electric service reliability.  This factor has been designated as the interrupted energy assessment rate (IEAR). 

   The segmentation method was used for performing all the system studies reported in this paper .  The various indices for the RTS are as follows:   Energy required = 15297.444 GWh
  Energy supplied = 15296.268 GWh
  Unsupplied energy = 1.176 GWh 
  Expected production costs = $156.346 million
  Expected failure costs = $5.91 million. 

   Impacts of generating unit unavailability  

   Given that the system contains a specific set of units with assigned forced outage rates and that there are no energy-limited units in the system, the system EENS and therefore the expected failure costs for a given load model are fixed.  In general, an increase in the FOR of a generating unit causes an increase in the system EENS, which in turn causes a corresponding increase in the expected failure costs.  The impacts of varying the unit FOR were studied using the IEEE-RTS.  The effect on the system EENS of changing the FOR of the 197 MW thermal units is shown in Figure 1.  The variation in the expected failure costs (using an IEAR of 5.02 $/KWh) is shown in Figure 2.  Figures 1 and 2 show that the system risk and failure costs respectively increase with an increase in the generating unit FOR (Figures 1 and 2 are directly related.  The difference in shape is due to the ordinate scales used in each case).  In a similar way, the impacts on the system risk of changing the FOR of all the 20 MW peaking units, the 400 MW nuclear units and the base loaded 50 MW hydro units are shown in Figure 3.  The 400 MW nuclear units have the highest FOR in the system and for this reason alone make a major contribution to the high system EENS and failure costs.  The 50 MW hydro units have the lowest FOR and therefore a large variation in their FOR does not affect the system risk index to a large extent.  The 20 MW thermal units are used for peaking purposes and hence do not contribute significantly towards the system inadequacy.  A large variation in their FOR, therefore, does not affect the system EENS as is evident from Figure 3.     Bound and resonant relativistic two-particle states in scalar quantum field theory  

  Leo Di Leo and Jurij W. Darewych 

   We derive relativistic particle-antiparticle wave equations for scalar particles, and , interacting via a massive or massless scalar field, (the Wick-Cutkosky model).  The variational method, within the Hamiltonian formalism of quantum field theory is used to derive equations with and without coupling of this quasi-bound   system to the decay channel.  Bound-state energies in the massless case are compared with the ladder Bethe-Salpeter and light-cone results.  In the case of coupling to the decay channel, the quasi-bound states are seen to arise as resonances in the scattering cross section.  Numerical results are presented for the massive and massless case. 

   Nous demontrons une equation d'onde relativiste gouvernant des particules et antiparticules (et) qui interagissent via un champ scalaire avec ou sans masse (modele de Wick-Cutkosky)  Dans la formulation hamiltionienne de la theorie quantique des champs, la methode variationnelle nous permet de deduire des equations avec ou sans couplage du systeme quasi-lie avec le mode de desintegration   Les energies de liaison dans le cas sans masse sont comparees avec les resultats obtenus par la methode du cone du lumiere et par l'equation de Bethe-Salpeter avec diagrammes de type echelle  Dans le cas d'un couplage avec le mode de desintegration, les etats quasi-lies sont identifies a des resonances dans la section de diffusion   Des resultats numeriques sont presentes dans les cas de masse nulle et non-nulle pour.   

   1. Introduction  

   One of the outstanding problems in quantum field theory is the computation of the bound-state spectrum and wave functions of hadrons at strong coupling  Lattice gauge theory has been an important method for studying the lowest hadronic states of quantrum chromodynamics (QCD), but it is very difficult to get detailed information about the hadronic wave functions in this approach.  The Bethe-Salpeter formalism has been the traditional tool for analyzing relativistic bound states, but this approach is difficult to implement, and is all but intractable for systems of three or more particles.  Thus it is of interest to consider other, more tractable, approaches, such as the Hamiltonian method or light-cone quantization, and to test them on model strongly coupled theories. 
   Scalar quantum field theory, in which massive spinless particles interact via another, massive or massless, scalar field has long been a favourite model for investigating methods of treating the relativistic bound-state problem.  See, for example, references and the citations contained therein.  In this approach, which in practice is perturbative, the relativistic two-particle system is described by a co-ordinate or momentum space equation in 3 + 1 dimensions.  The presence of a relative time coordinate in the resulting Bethe-Salpeter amplitude makes its direct interpretation as a Schrodinger-like wave function for the relative motion of the two-particle system somewhat problematic.  This situation does not arise in the Hamiltonian formalism of quantum field theory, in which the connection to the Schrodinger description is much more direct.  Recently, the variational method, within the Hamiltonian formalism of quantum electrodynamics has been shown to be effective in treating relativistic two-fermion states. 
   In the present paper we derive relativistic integral equations for two massive scalar particles interacting via massive and massless scalar fields, using a two and later a three Fock-space-state variational Ansatz.  These equations are solved approximately in various ways: numerical integration, a variational method, and by perturbation theory.  We compare the results, where possible, to the ladder Bethe-Salpeter (3) results.  

   2. Lagrangian, Hamiltonian, and variational method  

   For a system of massive particles, interacting via a massive (or massless) scalar field , the Lagrangian density is

[1]	 

where () is a complex (two-component) scalar field and (x) is a real scalar field  Thus, the corresponding Hamiltonian density is

[2]	 

where

[3]	 

[4]	 

and

[5]	 

where M0 and m0 are the (bare) masses and g,, v the coupling constants of the theory. 
   As is well known, the free quantum field theory (g = = v = 0) is solved by the Fourier decomposition (we take t = 0 in the present Schrodinger representation): 
[6]	 

[7]	 

where

[8]	 

[9]	 

and all other commutators vanish.  Note that in [6] and [7], and henceforth, we use the notation 
where, at this stage, m and M  are adjustable parameters, which we will later identify with the physical particle masses distinct from the bare masses, m0 and M 0, of the Lagrangian [1]. 
   To describe the bound state of a scalar particle-antiparticle system, each of mass M, we consider the Ansatz

[10]	 

where p1 + p2 + p3 = 0 in the rest frame of the system.  In [10], |0 is the vacuum state annihilated by the operators b, a, and d, that is, it is our trial vacuum state.  The coefficients F(p) and F(p1, p2) are to be determined from the variational principle

[11]	 

which is satisfied by the exact solutions, |  and E, of the field theoretic Schrodinger equation

[12]	 
 
  Since we are not interested here in the vacuum state but only in the particle-state excitations above the vacuum state, we normal order the Hamiltonian in applying the principle [11], and thus obtain the following set of coupled integral equations for the coefficient functions F(q) and F(q, p): 
[13]	 

and

[14]	 

where =  g/(2)3/2 .  Note that at this level of approximation (i.e., with the Ansatz as given by [10]) the term v 4 of the Hamiltonian does not contribute to the equations. 
   We decouple the integral equations [13] and [14] by taking E= +,=0, m0=m, and M=M0 in [14], which therefore yields

[15]	 

  Although this procedure may seem to be a rather drastic approximation, it must be borne in mind that the trial state [10], and so [13] and [14] are themselves severe truncations of the complete Fock-space expansion of the exact solution, which would contain an infinite number of terms in [10].  Indeed, the Ansatz [15] yields the exact result in the case where = (q2+M2)  1/2 is replaced, simply, by M in the Hamiltonian (i.e., fixed particles of mass M) in which case the  = v = 0 theory is diagonalizable (see, for example, Chapter 12 of).  
   Substituting the Ansatz [15] into [13] yields, for the  =0 case, the equation

[16]	 

   At this stage we note that in order that M  be identified with the physical mass of the particle (or antiparticle), we must take

[17]	 

where, as usual, the divergent integrals are controlled by a suitable cutoff, say 0|p|(q), where is finite. 
   The expression [17] is just a renormalization of the mass  Thus, if for a one-particle state we were to choose a trial state analogous to [10], 
[18]	 
 
and if we were to carry out the steps corresponding to those of [11]-[16], we would obtain, analogously to [16], the expression	
[19]	 

  This yields the correct expression for the one-particle energy, E=, provided that we choose the mass parameters in accordance with the condition [17].  Of course, this mass renormalization can also be handled in the conventional (and equivalent) way, by addition of suitable mass counter terms to the Hamiltonian. 
   With the imposition of the condition [17], [16] becomes

[20]	 

where   =/2M2 is the effective coupling constant, and

[21]	 

where we have included in [21] the contribution of the term of the Hamiltonian ([3]) that arises if 0. 
   We note, in passing, that the interaction kernel [21] of [20] can be identified with the on-shell invariant matrix element of the corresponding elementary Feynman diagrams for this theory.  Thus, for the one- "chion" (-particle) exchange diagram we note that, for  = 0, 	
	 

  The second term in the kernel [21] comes, similarly, from the invariant matrix element corresponding to the "contact" interaction . 
   In the nonrelativistic limit, |   q  |   M , [20] becomes, when  = 0, 
[22]	 

where =  E- 2M .  This is just the momentum-space Schrodinger eigenvalue equation for the stationary states of the two-particle system with Coulombic (m=m0=0) or Yukawa (m0) interparticle interactions.  For the case where the coupling is via a massless scalar field, that is the Coulomb case with m=0, [22] has the well-known hydrogenic analytic solutions, with bound state eigenvalues where  -  = 1, 2, 3,.  The corresponding eigenfunctions F (p) are just the Fourier transforms of the usual hydrogenic bound-state wave functions (see, for example,. pages 36-39) .  The solutions of [22] in the Yukawa case with m0, as is well known, cannot be expressed in terms of elementary analytic functions. 
   Equation [20] is a relativistic momentum-space Schrodinger-like integral eigenvalue equation for the stationary states of the two-particle system  Evidently the relativistic two-body kinematics is treated without approximation in this equation while the interaction, expressed by the kernel K (p, q) of [21], is described to within the limitations inherent in the Ansatze [10] and [15].  This kernel contains the one-chion-exchange contributions, which are dominant at low values of coupling constant, =  g2/16 M2. 
   We might mention, at this stage, that the same equation [20] is obtained at this level of approximation for both the particle-antiparticle or two-particle (or two-antiparticle) systems. 

   3. Approximate solution of the two-particle equation and bound states  

   The relativistic two-particle equation [20] can be reduced to radial form by setting where   q =|q| and Y lm () are the usual spherical harmonics, and carrying out the angular integration.  The result is

[23]	 

with

[24]	 

where

and Q1(z) is the Legendre function of the second kind. 
   Since it is not evident that the solutions of the radial equation [23] can be expressed in terms of common analytic functions, we have solved this equation approximately for a number of cases in three ways: perturbatively, variationally, and numerically  As has been pointed out already, in the nonrelativistic limit [23] reduces to the momentum-space radial Schrodinger equation, whose solutions are the well-known hydrogenic functions in the massless (m=0) or "Coulomb" case, and which have been extensively studied numerically for the massive (m=m00) "Yukawa" case.  Thus, if E nl () are the eigenvalues in the nonrelativistic limit (for the Coulomb case), then the bound state eigenvalues E nl () of the relativistic equation will reduce to nl () when    1, since 
   From the numerical study of integral equations similar to [23] (7, 8, 12) , as well as from the known spectrum of the radial Dirac and Klein-Gordon equations, we expect that the bound-state eigenvalue spectrum E nl () will start from 2M  at = 1 (1=0 in the Coulomb, m=0, case) then will follow the nonrelativistic values nl () downwards for 1 
   The integral equation [23] results from the extremum principle

[25]	 

where the functional E [f(p)] is given by

[26]	 

  Thus, for the relativistic energy eigenvalue E nl(), the first-order perturbative correction (to its nonrelativistic approximation, nl ()) can be obtained by evaluating the functional E [f p)], with f(p) replaced by the nonrelativistic momentum-space wave-functions  nl(p).  In short, to first order in perturbation theory we have

[27]	 

  For the Coulomb case (m=0), the nonrelativistic wave functions  <nl(p) are known analytic functions, hence the perturbative approximation [27] can be expressed in closed form, yielding, when expanded to order 4, the result

[28]	 

where the kinetic energy correction is, as expected

[29]	 

while the correction to the potential energy is

[30]	 

  Note that except for the potential energy correction V This is not surprising since the relativistic kinetic energy corrections must be independent of the particle spin, and only the dynamical relativistic corrections would be affected. 
   The present model in the massless (m=0) case and with = v = 0 has been solved in the ladder approximation of the Bethe-Salpeter formalism (this is the well known Wick-Cutkosky solution : 
[31]	   

  This is quite different from our result [28]-[30], since we do not obtain any 3ln or 3 terms in our approximation, but rather 4 corrections as in the quantum electrodynamics (QED) case. 
   At first glance it may seem surprising that our results do not agree with the Wick-Cutkosky ones beyond 2  However, it has been pointed out previously (13, 14) that the ladder Bethe-Salpeter approximation is a different and not necessarily a better approximation to the exact results, than approximations such as the present.

 In particular Gross (14) points out that the ladder Bethe-Salpeter approximation does not does not have the correct one-body limit   


   CHAOTIC VIBRATIONS AND RESONANCES IN A FLEXIBLE-ARM ROBOT  

  M.F. Golnaraghi

  Faculty of Engineering
 University of Waterloo
 Waterloo, Ontario
 N2L 3G1, Canada

  Received May, 1990, accepted April, 1991 No. 90-CSME-15, EIC Accession No. 2242 

   RESUME  

   Vibrations et resonances chaotiques dan un bras robotise  

   A partir du moment ou la notion de flexibilite est introduite dans le bras d'un robot, il faut d'attendre a ce que de serieux problemes concernent la precision et la stabilite viennent rendre son controle delicat.  Ces problemes ne peuvent etre elimines avec succes que si l'on prend en compte la dynamique non-lineaire du systeme.  Les bras flexible robotise qui nous interesse est a grande vitesse de deplacement, comporte deux degres de liberte et considere des non-linearites quadratiques dont les frequences naturelles sont definies comme etant 1 et 2.  Ce travail concerne l'etude du comportement de ce type de robot autour de la frequence de resonance interne .  Des simulations numeriques ainsi que des investigations analytiques ont ete faites sur un modele mathematique simplifie du systeme soumis a une excitation periodique.  L'utilisation de la methode des perturbations avec developpement a deux variables a permis de montrer l'existence d'un phenomene de saut et de "saturation" quand les resonance forcee et naturelle arrivent.  Les etudes numeriques montrent l'apparition de solution de type chaotique dans les regions de r&eacutesonance.  Les routes vers le chaos contiennent des bifurcations subharmoniques.   

  SUMMARY 

   Once flexibility is introduced into the arm of the robot, severe problems in the accuracy and stability are likely to occur which make control a critical issue.  These problems can successfully be eliminated only if the nonlinear dynamics associated with the flexible-arm is properly accounted for. 

   In this paper we study the behaviour of a two degree of freedom high speed robot with a flexible-arm, having quadratic nonlinearities with natural frequencies defined as 1 and 2, at internal resonance.  We perform numerical simulations as well as analytical investigations on a simplified mathematical model of the system, subjected to periodic excitation.  The two variable expansion perturbation method is used to show the existence of jump phenomena and `saturation' when both forced resonance and internal resonance occur.  Numerical studies indicate the existence of chaotic solutions in the resonance regions.  The routes to chaos contain subharmonic bifurcations. 

  INTRODUCTION 

   Flexible structures have been the subject of numerous recent investigations [1-4].  Included therein are the contributions in the field of robotics [5-8].  Majority of the papers on robotics are, however, aimed at the control aspects without considering the dynamics of linkages.  In general, once flexibility is introduced into the arm of the robot, severe problems in the accuracy, stability, and control are likely to occur.  A more successful control algorithm can be developed only if the effects of link dynamics is properly accounted for. 

   Robotic devices are usually modeled by coupled, nonlinear, differential equations which are impossible to solve exactly.  Addition of flexibility into the arm would further complicate the equations of motion [9-12] .  According to these studies, once periodic inputs with frequency are included, primary forced resonances occur at and .  Moreover, in the case of forced resonance, these studies show that the system exhibits'saturation' which is unique to systems with quadratic nonlinearities. 

   In this paper we perform analytical and numerical investigations of the dynamics of a high speed two degree of freedom flexible-arm robotic device, shown in Figure 1.  The arm is subjected to translational and rotational periodic inputs.  Because of the complexity of the actual system, a simple mathematical model is developed to ease the theoretical and numerical studies.  The two variable expansion perturbation method is used to analyze the stability of the system for and resonant cases.  Comparison of the numerical results with the perturbation solution indicate that the two variable expansion method provides a good approximation of the motion of the system when the amplitude of oscillation are sufficiently small. 

   Furthermore, we show that chaotic motions occur when forced and 2:1 internal resonances exist, concurrently.  The chaotic solutions are determined upon using bifurcation diagrams, fast Fourier transforms, and Poincare maps. 

  DESCRIPTION OF THE MATHEMATICAL MODEL 

   We use the model developed by [13,].  This model is a simple sliding pendulum mechanism as shown in Figure 2.  Mass M1 represents the mass of the dc motor magnet assembly.  Mass M2 is the mass of the body transported by the arm, as well as the effective mass of the flexible-arm r2.  The flexibility of the arm is taken into account by using a torsional spring with spring stiffness K2.  K1 represents the stiffness of the spring used to mechanically centre the position of mass M1.  C1 and C2 are the translational and rotational damping coefficients of the system, respectively. 

   The nondimensional equations of motion take the following form

 (1.1)   

 ,(1.2)  

where dots represent differentiation with respect to the nondimensional time, defined as
 
 (2).

  In (2), nd is the nondimensionalizing frequency.  The nondimensional variables are and, with 

 ,(3)  

and = 2 representing the angular position of the arm.  The nondimensional mass is defined as

 (4).

  The natural frequencies of the uncoupled, unforced linear system are

 ,(5.1)  
 
and

 	(5.2)  

  The nondimensional damping parameters are defined as 

 ,	(6.1)   

and

 	(6.2).

  The normalized forcing frequencies are

 ,	(7.1)   

and

 ,	(7.2).

  The base rotation is sinusoidal and takes the form

 (8)  .
 

   At this point we scale equations (1) by introducing a small dimensionless parameter,  Hence, we posit a change of variables such that

 ,(9.1)  

 ,(9.2)  

 ,(9.3)  

and

 ;(9.4)  .

  Note that (9.3) implies that F2 be small too, i.e. F2=f2. 

   In order to perturb off the undamped linear equations, we choose to scale the damping coefficients 1 and 2 as

 ,(10.1)  

 (10.2)  .

  Then we substitute equations (9) and (10) into (1.1) and (1.2) and expand the resulting equations using Taylor series for small , such that sin() and cos() are replaced by + and 1-, respectively.  With these assumptions, the equations of motion take the following form

  ,(11.1)  

 , (11.2)  
 
where

 (12).

  Note that the nonlinear terms in (11) are due to the effect of rotation on the geometry of the structure.  Note also that the unforced equations of motion have a stable equilibrium position at ( =0, =0, =0, =0). 

  THE TWO VARIABLE EXPANSION PERTURBATION METHOD 

   Using the two variable expansion perturbation method [18-21] , we replace the independent variable by two new variables, and, such that

 ,(13.1)  

and

 ;(13.2)  

where is just and is a slow time variable.  The idea of the method is to permit the dependent variables and   to depend explicitly on two time scales, and.  For example, periodic steady state behaviour will occur in , while approach to steady state will occur in. 

   Using the chain rule, we can rewrite the time derivatives of (, ) and (, ) as 
 , 14.1)  
  14.2).

  We also expand and as

 ,(15.1)  

 .(15.2).

  In order to study resonance cases, we use detunning parameters 1 and 2 such that

 (16.1)  

 (16.2).

  Note that we select fl=f1 to avoid secular terms in the zeroth order equations. 

   Substituting (13) - (15) into (11) and collecting terms, we find the zeroth and first order equations to be
Order 0 

 ,(17.1)  

 ;(17.2)  
 
Order 

 ,(18.1)  

 ;(18.2)   

where the subscripts represent the partial derivatives.  The solution of (17.1) and (17.2) can be written in the form

 ,(19.1)  

 ,(19.2)   

where

 (20). 

   We now substitute (19.1) and (19.2) into equations (18), suppressing the secular terms sin(1) and cos(1) in (18.1), and sin(2) and cos(2) in (18.2).  Note that we have written

 ,(21.1)  

 ,(21.2)   

 (21.3). 
 
  Due to the length of the expressions, we will eliminate some of the intermediate steps.  Before writing the solvability conditions, we will introduce a polar transformation such that

 ,(22.1)  

 ,(22.2)   

 ,(22.3)  

 ;(22.4)   

where, a- and - are real functions of.  This enables us to obtain the secular term equations in a more convenient form, namely

 ,(23.1)  

 ,(23.2)   

 ,(23.3)  

 ;(23.4)  

where

 ,(24)  

 (25). 
 
  The equilibrium solution is obtained when the right hand sides of (23.2-23.4) are set equal to zero.  There are two possibilities for these equilibrium solutions, ai=i= 0.  The first case is

 ,(26.1)  

 ,(26.2)   

 ,(26.3)  

 ,(26.4)  

which implies that the O() solution is essentially that of the linear system, as follows

 ,(27.1)  

 (27.2). 

  The second case is

 ,(28.1)  

 ,(28.2)  

 ,(28.3)  

 ,(28.4)  

where, 
 ,(29.1)  

 (29.2).
 
  Hence, the steady state response for this case, with the use of equations (28), is

 ,(30.1).(30.2). 
 
   Comparing these two possible responses, we can see that when the internal resonance holds, the solution has an extra term beyond that of the nonresonant solution.  This implies that will go through a subharmonic bifurcation and is excited by half the frequency of translation 1 in addition to the rotational forcing frequency 2.  From the solution for, we see major difference with respect to nonresonant solution.  That is, the amplitude of the mode is independent of the forcing amplitude f1 (i.e. saturation phenomenon).  We can observe these facts clearly in Figures 3 and 4.  The amplitude response shown in Figure 3, describes the modal amplitudes of response as the forcing amplitude f1 varies.  In this case, 1=-2.0,2=-1.0,1 =9.9,2=0.825, and 1 =2.  The stability of the equilibrium solutions was obtained by finding the eigenvalues of (23.1) - (23.4), evaluated at equilibrium values (26) and (28).  Hence, the solid lines in Figure 3 represents stable solutions while the dash lines indicate the unstable parts.  As we can see there exist two critical values of f1, defined by (28.2) and (28.4) as

 ,(31.1)  

 ,(31.2)  

where, for fc1 The values of a1 and a2 in this region depend upon the choice of the initial conditions.  The system at this region can go through a 'jump' phenomenon, based on the values of initial conditions.  The frequency response curves when 2=-2.0, 1=9.9, 2=0.825, f1 =10.0 and =1=2=1+1, are shown in Figure 4.  In this case, 1 corresponds to the detunning of the forcing frequency, so that when 1=0, the frequencies are related by  =1=2=1.  Again, there exists a jump between the resonant and nonresonant solutions.  The jump however, occurs only in one side of the resonance curve, and that is due to the fact that 20; hence, the resonance curve is unsymmetrical. 

   In order to illustrate the validity of the perturbation solutions, we numerically integrate the governing equations for and, (1.1) and (1.2), and compare the results with the solutions of (26) and (28) which are the equations defining the steady state values of response amplitudes.  The frequency response of equations of motion (1.l) and (1.2) is shown in Figure 5.  In this case, we have fixed 1=1.8, 2=1, 1=0.99, 2= 0.0825, F2= 0.01, and F1 = 0.1.  Note that the damping values have been selected to match those of the experimental system in [13].  Comparing Figures 4 and 5 we can see that the perturbation solution provides a good approximation of the behaviour of the actual system.  Comparing the amplitude response of (1.1) and (1.2), shown in Figure 6, with the perturbation results in Figure 4, one can arrive at the same conclusion.  Note that in Figure 6, the system parameters are fixed at  =1=2=1.5, 1=1.8, 2=1, 1= 0.99, 2= 0.0825, and F2 = 0.01, while F1 is varied. 

   Figure 7 illustrates the phase portrait and the frequency spectra of the motion at  =1.3 prior to the resonance.  Clearly, the response is following the driver, and the system behaves as a linear system.  However, at  =1.8, the system is in resonance and exhibits a subharmonic response.  The frequencies of this response are shown in the frequency spectra of and, in Figure 8. 

  EXISTENCE OF CHAOTIC SOLUTIONS 

   In the previous sections, we discussed the behaviour of the system for small oscillations, and obtained the possible solutions for the system response using the two variable expansion perturbation method.  In this section, we illustrate the behaviour of the numerical system at, for high forcing amplitudes, and show that the system exhibits chaotic solutions. 

   In order to illustrate the transition to chaos as the forcing amplitude increases, we obtain the bifurcation diagram of the motion of for two cases, shown in Figures 9-a and 9-b.  Figure 9-a is obtained for the parameter values  =1=2=1.8, 1=1.8, 2=1, 1=0.99, 2=0.0825, and F2 = 0.01, while F1 is varied from 0 to 0.6.  In this case we can see a very interesting phenomenon as F 1 increases.  The system undergoes two period doubling bifurcations, and reaches a period four, before going back to a period two at F20.3.  Chaotic solutions are observed upon further increase in the forcing amplitude.  This behaviour is not typical in systems exhibiting period doubling routes to chaos, and is has been seen in few other multi-dimensional systems (e.g. see  .  A more conventional response is shown in the bifurcation diagram of, for  =1=2=1.8, 1=1.8, 2=1, 1=0.99, 2= 0.0825, and F2=0.75, Figure 9-b.     S'ware vendor callbacks unsatisfactory, says IDC  

   TORONTO - International Data Corp. (Canada) Ltd. (IDC) has completed a survey of user operating system software support needs and expectations. 

   The purpose of the 1990 Canadian User Satisfaction with Software Support survey is to let vendors know how successful they are in the eyes of customers.  Vendors can compare customer satisfaction ratings in 1990 to those from the 1989 Canadian User Satisfaction with Software Support survey to discover where they have made progress and where they should concentrate their efforts in the upcoming year. 

   It is apparent customers still expect total service from a software service provider, based on average importance ratings of individual software support criteria.  According to the survey: 

   Overall support and quality control of software received were deemed the most important initial support criteria. 

   The ease of reporting a problem and receiving ongoing feedback on the status of a problem / solution were rated the most significant criteria of telephone support. 

   The ability to provide permanent fixes, quality of updates / revisions and their ease of installation, and the ability to provide workarounds were all judged as equally important ongoing support criteria. 

   The ease of maintenance and quality of remote diagnostics were rated as the most significant additional services. 

   One industry trend observed is that vendors, for the most part, are not meeting customer expectations in regards to callback times in both emergencies and under normal circumstances. 

   Hewlett-Packard is the only software support provider meeting its customers' average acceptable callback times in emergencies and under normal circumstances.  In fact, average callback times under normal circumstances have increased; only Hewlett-Packard and NCR achieved decreases in their average callback times under normal circumstances since 1989.  However, average callback times in emergencies have decreased; only IBM and Bull achieved increases in their average callback times in emergencies since 1989.  Vendors are concentrating too much on quicker responses in emergencies to the neglect of their response time under normal circumstances. 

   Customer ratings of individual vendors' performance are included in the report.  Vendors assessed include Bull, Digital, Hewlett-Packard, IBM, NCR, and Unisys. 

   For more information about IDC's 1990 Canadian User Satisfaction with Software Support survey, contact Mark Pellettier at (416) 369-0033.  </I

    Win over friends, neighbors with community marketing  
  by Catherine Callaghan Special to CDN 

   Common sense dictates that some of your most important business comes from buyers in your area.  But according to Tracy Groves, marketing manager for Concord, Ont.-based Computer Brokers of Canada (CBC), too many dealers overlook their own backyard when it comes to planning a marketing campaign.  "Community-based marketing is vastly underutilized," she says.  "Dealers have to be shown just how cost-effective it can be." 

   There are several good reasons to act locally when it comes to your overall marketing strategy.  Cost is a major consideration. 

   "For anything other than a local campaign, the costs will just destroy you," says Robert Cohen, president of The Cohen Group, an integrated marketing/advertising firm in Richmond Hill, Ont. 

   It's also a chance for you to differentiate yourself from the competition, using the hometeam advantage, or "buy local" mentality to your benefit.  Over the long haul, it's a chance for you to generate goodwill - to establish that your firm is a good corporate citizen.  In a competitive marketplace, the dealer that earns the consumer's trust gets the sales. 

   Not all businesses can approach community-based marketing the same way.  The vehicles you choose depend on who your customers are, says Groves.  "Community marketing is especially important if you serve the retail customer," she says.  Buying a computer represents a big investment - and a major risk - for most, so building trust and credibility over time is important. 

   The scope of your campaign also depends on your size.  A dealer like ComputerLand can afford a major outdoor advertising campaign, says Groves, but one-man shops may have to settle for bus-shelter ads outside the store. 

   Finally, location plays a big part in how you stage your campaign.  In major centres, says Cohen, it's critical to establish yourself as a specialist in some area - say, networking or peripherals.  In smaller communities, where you're up against a handful of competitors, you want to establish yourself as an expert in all areas. 

   The tools of the community-marketing trade will look familiar.  CBC's Groves points to local print and radio advertising as two of the most common ways dealers reach out to their neighbors.  "Community newspapers are extremely well-read outside major centres like Toronto," agrees Cohen.  And while TV advertising is often out of the question for companies on tight budgets, he says, spots on local cable channels can be highly cost-effective. 

   Richard Dexter, co-owner of Basys Consulting Ltd. in Dartmouth, N.S., is a firm believer in the power of local print media.  He and partner Roy Drinnan advertise regularly in the monthly paper that goes to the 2,000 neighbors in their industrial park. 

   "One of our target markets is small and mid-sized business," says Dexter, "and the majority of them fit the profile."  Basys pays between $300 and $400 for an ad in the monthly Burnside News, and Dexter says the campaign plays an important part in the firm's efforts to overcome price competition from dealers in Toronto. 

   Local business and community associations offer another way to reinforce your hometeam advantage.  Time, of course, is a limiting factor, says Cohen, and the extent of your personal involvement has to balance against potential returns: "You need to get your name known in the community without taking up a lot of your time." 

   Membership in a local Chamber of Commerce is a good way to start. 

   Isaac Ehrlich, owner of Richmond Hill, Ont.-based laptop and peripheral reseller Keysoft Network Inc., belongs to three local Chambers.  Listings "tell Chamber members who we are and what we are," says Keysoft general manager Barbara Smith, "and that's reinforced when we attend local functions.  Most of these people really make an effort to buy local." 

   The networking paid off recently when Keysoft landed a deal to sell 10 laptops to the municipality of Vaughn, north of Toronto.  The deal came about when Ehrlich, who lives in Vaughn, met with a municipal representative at a recent Chamber meeting. 

   Event marketing - hosting events ranging from wine-and-cheese open houses to champagne-splashed product launches - has become increasingly popular in recent years, says CBC's Groves.  Event marketing can work very well, she says, particularly in the computer industry.  "Because it's a very technical business, you need to show people a human face when you get them in the door." 

   But she warns that customers have become more blase' about events over the past couple of years, simply because there are so many of them.  As a result, businesses are having to go to even greater lengths to draw a crowd.  "You'll have an event and no one'll show up.  Then you find out that Joe down the street held one last week, and he had champagne and you only had beer - you get into that sort of competition," she says. 

   One response, says Groves, has been for smaller firms to pool their resources and stage "mini trade shows."  Renting space in a local hotel or meeting hall, a group can set up booths and invite members of the local community to browse and partake of food and drink.   With the combined muscle - and budget - of a number of firms, says Groves, mini trade shows offer one way to get your name in front of local buyers. 

   Direct mail offers a very precise way to carry your message to the people who count.  You can buy lists from list brokers, local associations like your Chamber of Commerce, or even church and community groups. 

   Roland Lau, owner of Calgary's The Home Computing Centre Inc., used postal code data to send flyers to 40,000 households in his immediate area.  The effort cost him about $7,500, and while he can't attribute a specific increase in sales to the effort, he will say the mailing increased his visibility. 

   When it comes to competition, says Lau, dealers have two choices: "You can be the biggest, or you can be closest to your community."  Lau estimates that more than 70 per cent of sales come from buyers within a 10-kilometre radius of his retail outlet. 

   Depending on the size of your company, outdoor advertising can be a good way to keep in touch with your community.  But both Groves and Cohen warn that, like any advertising, billboards, bus shelters and taxi tops only work if the ads emphasize some unique selling point. 

   "There is a place for outdoor advertising if you can afford it, and can play up some unique selling feature - if you've got an exclusive on a product, for example," says Groves.  "Everyone's got basically the same bill of goods, so you have to focus on what you do better." 

   Whether your community marketing campaign pays off in direct sales today, or paves the way for tomorrow's orders, the push to act locally remains the same: developing a relationship based on trust. 

   "Being a good corporate citizen is very important, especially when you're talking about a technical business like computers," says Groves.  "When you build that association with the community, they (local buyers) get to know you and trust you.  That's what IBM did - everyone trusts IBM, and look where they are today."  </I

    Quebec makes moves to boost s'ware profiles  
  by Nina Gilbert Special to CDN 

   How many software companies with an excellent product that meet a real demand remain virtually invisible despite their technical expertise? 

   In Quebec, the number is far too high, according to Yvon Blais, manager of the Quebec computer sector for the Ministry of Industry, Science, and Technology. 

   The problem is lack of marketing expertise, and many, including Blais, hope that the recently established Centre de Promotion du Logiciel Quebecois (CPLQ) will address this concern. 

   The centre, which began operations in September 1990, is a non-profit organization funded by the federal Department of Communications and the provincial Ministry of Communications. 

   The ministries are contributing $200,000 and $400,000 respectively over a period of two years, after which time the centre expects to be self supporting. 

   As the name implies, the centre aims to promote the commercialization of Quebec software. 

   Blais recently threw his support behind the new organization by inviting the CPLQ to co-sponsor a fall workshop that prepared Quebec companies for Comdex. 

   According to Michele Guay, general director of the CPLQ, the Comdex workshop was the first of many that the centre plans to be involved in. 

   "Within a few months we expect to be hosting one workshop per month and one showcase per week," said Guay. 

   Showcases, held in the centre's Montreal showroom, will allow software producers to present products to prospective clients and distributors. 

   Guay's strategy for achieving the centre's goal is two-fold.  On the home front, she intends to help Quebec software producers fully exploit their local markets.  As part of this effort, the CPLQ will try to get large Quebec companies to consider locally developed software before turning to the United States or elsewhere.  Guay believes a strong local base gives companies a real advantage in the international market. 

   The CPLQ also has projects underway for companies targeting markets outside of the province.  The centre plans to collaborate on a national marketing network for educational and training software. 

   Will Dubitsky, an industry policy analyst with Communications Canada, has organized a symposium for April in Vancouver.  At that time, groups from across Canada will meet with Dubitsky and B.C. Tel to discuss implementation. 

   According to Dubitsky, the CPLQ is the only centre of its kind in Canada right now.  They were the obvious choice to represent Quebec in the courseware network. 

   The CPLQ is also collaborating on a plan to give 10 Quebec companies a boost into the international market.  The companies will benefit from the expertise of a marketing firm and the involvement of government agencies.  For the relatively low cost of $5,000, the firms will be helped to launch an international campaign. 

   The CPLQ intends to play an important role as a focus for information about what is available to software producers and distributors.  "We want to be the hub of a net work made up of producers, distributors, buyers, associations, and government agencies involved in the Quebec software industry," explained Guay. 

   Many software producers feel a dire need for just such an information centre.  </I


   Is Unix really open?  

   So you've decided on Unix.  But have you decided on which Unix? 

   You're probably letting your hardware decide for you.  After all, the vendor of hardware will likely belong to one of two groups, Unix International (UI) or the Open Software Foundation (OSF). 

   Maybe you'll let the software or the application determine which choice you make.  Don't count on it.  Most software firms are hedging their bets by, at least for now, committing to both the UI and OSF groups. (See story, for example, Computer Associates commits to Unix, p. 21). 

   But why not try this test?  Ask your supplier if their version of Unix is open. 

   Why do we raise this question now?  It has to do with the great momentum behind open systems and the continuing uncertainty around other operating systems. (See stories, p.1).  Open systems may yet force the issue between which version of Unix will emerge as the standard. 

   Not surprisingly, David Sandel, vice-president of marketing, Unix International has stated that he sees "Unix System V as the base of open systems." 

   What this talk does is set up a confrontation by turning the road to open systems into a collision course for competing versions of Unix. 

   If everybody wants open systems, it stands to reason that they will only want one version of an open system.  And if they only want one version of open systems that is based on Unix, then one of the two is sure to face extinction. 

   It's too early to tell if users will go the UI or the OSF route.  Nor do we have any way of knowing if one of the two versions is technically superior to the other. 

   But the hype around open systems seems to have the affect of forcing this issue earlier than we expected.  Users should look now for signs of one version dominating the other. 

   The early returns place UI's version in front.  Said AT&T Canada's Benjamin Scott at the recent Open Systems show in Ottawa: "By the end of 1990, more than 300 companies were shipping products based on System V release 4 platforms.  With those kind of numbers, the UI version has the best chance of becoming the standard around which open systems will flourish. " 

   In contrast, users, as IBM and others have done, should raise some serious concerns about AT&T's ownership of Unix System V. (  Although, it should be noted that AT&T is negotiating to sell off as much of the 40 per cent of its Unix operating system). 

   If an open system is all about being non-proprietary, users may yet look to the OSF for direction. 

   New columnist  

   The Communications section of Computing Canada welcomes another monthly columnist in this issue (see p. 40).  Clyde Boom, director of technical services for Lancom Technologies, a Toronto-based computer services firm, will contribute a LANs (Local-Area Networks) column from a training and systems integration perspective.     </I

    Training should remain a priority  
  by Paul Swinwood 

  Paul Swinwood is president, Learning Tree International, a Gloucester, Ont.-based firm that specializes in telecommunications, software and management training. 

   Times are tough out there.  Sales managers are reporting lost sales, production managers are reporting low backlogs, CEOs are warning of layoffs and controllers are calling for cutting expenses. 

   What can a manager do?  First no travel, no training, no more giveaways - cut expenses.  Congratulations, well done. 

   That solution may be too simple.  A leading edge, competitive company, however, will continue to spend on two major areas during a recession: future projects and future productivity. 

   Successful companies plan carefully, review projects annually and focus on products, productivity and most importantly, people. 

   As a training institution, we typically feel the downturn significantly as our clients freeze spending.  In our experience, the major successful players in the technology fields react well to market pressures and then focus on the resources available. 

   Some tips on using these resources include: 
  review your current product and procedures for short-term productivity gains.  Can better training improve productivity of the staff;
  review your development needs.  Could training cut development costs and time required to bring your newest product to the market quicker and gain a market edge; and,   review your staff needs.  Could training provide the morale boost and productivity improvement to weather these tough times?  Most training can be brought to your location.  You save on travel, staff availability costs and gain productivity immediately because the training is focused on your environment and needs.  Tough times call for tough measures, but the wise manager recognizes training remains important.  </I

    IN CONVERSATION  
   Mark Skapinker  

   Mark Skapinker is a co-founder and president of Delrina Technology Inc. 

   He is credited with helping to define and to promote the forms processing market through his participation in product innovation and a number of industry engagements.  Under his direction, Delrina's PerForm software has garnered a number of industry awards. 

   Prior to Delrina, he was director of product development for Batteries Included, before its acquisition by Electronic Arts. 

   He has a post-graduate diploma in computer science from the University of Jerusalem. 

   He recently spoke to Computing Canada editor Martin Slofstra at Delrina's head office in Toronto. 

 

    Derlina Technology Inc.   

   Delrina Technology Inc. is a wholly-owned subsidiary of Toronto-based Delrina Corp. and is recognized as a leading supplier of PC-based graphics forms processing software.  It claims nearly 100,000 units shipped. 

   Other subsidiaries of Delrina are Carolian Systems (data centre management systems for HP 3000) and a 51 per cent share of SoftPort Technologies, distributors of accounting software. 

   In April 1990, IBM Canada Ltd. acquired a 11 per cent stake, representing the first such investment by IBM in a Canadian software company. 

   Delrina Corp. also has offices in Ottawa, Los Gatos, Calif., Washington D.C. and England.  Revenues for the period ending June 30, 1990 were $8.1 million, up from $5.0 million the year before.  </I

    CC: How would you define forms processing and what does it include?  

   Skapinker: Forms processing is basically the computerization of forms.  It takes parts of many other technologies and brings them all together.  In other words, it takes the printing, WYSIWYG (What You See Is What You Get) and complete page layout aspects of desktop publishing, the database data entry elements from database, printing technology, font technology and it also includes communications. 

   CC: Who in the organization would control the forms processing function?  

   Skapinker: I think it depends on the size of the organization.  What we've done is we've split our product line completely down the middle.  So we have regular PerForm, which is really aimed at the entry-level or smaller business, where it's the owner-operator or the administrative people who look after the forms.  Then we've got our PerForm Pro, which is really aimed at the high-end of the marketplace. 

   Certain companies may have a forms department.  The administrative people may look after it, and often, the financial people look after it.  Certainly the MIS people look after only a certain kind of form.  However, it's the senior managers beyond the individual departments, who really see the hidden cost of forms and the advantage of forms processing. 

   CC: Are organizations starting to integrate their forms processing function with their overall information strategy?  

   Skapinker: Very much so.  There is no need to fill in a form on paper and then re-enter the data in afterwards.  It makes absolute sense to be building the form into an existing network of machines or communication of machines, get the data in, be able to use it anywhere within the organization, and of course, get rid of a lot of the waste. 

   Why keep writing your name and address on 50 different forms when you only really need to update your address once and it will appear on any other forms? 

   CC: Forms processing is one application that is said to be ideal for client / server applications.  Do you see that as so?  

   Skapinker: If you ever look at the type of money that people are currently spending on paper forms, it starts becoming far more realistic. 

   There's huge costs based in terms of processing a form.  The cost of processing a form, compared to the cost of printing a paper form is about 50 or 60 to one.  So for every dollar that you spend on buying a paper form, you spend $60 on processing that form. 

   For years, we heard about how the industry had managed to computerize itself, and small office functionality had been able to computerize itself, yet when it came to the large overall management of an office, it has never been able to get computerized.  It's really quite interesting to have a look why this has never happened. 

   A lot of this is that the hardware hasn't been there.  You need the graphics hardware, you need laser printers, you need networks, you need a database which exists out there on a client/server basis. 

   With that all in place, we now have the ability to pull all this together and actually computerize the communications between individuals in an office, which is forms processing.  I think the opportunity is enormous in terms of real live users and the application of form processing in the office. 

   CC: So electronic mail, for example, could be done in the way of forms when you introduce a large enough user base? 

   Skapinker: I don't think that people think in terms of electronic mail.  When I go on a business trip and I come back and I fill in an expense form, I want one copy to go to my manager who is going to sign it off.  

   My manager, once he's signed it off, will probably send it through to the accounting department who will generate a cheque and send that cheque back to me.  I don't think of that as electronic mail.  I think of that as the routing and the processing of the form that I've just filled in. 

   When I've sat with an employee and gone through an employee evaluation process, and I file that away in their file, I don't think of that as mail or database.  I just think of it as, well I've now written this form and I've filled this in, let's put it away. 

   One of the big advantages of form processing is it gets rid of all the jargon of having to update your database, electronically send a piece of information from one person to another and put it in terms that people understand. 

   CC: Are you saying that the form is a good way to automate your business because it's what people can relate to?  

   Skapinker: It's proven.  Every business uses forms as a means of communication and can relate to a means of communication. 

   Part of the difficulty is that people on the entry-level, computer level really are looking for a means of communicating and using computers.  When they really got into word processing, which is what a lot of them do, it really just replaced the type-writer. 

   Why is WordPerfect the most popular word processor?  Because it's the most similar to a typewriter. 

   CC: Forms processing is a hot market for you now.  But what do you to do for an encore and how do you avoid becoming a one product company?  

   Skapinker: The software industry has shown over and over that positioning and market share is very, very important. 

   When we started off, our initial aim was to become the market leader.  Within a one-and-one-half to two-year period, we became the market leader both in the high-end and the low-end.  Our intention is to remain there.  We have a strategic relationship with IBM, that can maybe lead straight into that. 

   By creating a strategic relationship with market leaders, such as IBM, our credibility factor goes up, and our chances of remaining in the market as a market leader, remain very high.  Certainly IBM today owns 11 per cent of Delrina.  We are the only Canadian software company that IBM has invested in. 

   CC: Do you see yourself diversifying into different products or building on forms processing?  

   Skapinker: You will see us building on forms processing.  However, other related graphics business-type applications may become quite interesting to us.  Our long-term aim is to be a leading graphics business software Canadian publisher.  And whatever it takes to do that, you will find us doing.  Certainly, once an infrastructure is built, it will make sense for us to expand into other products.  </I
   Unmet needs  

   In our organization, we have two facets to our computing function. 

   On one side there are those hired to develop applications - they are programmer / analysts in the traditional sense.  The other side is there to support our expanding base of PC users.  We call them PC support specialists. 

   The last time I checked, both groups were running around furiously.  I'm not sure why but it must of had to do something with mission-critical applications, whatever that is. 

   The reason for my concern is I needed something done.  I wanted a certain kind of software loaded on my machine.  It's not a big request but I suppose it ranks about number 112 on their list of priorities. 

   At our company, there is probably an argument for having two groups responsible for our systems.  In theory, it should work. 

   But where the whole systems breaks down is this growing list of unmet end-user requests. 

   A co-worker needs a modem installed so she can work at home.  Another fellow wants to upgrade his PC with a color monitor.  A third is having troubles with a hard drive.  And to top it off, I have a boss who can produce a one-inch thick stack of printed e-mail messages each representing a request to the PC support supervisor. 

   These are requests that I know of, these are the ones from our department.  Now multiply that by all the other users in all the other departments and what you have is a rather large backlog. 

   Our IS function, such that it is, simply is facing more demands than it can satisfy.  And it is, however undeserved, getting a reputation for unresponsiveness. 

   I'm told, many, if not most organizations, are experiencing similar frustrations. 

   My concern is that IS has been so preoccupied with "serving" others, aiming to please but forgetting all that is promised, that I, typical end-user, have become reluctant to add to their list. 

   So what do you do?  The obvious answer is to add more resources and more people.  That's not popular in these days of cutting back. 

   A second would be to make users more self-sufficient but I'm afraid co-workers would resist any attempt to be transformed into "techies." 

   A third is to involve end-users more in the judicious use of resources that always seem to be limited and to help our technical people set priorities. 

   I think that maybe our computer department, or any IS organization, would be better off if it was run like a business. 

   This does not at all imply that you want to turn IS into a profit centre.  But what it means is doing a better job of telling your IS personnel exactly what is you do to make money - so they can discern between "strategic" use of their time and what is a meaningless digression. 

   And it means accountability, to other departments and to upper management, and depending on your type of business, to customers or suppliers as well. 

  - Martin Slofstra  </I

    Wang profits from change in focus  
  by Alison Eastwood 
  Computing Canada 

   After a turbulent 18 months beginning with the accumulation of $575 million (U.S.) in debt, Wang Laboratories, one of the most successful proprietary vendors of the '80s, has a new strategy - it wants to be liked. 

   "We got away from trying to be known to a core base of customers, and we just used our name to get to everybody who wanted word processing," admits acting president of Wang Canada, Vaughn McIntyre. 

   Most of the Fortune 1000 companies in Canada have at one time or another dealt with Toronto-based Wang.  However, the company's focus was on the computer marketplace rather than on the customer.  Even its marketing strategy was guaranteed to turn users off.  

  "We tended to have advertisements that other computer companies would understand and know but that our customers wouldn't.  It became a kind of one-upmanship on the platform."  McIntyre is adamant: "We've got to change that." 

   Regaining people's trust is central to Wang's new strategy.  McIntyre says the company will enter into new business partnerships but confesses: "Over a five-or six-year period, not unlike some of our competitors, we do have to admit to having built up excellent partnerships and then destroyed them with the stroke of a policy change." 

   At the end of 1989 Wang Canada's parent company, Wang Laboratories Inc. of Lowell, Mass., found itself in debt to the tune of $575 million (U.S.).  "We had a very severe cash problem," said McIntyre.  Restructuring the business appears to have paid off - Wang reported a very small profit in the first quarter of ended Sept. 30, 1991, and is hoping to build on that, although, as McIntyre is aware, "it's going to be touch and go for a while." 

   McIntyre attributes the turn-around to the refinement of Wang's business internally and not to the response of its customers.  "I would not want to trick anybody into thinking that people have flocked to our door and begun to buy just because we've cleaned up our act," he said.  "We expect that to come." 

   At Wang Canada, there have been major changes: director of marketing McIntyre had to step into the shoes of president Steve Trotter, who returned to his home country New Zealand four months ago.  "Big disappointment for him, because he was the basis for some of the changes we'd made here," said McIntyre.  "He wanted to watch some of them happen." 

   Changes include a focus on open standards and a push towards the image processing market.  Wang Canada has just announced Open/image for Netware, which integrates document image processing into Novell's Netware 386 network operating system, and Open/image-Windows 3.0, a new release of its image processing software for 286, 386 and 486-based PCs. 

   A major retailer in Toronto has chosen Wang to supply hardware and software, said McIntyre, "and that was a significant win for us.  All our competitors were in there." 

   The company has established an imaging centre in Winnipeg, in conjunction with the government of Manitoba, and is developing a document processing package called Upward, which will incorporate Windows 3.0.  Users will be able to create documents, "heaven forbid, on one of the other products like WordPerfect," which can be transferred back and forth.  Upward will be available within the next couple of months.  </I

     Promoting new image system is a family affair   
  by Alison Eastwood 
  Computing Canada 

   Take one film producer, a stockbroker and a software package developed by church-going computer wizards in New York and you have Canadian Interlinear Technology Inc. 

   If this sounds a little bizarre, so is the story of how this Willowdale, Ont.-based company and its flagship product, MEDIS, came into existence. 

   Founder of Canadian Interlinear Technology Angus Sullivan, an Irish Canadian, used to be in the film business.  Last July, on a visit to New York, Sullivan encountered a friend from church who had the documentation to a new software package called MEDIS. 

   "He said, 'This is some technology some friends of mine and some people in the church have developed, and would you like to see what you can do with it in Canada?' " recounted Sullivan.  "I said, 'What do I know about computers?'  Basically I'm a script-schlepper." 

   What Sullivan did was to take the manual back to Canada.  "I thought to myself, 'They tell me in New York that this is hot, hot, hot but what do I know?  How can I find out?  How can I prove it?'" 

   He attempted to interest investors, including one who had backed some of his films, but was told the same thing: go and see some people in the business and talk to end-users.  

   So Sullivan went to Boeing, Spar Aerospace, GE Aerospace and Litton Systems.  He knew he had hit upon something after he left the manual with one of the technical experts at Litton, "because when I came back he was caressing the manual.  I thought he was about to have some weird perverse type of relationship," Sullivan recalled.  "I thought, 'There must be something in this manual;' as an Irishman I can judge passion rather well." 

   MEDIS (Modular Electronic Document Image System) is a document image system with an open, client-server-based architecture that consists of seven software modules designed to run on Unix platforms.  It is compliant with the Computer-aided Acquisition and Support (GALS) initiative specified as a requirement by the U.S. Department of Defense for future defence contracts, which is why Interlinear is initially targeting the aerospace industry.  

   Sullivan showed the product to a chief engineer for one of the major aircraft manufacturers who, while impressed by MEDIS, was "absolutely terrified of the technology because it's his responsibility to bring his company into that kind of thing." 

   Still cautious, "having been in the film business a long time you get your hopes up a lot, and you get 'em smashed a lot," Sullivan contacted his cousin, Robert Angus German, founder of Geac Computer, who pronounced MEDIS "awesome" and advised Sullivan to recruit "some super-technotron" to sell the software. 

   On the strength of this another cousin, Clive Sullivan, quit his job as a stockbroker at Hector Chisholm & Co. in Toronto to sell the MEDIS package and finance the start-up of Canadian Interlinear Technology, along with "an old friend-of-the-family-type," W.K. Buckley Ltd. 

   The company was incorporated on Feb. 1 this year and the Sullivans have exclusive Canadian rights to the MEDIS technology as well as an option on 10 per cent of the American company, Interlinear Technology. 

   The main problem, (Angus) Sullivan admits, is the long lead time between starting up a business and selling your first unit, "which is a long way down the line; so, basically, this is not the type of investment that brokers would advise." 

   Still, it helps to have relatives in the right places.  And Sullivan has a lot of them.  "When the Irish Catholics got to Canada centuries ago they were like rabbits in carrot heaven, so they just went through the place and ate everything and reproduced randomly all over.  I've got cousins from coast to coast."  </I

     Help desk professionals ask for a little respect   
  by Andre Fuochi  
  Computing Canada  

   TORONTO - In its first meeting, the Toronto chapter of the Help Desk Institute got off the ground in what it hopes will become a forum for help desk and customer service professionals. 

   "Over the years help desk professional have implemented technology but it's never been truly successful.  At times we aren't even able to articulate the needs of our industry.  That's why we're here, to share our experiences and build from them," says Terry Garbutt, technical product support consultant for the City of Toronto. 

   "People within our industry are never given the proper recognition they deserve.  We need a forum to address the problems and concerns facing help desk professionals.  We didn't want to create another 'me-too' organization.  There are plenty of those around," he adds. 

   The chapter was formed last October and is the first Canadian chapter for the organization.  It plans to set up a branch in Calgary later this year.  The cost to join is $35 a year for members $50 per person for vendors. 

   "We want to give people who work on the front line an opportunity to participate.  Usually, it's the manager or the technical specialist who attends.  In this chapter, we're hoping to inform everyone from the manager to the people who answer the phones," says Gabby Winter-stein, manager of a customer support centre for the Bank of Montreal's customer services department. 

   "We don't need to re-invent the wheel.  We don't have to know each other's secrets but if a company is doing something right both in terms of procedure and technology, then why not share it with us.  It's a very pragmatic approach to sharing information," he says. 

   "There are people in our organization just starting a help desk and those who have one of 100 people or more.  People have a vehicle for getting together and sharing information, both from individuals within their industry and from other areas," Winterstein says. 

   The chapter is looking for input from members, and from those interested in joining, to get some idea of the issues that future meetings should address.  The next meeting of the Toronto Help Desk Institute will take place March 12. 

   For further information, call either Gabby Winterstein at (416) 398-8800 or Terry Garbutt at (416) 392-7077.  </I

    Stratus financial results clarified  

   The In conversion with Robert Gordon of Stratus Computer INC.


   Gates vows to remain in OS/2 camp  
 
 
  by Paul Barker 
  Contributing Editor 

   MISSISSAUGA, Ont. - Windows, multimedia and OS/2 will form the cornerstone of Microsoft Corp.'s development efforts during the next decade, the company's co-founder and chairman says. 

   Bill Gates' commitment to OS/2 follows a published report stating that Microsoft planned to abandon an operating system which has generated little interest since its introduction four years ago. 

   "The article was as wrong as an article can be," said Gates.  "We've got more people focused on OS/2 than we ever have.  Particularly when a product's in a position like this, we need to make it absolutely clear what our commitment is." 

   He told reporters here prior to the opening of Microsoft Canada Inc.'s new headquarters, that OS/2 remains the "highest priority" for his company. 

  "The story with Windows from 1983 until just a year ago, was that people said, 'come on, give up, why are you working on that?  It's not important.'" 

   "We had to come back in every speech and say this will be important," he said.  "We find ourselves in exactly the same position with OS/2 today." 

   And the success of Windows 3.0, the graphical user interface (GUI) that burst onto the scene last May, has been nothing short of phenomenal. 

   More than two million copies have been sold worldwide since the introduction and the number of Windows-based applications has soared to 1,000. 

   That figure is expected to double by next year, Gates said. 

   When asked if Microsoft planned to go head-to-head with Cupertino, CA's Apple Computer Inc. as part of a GUI war, he responded by saying "there's no doubt that if they rest on their laurels life will get tough for them." 

   "Macintosh users are fiercely loyal to the Macintosh," he said.  "Windows on the other hand has the advantage of being compatible with the majority of PC hardware."  

  "If you want a portable machine, I can show you 20 that are great Windows machines.  Arguably, there's one or zero great portable Macintoshes." 

   Gates also denied that as custodian of the DOS operating system, Microsoft holds an unfair advantage over other software developers.  "We've been a developer of operating systems and applications from the beginning of the company," he said. "There's nothing new about this.  

  "If we get an advantage by offering MS-DOS, did it help us in our applications, did it help us versus Lotus 1-2-3, did it help us versus WordPerfect?  If it did, I didn't notice." 

   Each application, said Gates, has to "stand on its own." 

   "I really don't see any correlation," he said.  "I'm always curious for someone to provide some example where as the provider of DOS and Windows, we get some advantage." 

  "...We went to Lotus and begged them to do Windows applications - actually signed a contract with Mitch Kapor which they later reneged on; we went to WordPerfect, Ashton-Tate and begged them to do it." 

   All three vendors, said Gates, will "recount how boring it was to have me come down year after year and say the Microsoft application strategy was to focus on Windows." 

  "There were a lot of other people who listened to that story - people like Aldus, Corel and Micrografx. ...  They are seeing the rewards of picking an environment that happened to succeed." 

   Of multimedia, he said, its greatest impact will come in "consumer-type applications.  Encyclopedias, sports, medicine, travel - all of this on a compact disc with sound and pictures and you get to choose what information you're interested in."  </I

    Industry not exempt from recession, say analysts  
  by Dianne Daniel 
  Contributing Editor 

   The 1990 recession has left its mark on the information technology industry despite strong financial showings from some companies for the period ended Dec. 31, 1990, say industry analysts. 

   According to Andrew Toller, a consultant with The DMR Group Inc. in Toronto, 1990 was "overall a bad year for the industry."  He says despite a few exceptions - relatively small specialized companies and those who made key announcements in 1990 - growth in the industry has slowed and he sees no signs of it bouncing back soon. 

   International Data Corp. (Canada) Ltd. (IDC) based in Toronto reports, "... Demand for information technology is likely to be more directly affected by the general economic climate over the next few years than recent history might lead one to expect." 

   According to IDC, the belief that the IT industry is independent of the overall business cycle is a myth.  Instead, the research firm reports that users' buying behavior is highly influenced by the economic climate.  "Evidence indicates information technology is as much a tar-get of current cost reduction efforts as are other expenses in the organization...it is reasonable to expect the recession will be a damper on IT industry demand." 

   Although Willowdale, Ont.-based Evans Research Corp. (ERC) predicts IT will "experience a slightly softer landing than the general business environment," it too projects slower growth for most segments in 1990 and 1991. 

   In its report Trends and Forecasts for the Information Technologies Industry, 1990-1994, ERC expected the software segment to grow 15.6 per cent in '90 and 15.7 per cent in '91, down from 16.5 per cent in '89.  The hardware segment was expected to show growth of only 6.3 per cent (including hardware maintenance) in 1990 compared to 7.5 per cent in 1989.  And the PC market was expected to slow from 19.8 per cent in '89 to 16.9 per cent in '90. 

   In addition to slowed growth, the industry also experienced its share of losses.  Beginning with B.C.'s Mission Cyrus which stopped production and laid off all but a few employees late in the summer of 1990, there were five no-table failures.  Canada Remote Systems Inc. went into receivership; Jonas & Erickson Software Technology Inc. was bought by Markham, Ont.'s Geac Computer Corp. Ltd. after filing an assignment in bankruptcy; Myrias Research Corp. laid off all of its staff and appointed a receiver; and, Tinton Falls, NJ-based Concurrent Computer Corp. ended the year by letting 90 employees go as it attempted to ward off a US$55-million debt. 

   Yet financial reports indicate that not all was gloom and doom in the industry for 1990. 

   Armonk, N.Y.-based IBM Corp. reported a 10.1 per cent increase in annual revenue in 1990, reaching US$69 billion.  Toller attributes the financial success to IBM's launch of its 390 architecture which he claims has boosted growth in the already saturated mainframe segment of the industry. 

   ERC's report echoed Toller's comments.  "While IBM is not the only mainframe manufacturer with new product introductions in 1990, it certainly will have the most significant impact on the high end of the market," states Evans. 

   Microsoft Corp. of Redmond, WA, also weathered 1990 well.  The PC software developer reported a 55 per cent increase in revenues for the six months ended Dec. 31, 1990, compared to the same period the year before. 

   The company's performance isn't surprising considering the increasing demand for Microsoft Windows 3.0.  A report from Computer Technology Research based in Charleston, SC, has Windows 3.0 performing well in the short term although it questions its long-term impact. 

   Riding the wave of the Windows 3.0 announcement was Ottawa-based Corel Systems Corp. with its popular PC presentation package CorelDraw, version 2.0.  The company's fourth quarter, ended Nov. 30, 1990, was its best ever posted with net sales hitting $12.2 million, an increase of 56 per cent over 1989's fourth quarter.  Net income for the year was $7 million compared to $2.9 million in '89 . 

   Toller claims the company's success is largely due to the fact that it was prepared for Windows 3.0 and was able to ride the first wave of compatible products. 

   Houston's Compaq Computer Corp. and Cupertino, CA-based Apple Computer Inc. are two PC hardware vendors who performed well in '90 .  Compaq's net income increased 70 per cent to US$135 million for the fourth quarter compared to US$79 million in 1989.  Apple recently posted revenues of US$1.676 billion for its first quarter of fiscal 1991 ended Dec. 28, 1990 - up 12 per cent from the year before. 

   Toller says Compaq experienced about a 25 per cent growth, largely due to successful sales worldwide - specifically in Japan and Australia.  The company also introduced nine well-received products in '90 in the areas of 386SX-based desktops, notebook PCs and PC systems. 

   Apple attributes its outstanding performance to the Macintosh Classic released this year with a reduced entry-level price.  The company claims Macintosh unit shipments have increased by 50 per cent as more first-time users are being attracted by the lower price. 

   And Silicon Graphics Inc. of Mountain View, CA, may have found its niche.  Net revenues for the workstation vendor totalled US$136 million for the quarter ended Dec. 31, 1990, an increase of 32 per cent over revenues for the same period a year earlier. 

   In its trends and forecasts report, ERC predicted 28 per cent growth in the technical workstation market for 1990 as awareness among users continues to grow.  Silicon Graphics was also a notable addition to ERC's The Top 200 IT Companies released last year. 

   In general, a survey of financial reports shows the strongest gains occurring in the hardware segment of the industry: Conner Peripherals Inc. of San Jose, CA, reported a yearly net income of US$130.1 million, up 213 per cent from 1989; Mountain View, CA's Sun Microsystems Inc. reported second quarter revenues for the period ended Dec. 28, 1990, of US$753 million which is 27 per cent higher than a year ago; and, Stratus Computer Inc. of Marlboro, MA, announced fourth quarter 1990 revenues of US$118.5 million, up 21 per cent from '89 . 

   Software companies surveyed recorded the lowest gains.  Legent Corp. of Vienna, VA, expects revenues of US$51.7 million for the three months ended Dec. 31, 1990, compared to US$40.7 million the year prior; El Segundo, CA-based Teradata Corp. reported revenues of US$111.2 for the six months ended Dec. 31, 1990, up from US$97.6 million a year earlier; and, Informix Corp. of Menlo Park, CA, plans to restructure its operations after announcing an expected loss for the fourth quarter and year ended Dec. 31, 1990. 

   Restructuring and layoffs were prominent throughout 1990 as many IT companies struggled.  But Toller is quick to point out that restructuring isn't a response to recessionary times, but rather a response to a changing industry or market. 

   Wang Laboratories Inc. of Lowell, MA, has shown that sometimes it's not too late to turn around.  The company showed an operating profit of US$4.9 million for the six months ended Dec. 31, 1990, compared to a loss of US$38.4 million the prior year.  Net loss for the same period was down from US$72.6 million in 1989 to US$22.1 million in '90 . 

   The company's focus on imaging and related products can be credited for its financial gains.  Wang claims unit sales for its imaging line for the first five months of fiscal 1991 exceeded those for the entire 1990 fiscal year. 

   A stronger focus on niche markets has also helped Westborough, MA-based Data General Corp. weather the recession.  With a major product announcement scheduled for later this month, the company has posted first quarter 1991 profits of US$12.5 million, up from a loss of US$20.5 million at the same time last year. 

   Data General says the turnaround is due to reduced costs, a reorganized sales force and a strengthening of focus on niche markets like health care, geographical information systems and imaging. 
 </I

    Gandalf unveils government LAN strategy  
  by Alison Eastwood 
  Contributing Editor 

   NEPEAN, Ont. - Gandalf Technologies Inc. is aiming to consolidate its share of the government market with a range of 10Base-T connectivity products. 

   The company has developed 10Base-T-compliant modular wiring hubs, called Access Hub 12, 48 and 132, that are designed to link up local-area network (LAN) node devices such as PCs and fileservers.  The Access Hub series coexists with the 10Base-T-compliant mini media access unit (MAU) transceiver, announced last October, and the network interface card LANLine/AT, introduced in January. 

   An Ethernet LAN wide- and local-area networking bridge, multi-protocol wide-area router and SNMP/OSI network management product will be available in May. 

 "The 10Base-T (wire transmission) standard has been issued as policy ... on the way the government wants to install structured wiring systems," said Paul Hession, Gandalf vice-president of Canadian sales.  "We're well-positioned to exploit that." 

   Gandalf already sells into federal government departments such as Health and Welfare, Public Works, and Agriculture.  Hession also predicts expansion in commercial sectors such as finance, insurance and communications (existing customers include Wood Gundy, Telecom Canada, and the CBC). 

   The 10Base-T line extends the company's terminal interconnection and wide-area networking product range based on its Starmaster network processor, which incorporates standards such as X.25, SNA and TCP/IP. 

   The difference with Access Hub is that users can have access to PC data via a LAN, whereas PCs only fulfilled a terminal emulation function under Starmaster. 

   Director of Gandalf's LAN business unit, Peter Burke, said customers had begun to demand PC support, "so we can't ignore it.  It bodes well for a good combination - if we get there in time." 

   On the 10Base-T side, Hession sees an "opportunity" of $600 million worldwide, growing to $2 billion within the next two years.  "It could very well represent, to the Canadian marketplace, 10 per cent of our revenues within 10 months of this launch. " 

   Access Hub uses unshielded twisted-pair wiring and Hession claims that, as technology has evolved, "the data rate we can support ... is 10 million bits per second, which was unheard of five or 10 years ago."  </I

    Crowntek picks ALR "alternative" 

  by David Tanaka 
  CDN Staff 

   Markham Ont.-based Advanced Logic Research (Canada) Ltd. (ALR) has signed a national reseller agreement in Mississauga, Ont.-based Crowntek Business Centres Inc. 

   Crowntek president and CEO Stewart Davis said the company has been looking for a third PC line for about a year, mainly in response to softening brand loyalty amongst corporate PC users.  </I
   CONSISTENT COLOR PRINTING  
  BY RON JEGERINGS

   Making good prints from color negatives isn't difficult.  Given a decent darkroom, a weekend can be spent making attractive prints.  Your level of production may be low, however.  We all dream of higher output - if for no other reason than to churn out enough photo Christmas cards for the season without staying up half the night. 

   In your quest for increased output remember the two C-words: conservative and consistent.  While printing with the speed of a commercial photofinisher is an impossible dream for studio and home darkrooms, there is a lot you can do to improve. 

   First, examine your film habits.  Amateurs tend to try all the new films, while professionals pick a few and stick with them.  A pro will also buy in bulk from the same emulsion batch and cold store all but those for immediate use in order to ensure exact color balance. 

   For experimenting, expose a variety of color-negative films, using the same light and subject.  Avoid trying every film in the store since some may require different enlarger filtration for neutral color balance. 

   Like film, it's wise to buy a large quantity of enlarging paper from the same emulsion batch.  This cuts down on the number of sheets wasted every time you must fine-tune color balance for a new batch.  Consistent paper processing isn't a chore at moderate temperatures.  A simple drum system with a water pre-soak usually does the job, and a water bath for chemistry bottles can be controlled by adding hot or cold water. 

   But low-temperature processing is slow.  Reducing time with higher temperatures, however, may require the use of a table-top processor, or a system that rotates the drum in a temperature-controlled water bath during processing.  Otherwise, temperature drift will have you constantly fiddling with enlarger filtration.  Shorter processing times demand that solutions flow quickly over paper.  A drum system (like a Jobo with an optional Lift) that allows quick solution exchange can also help.  Some printers will compromise between room temperature and the recommended maximum temperature, which decreases humidity to make the process easier. 

   Once you pick a temperature, stick with it.  Change the temperature and you may change color balance and density.  Trying different color chemistries can also alter color balance, as can using a water pre-soak before development for one session and not for the next. 

   Be consistent in all things including the direction you turn enlarger filter dials.  Take your pick, either clockwise or counter-clockwise, but always turn them in the same direction for equal tension and repeatability. 

   The color temperature of printing light should also be consistent.  Tungsten-halogen bulbs in dichroic color enlargers are more stable than household tungsten bulbs normally found in condenser enlargers.  When buying a dichroic color enlarger get a stabilized power supply to compensate for line-voltage fluctuations.  They cost more than simple step-down transformer power supplies sold with some color enlargers, but you will recover the cost in saved paper. 

   While tungsten-halogen bulbs are expensive, buying two or three at a time is cheaper in the long run.  If they are from the same manufacturer, chances are good that only slight filtration changes will be needed when one blows.  And if changing bulbs in your enlarger is fairly easy, you may wish to keep a separate bulb for each color process: one for printing from color negatives, another for making duplicate transparencies.  Using up a bulb to make black-and-white prints after you went to all the bother of doing ringaround tests to establish color balance is foolish.  A few electronic color heads automatically correct color balance, and additive printing systems can compensate for bulb variations. 

   A color analyzer can speed up production by metering exposure and filtration, but isn't like a camera meter in simplicity.  To make a print with correct color balance and density, program the analyzer for the filtration and exposure used.  If you have a master negative that can monitor printing and processing, and it prints with different filtration from session to session, you will spend more time resetting your analyzer than using it.  It's time to analyze technique and see how consistency can be achieved.  Even consistent printers may have to tweak an analyzer program slightly at the start of a session to overcome such problems as chemistry aging, so don't add avoidable problems.  The advantage of test strips and test prints is that they compensate for day-to-day system variations. 

   Once programmed, an analyzer should compensate for color shifts caused by myriad factors: shooting under different lighting conditions, using different enlarger mixing boxes or different bulbs, bulb aging, a white-light lever that doesn't always return dichroic filters to the same position, enlarger lenses with slightly different color correction, and much more. 

   Most color analyzers have easel probes that offer a choice of spot or integrated readings to compensate for varying negative density and magnification changes.  In the integrated mode the same easel probe is used, but a diffuser under the enlarger lens scrambles tones together so only one reading is required. 

   Spot readings are more precise, and like camera spot readings demand more of the operator.  You have to program specifically for the color being metered.  Write a program for flesh tones and not only will it attempt to make grass the color of that flesh tone, it will try to turn all flesh tones the same color. 

   For this reason, most photographers program for a grey tone; it's a good idea to have the image of a standard 18-percent grey card, like Kodak's, in your master negative.  You can use this to set up the analyzer, then look for equivalent tones in mixed negatives, such as pavement and highlights on dark clothing. 

   Since accurate judgement is called for, some photographers use only spot probes when there is time, to place a control grey card in the scene (at the shooting stage.)  Wasting a negative sometimes saves printing time. 

   Exposure errors occur in integrated readings when the negative has large light or dark areas. (They also depend on a mix of scene colors adding up to neutral density.)  Take a picture of a white dog against a blue sky and you will print a yellowish dog as the analyzer tries to correct for the excess blue.  One factor that aids integrated readings which depend on a mix of scene colors adding up to neutral density is that many colors are closer to neutral than we imagine. 

   So, pick a few films and stick with them.  While in theory you program the analyzer only for the paper in use, some have to be programmed for the combination of paper and negative.  The fewer negative-paper programs you have to write - or the fewer small adjustments that have to be made - the more time you spend making prints. 

   Analyzers can be slow.  With subtractive enlargers, changing one filter setting can affect another channel and you may have to go through the filter settings two or more times to eliminate cross-talk. 

   Making up for this slowness is the fact that one analyzer can be used with different enlargers.  Buy a self-contained electronic system, like some of Durst's enlargers or a Beseler/Minolta 45A head, and production increases but you are tied to one machine and perhaps one format.  If this isn't a problem, and you have a lot of spare change, remember that systems vary functionally and require individual research. 

   What holds true for all systems is the need for consistency.  Skip all over the map trying new chemistries, films, processing techniques and equipment and you'll spend more time programming fancy equipment than making pictures.  </I

    ILFORD 400 DELTA  
  By Henry Gordillo 

   Ilford has introduced a new fast, fine grain black-and-white film: 400 DELTA.  In its technical information brochure (#9446), Ilford promises that DELTA is "ideal for pictorial and fine art photography."  They claim that "prints made from 400 DELTA film exposed at EI 400/27 have very fine grain and outstanding sharpness" and "the results are similar to those expected from a conventional medium speed black-and-white film." 

   Interest in DELTA is high, and its arrival has been eagerly awaited.  Ilford has always maintained an unwavering allegiance to black and white, and I thought they might be able to solve the problem of producing a fine grain version of Kodak Tri-X which continues to be my favorite black-and-white panchromatic film.  I like the quality of the Tri-X image and the fact that dodging/burning and bleaching can bring difficult to print areas of the print in line with the entire image. 

   400 DELTA is Ilford's response to Kodak's T-grain technology. 

   EVALUATION  

   T-Max films are easy to overexpose and overdevelop with the result that grain structure in the affected areas is scrambled and will not print image detail.  So while T-Max is a very fine grain film, it is also a very fussy film best used under extremely controlled studio situations and processed by a properly tuned machine.  Ilford Core Shell Crystal technology on the other hand seems to avoid these problems while still producing fine grain. 

   Reviewing DELTA opens up a lot of good, interesting questions about testing and using black-and-white film.  400 Delta is a beautiful film but it doesn't test out at its rated film speed.  It is really a medium-speed film and a very good one.  The film is crisp, clear, prints beautifully, and has tiny grain.  It compares well with Agfa 25, the slowest black-and-white 35 mm film currently available.  However, it doesn't test at a consistent 400 ISO.  If you want to be sure to capture detail in the shadows, it tests at 100 ISO in practice. 

   THE TESTS  

   Here's what I do to test a black-and-white film.  First, I double check the manufacturer's speed rating for the film.  I do that to protect shadow values.  If the film speed is set incorrectly high, then the shadows run a good chance of not being recorded.  If the speed is set too low, then the film receives unnecessary exposure which can result in increased grain and loss of detail in the whites.  Secondly, I test for the proper development time for the film.  The film needs the development which will allow an exposure three stops above what the meter indicates to print as white with just a little bit of detail.  There needs to be some separation between anything that receives that exposure and the paper base.  Thirdly, I determine the exposure the film needs for a multi-toned subject (including blacks in shadow and whites in sunlight) on a sunny day with sharp-edged shadows.  Among other things, this is a way to compare one film to another under actual shooting conditions.  Lastly, I see how the film responds to a variety of situations and double check my results by using the film a lot.  I need five to 10 rolls of the same emulsion batch to run the tests.  One for starters, then maybe another two to figure out the film development time, and the rest to use in practical picture taking to double check the results. 

   These tests are definitely not just for lab technicians.  These are practical tests that anybody who uses black-and-white film should do. 

   TEST RESULTS  

   Ilford recommends two of its own film developers: Ilfotec HC (a liquid) diluted 1:31 and ID-11 (a powder) used undiluted in its stock solution form. (Literature from Ilford U.S. mentions ID-11 PLUS developer which is not available here.  We get regular ID-11 from England.  According to Ilford Canada these two developers are different!)  I used the Ilford developers in addition to my regular film developer Kodak HC-110 diluted 1:31. 

   With all three developers, the film rated at about 100 ISO using tungsten lights.  Had I used daylight, which is more actinic, it would have rated at about 160 ISO.  HC-110 consistently gave a bit more density than the two Ilford developers.  All three developers produced negatives that look much better than medium speed negatives.  Printers produced from them have excellent tonal separation ranging from deep black with detail to light whites just barely separated from paper base.  </I

   LIGHT MAGIC  
   The fibre-optic Hosemaster makes painting with light a photographic reality.  
  By Chris Knowles 

   Behind a studio door the pop, pop of a commercial photographer's army of strobes has been replaced by an odd clapping sound and what sounds like the beep of a truck's backup warning signal.  Inside, Pat LaCroix is bouncing around the set, waving the source of these new and unusual sounds - the Hosemaster - over and around and under a saxophone player and the tool of his trade.  The Polaroid is held under the light of the studio's kitchen stove: the image is remarkable.  The bell of the sax (from whence the sound emerges) glows with hazy highlights.  The player himself is awash in diffused light.  And the difficult exposure balance between the player's face and the gleaming brass instrument has been pulled off perfectly.  What's equally striking is how the image seems to be sharply defined in one part and deliberately hazy in others. 

   LaCroix, a professional photographer for 25 years, is clearly pleased.  "I've been shooting a long time, so it's important to get a little creative buzz every now and then. When I started using the Hosemaster, I began to think, 'Hey! I could use it for this, and this and this....' " 

   The Hosemaster, to hear Hosemaster afficionados such as LaCroix, Peter Horvath and Ottmar Bierwagen describe it, is an innovative new creative tool that may change the face of studio photography.  All three Toronto-based shooters are using it, for both corporate and personal work.  "I have a feeling it's going to become a standard," says Horvath.  "It's definitely right up there on the 'new' list." 

   "I was just astonished by the thing," adds Bierwagen.  "I think it's the most interesting piece of lighting innovation to come along in 20 years." 

   Horvath makes a "vacuum cleaner" analogy; it's an apt description for the Hosemaster, invented by American photographer Aaron Jones, looks just like a pull-along vacuum.  Instead of sucking in dust, however, it beams out light through a 4.6-metre (15 ft.) length of fibre-optic cable.  One end of the cable plugs into a rolling power-supply case, which holds a fan-cooled quartz-arc 5500 K lamp.  The business end of the cable is capped by a rectangular-shaped plastic housing about the size of a small household paintbrush, onto which light-shaping masks or gels can be clipped or Velcroed. 

   Holding this black "paintbrush" by hand, the photographer moves around the set, brushing a stroke of light here, dabbing a pinpoint beam there.  Horvath says it's the closest he's ever come to "feeling like a painter." 

   The Hosemaster differs from other fibre-optic lighting systems, such as the HMI (in existence for 15 years), in that it has its own shutter system, which is mounted on a stand just in front of the camera lens.  The shutter is operated by a button on the paintbrush; another button causes a filter holder (usually fitted with a soft-focus filter) to swing up in front of the lens.  To create a picture, then, the studio is made dark, the camera is set for a time exposure, and the photographer uses the paintbrush to illuminate various parts of the scene, clicking the noisy shutter (the source of that clapping noise) open and closed as he paints with the light, swinging the filter back and forth to make some highlights soft, some hard.  A beeper sounds at one-second intervals while the shutter is open to help the photographer time each exposure: a half-beep wash for the saxophonist's face (so he's not restricted to stay perfectly still), four beeps for the sax's bell, and so on.  And on.  LaCroix's masterly photographing of an otherwise near-impossible publicity shot of his friend Al Clootens, consisted of three different exposures: an initial wash with a wide-open Hosemaster diffused; gold gel clipped into the Hosemaster which was then stuck inside the sax's bell and exposed for four seconds; a half-second exposure with the Hosemaster outside the sax.  Once the initial wash is over, and the face is properly exposed, it doesn't matter if the face moves at all, since these are all separate exposures. (The glow around the sax player was from a diffuser placed in front of the lens.)  "It's a lot of mental gymnastics," admits LaCroix. 

   What LaCroix and others love about the system is the lighting accuracy it affords.  "You can really isolate the light," says LaCroix.  "Even with the most spectral of strobe spots, you can really only get so small.  I'm always playing around with shaving mirrors (to get a narrow beam), but even then you still get spillover." 

   LaCroix pulls out a picture of a collection of small dolls, each of which was lighted individually and differently.  The face of a very ugly, old French hunchback doll was lit from underneath to enhance the sense of ugliness, while a delicate porcelain doll was given a softer wash of light with diffusion.  The face of one of the dolls is pin-sharp, the others slightly diffused.  "You can make one face focused and one diffused with a strobe, but you'd never be able to get the different qualities of light with any other type of lighting," he points out. 

   Because the lighting is applied by hand, and the exposure timed by ear, invariably each frame is slightly different from the one before, and each truly the creative product of the individual holding the "brush."  This unpredictability in the Hosemaster's character is something Ottmar Bierwagen enjoys.  "Every image you get out of it is unique, and there's a little bit of a thrill in that because you sort of don't know what's coming."  Bierwagen created the still life shown here by starting with a strobe exposure of two stops under what his meter read, followed by about 15 exposures using the Hosemaster.  Holding the light close to the flowers, he painted the stems with a slashing motion, and then waved the light inside the buds for about three seconds each.  Next, he projected the light through the back of the vase for about 20 seconds to give it "just a small hint of translucence."  Using a mask with a tiny opening for the light beam, he then circled the light around one side of the pear and red marble.  "You can't control strobes well enough to give you that many spots of light; you'd have to have 35 snoots of various kinds all over the place." 

   Constant movement of the Hosemaster "paintbrush" (and the photographer's body parts) is necessary to avoid imprinting a ghost silhouette on the film.  And, of course, the light mustn't be aimed directly at the lens. 

   A concern of some photographers is that the "Hosehead look" will too soon become a cliche?  Peter Horvath doesn't agree.  Initially skeptical of its potential, Horvath soon fell in love with the versatility of the system and the hot neon colors the light produces.  "I'm seeing a real saturation I wouldn't normally get with strobes," he says.  "I feel like I'm on to something with it: a really new way of shooting." 

   Horvath's "new mutant art form", as he calls it, springs from his technique of not using a base strobe exposure at all, but instead relying on the Hosemaster - usually heavily gelled - for all of an image's light.  "One disadvantage," he says, "is that I have to remember what I did - how long I exposed each area.  I'm trying to develop it into an intuitive thing; I'd like it to come more from my gut than from my head." 

   This kind of innovation doesn't come cheap, however.  Vistek in Toronto, Canadian agent for the Hosemaster, is asking $8,695, or $150 per day on rental. 

   LaCroix bit the bullet and bought one, but both Bierwagen and Horvath are still reeling from sticker shock.  The cost is bound to come down, though, as the Hosemaster's popularity spawns imitators. 

   And most feel the Hosemaster will catch on quickly, as photographers and art directors continue to experiment with a new way of painting with light.  As Ottmar Bierwagen notes, "The style in photography for the last 10 years has been to haul out that 10-foot bank of light and whap through two or three or four thousand watts and make it nice and even and smooth from edge to edge.  I think that style is changing, thanks in part to younger photographers who are using hot lights and shadows and are breaking all the rules.  This Hosemaster kind of lighting system is one more tool of change."  </I

    LIGHT LOSS  
   Light is like Shangri-la - it's there, you just have to know where to look for it.  
  BY RON JEGERINGS

 

   For both amateurs and professionals, one of the great photographic puzzles is the way in which light loses intensity over distance.  Logically, moving a light from two metres away to four metres from the set should halve the intensity.  Logically, you open the lens one stop.  But your light meter says two stops and the aperture is too large for depth-of-field.  Move the light closer and illumination is too uneven across the set.  You become like the proverbial cat chasing its tail. 

   If light loss isn't logical, how can anyone gain control?  Fortunately, a logical pattern is described by the inverse square law.  The mathematics are basic, and once the concept is understood you can chuck the pocket calculator. 

   Though terms like light loss and falloff are convenient, they are misleading.  Light isn't lost over distance, just dispersed differently.  Darkroom workers will understand dispersion.  As an enlarger head is raised, more of the easel is illuminated, but it is merely the same amount of light spread over a larger area. 

   The key, technically speaking, is the inverse square law.  It doesn't matter whether you use feet or metres to calculate, though metres may be too broad and centimetres too minute, for the principle of the inverse square law still holds true: l/2 2  and 1/4  2 = 1/4 and 1/16 

   Light intensities are 1/4 and 1/16 at two and four metres respectively, which explains why the meter recommends two stops instead of one stop more exposure at the longer distance. 

   What happens if we make 1/4 and 1/16, f/4 and f/l6?  Away go the math calculations when working under pressure.  You can match distances to aperture numbers simply because aperture light control is related to the inverse square law.  One side of a set might be four feet from the light (f/4) and the far side eight feet (f/8).  Two stops of light are lost, which is only acceptable for a special effect. 

   Understanding the inverse square law is essential for understanding light in photography and for solving all sorts of lighting problems such as how to achieve even illumination. 

   Lighting problems are inevitable it you do not understand the difference between uneven illumination and high contrast - an easy misunderstanding since uneven light can be almost as contrasty as light that casts deep shadows.  To cure uneven light, a photographer might add a fill light at the camera, which fills in the shadows but still results in uneven light by illuminating both sides equally. 

   Understanding how much light is lost at near distances helps when on-camera flash causes washed-out foregrounds and black-hole backgrounds.  Black backgrounds may not fill a shot - if they add drama or obscure unwanted objects - but washed-out foreground faces are deadly.  When space allows, try a longer lens and shoot from a greater distance for more even illumination. 

   Bounce flash isn't used in color as it is in black and white, for off-white ceilings and walls can create oddly-hued complexions.  Even so, when the bounce surface is off-white (or near-white without the cold tint of green or blue), bounce flash increases light distances (from light to bounce surface and then back to subjects) and illumination is more even.  The only stipulation may be to use a fill card on the flash head to open up shadows cast by bounce lighting.  Moving a light far enough back requires unacceptably large apertures, or the knocking out of a wall.  The solution is to use fill cards on the opposite side of the set to bounce the light. 

   Again, there is confusion between the contributions of even light and fill in lowering contrast.  A large source - such as ceiling or wall with bounce flash creates its own fill to lower the contrast.  Therefore, there is a tendency to attribute contrast reduction solely to source size and discount the contribution of even illumination. 

   An umbrella is a large source which provides its own fill and lowers contrast.  When used for bounce flash it also reduces light fall-off since the light travels up the shaft before returning to the subject, increasing light distance.  Some photographers call umbrellas wall extenders, for they are particularly helpful in expanding small rooms. 

   Feathering the light is another trick.  Since most the lights have a brighter central hot spot in the illumination patch, you can aim a light in such a way that bright central rays travel farthest, thus creating more even illumination.  Some lights are difficult to feather.  When there's no visible hot spot and no intense central light, it's not so easy to know exactly where to aim the centre of the light to achieve even illumination.  </I


 <I
  RCMP using new technology to pull fingerprints from plastic 

   Detecting fingerprints on plastic bags crammed with illegal drugs has been about as easy as getting a crime lord to declare the source of his income to the tax man. 

   But innovative technology using fumes from super-strength glues in a vacuum chamber is giving Royal Canadian Mounted Police investigators clear fingerprints from plastic sandwich bags and other materials that normally yield little evidence. 

   Developed by a scientist at the National Research Council in Ottawa, the technique produces quality prints faster than methods usually used by police.  And it is also safer. 

   RCMP Inspector Richard Shaddick, head of the Mounties' science and technology branch in Ottawa, says prints don't stick well to plastic bags, bottles or porous materials. 

   Methods including dusting will continue to be used by investigators, but the vacuum-chamber method is providing an important new tool in crime fighting. 

   Fumes emitted by cyanoacrylate, the active ingredient in the extra strength glue, have been used for more than a decade to detect fingerprints.  But the material has to be heated to produce the vapors that enhance the ridges on prints. 

   That process requires bulky equipment, often produces imperfect prints and increases exposure to noxious fumes. 

   About a year ago, John Watkin, a scientist on contract to the NRC, experimented by using a vacuum chamber to lower the atmospheric pressure so the cyanoacrylate boils and vaporizes at room temperature. 

   This method, says John Arnold, Watkin's supervisor and manager of the NRC public-security program, produces superior fingerprints. 

   The development is the first for the Canadian Police Research Centre, an Ottawa-based joint venture of the NRC, RCMP and the Canadian Association of Chiefs of Police. 

   The cylindrical vacuum chamber is about five feet long and one foot in diameter - large enough to hold dozens of plastic bags or a weapon as big as a 12-gauge shotgun. 

   When fingerprints are exposed to the cyanoacrylate vapor, a polymer coats the ridges.  The item bearing the prints is then removed from the chamber and dipped into a fluorescent dye solution. 

   Prints are then viewed under a portable fingerprint lamp, also developed for the NRC by Watkin.  Under the Luma-Lite, the prints show up as a brilliant yellow on a black background. 

   Arnold says 10 print units are being shipped to RCMP field-identification centres across Canada for testing.  One has been used since November at the RCMP forensic identification laboratory in Ottawa. 

   Arnold said the units cost about $50,000 to develop and could sell for $10,000 to $20,000 each.  He said the NRC is interested in licensing the technology to companies that will manufacture and market them. 
 </I

 <I
  Danish firm testing new system to cut acid rain-producing gases 

   A less costly way of cleaning up acid rain-producing gases emitted by power stations and other industrial systems will be tested in the U.S. 

   Danish firm FLS Miljo will install its gas-treating system at a Kentucky plant for a test that will begin next year. 

   FLS Miljo is a subsidiary of the Danish company FLS Industries - world's largest maker of cement-production equipment - and a leading maker of cement kilns and industrial hardware. 

   Miljo has been diversifying into environmental control systems, including equipment to remove sulphur dioxide gas pollution. 

   As part of Miljo's strategy to produce new clean-up equipment for industrial gases, an experimental system is to be installed next year at a power station in Shawnee, Ky., a test plant run by the Tennessee Valley Authority. 

   If the Danish equipment operates as planned, it could be scaled up to provide equipment for large-scale power stations. 

   The U.S. Department of Energy is paying for the US$4-million (C$4.7 million) filtering system at Shawnee. 

   Removing acid gases from industrial equipment is crucial to reducing acid rain and other pollution.  It is generally accomplished by scrubber systems that pass flue gases through a water-based slurry of lime.  The process turns sulphur dioxide gas into solid calcium sulphate. 

   The new system from Miljo uses the same principles as traditional scrubbers developed by several companies, including Mitsubishi of Japan and Sweden's Flakt, part of Asea Brown Boveri. 

   Like conventional systems, the FLS hardware uses lime - a mixture of water with calcium oxide or calcium carbonate - to react with the gases. 

   But Miljo's system reduces the size and expense of the process by changing the design of the chamber in which the chemical reaction takes place. 

   Lime is directed at the gas stream as it moves up a vertical column.  Conventional scrubbing systems spray lime using a massive chamber in which the gases move horizontally. 

   The Miljo design ensures that the gases stay in contact with the calcium oxide longer.  It also recycles some of the lime. 

   The equipment includes hardware to filter out ash and dust.  It recirculates some of this material along with the lime, which aids the chemical reaction by reducing the acid content of the flue gas. 

   The overall impact of the new design, says Erik Hoffmann-Petersen, Miljo president, is to reduce the size of the complete system. 

   That means a Miljo system can be installed for two thirds of the price of a traditional scrubber. 

   Hoffmann-Petersen says Miljo's gas-removal system could be adapted for use by operational power stations and would cost US$10 million to US$20 million. 

   Versions of the clean-up equipment that will be installed in the U.S. already are being used at six Danish municipal incinerators. 

   The systems are relatively cheap and remove sulphur dioxide and other gases from the waste streams of these plants. 

   The importance of the Kentucky project for Miljo is that it could provide a big break in the large North American market for acid gas removal equipment.  Miljo already derives 25% of its revenues from the U.S. 

   As part of efforts to expand operations in the U.S., Miljo recently bought Airpol, a small environmental-control company based in New Jersey. 

   Besides new acid-gas clean-up hardware, Miljo sells conventional scrubbing equipment that is based on Mitsubishi's design.  Miljo also sells electrostatic precipitators - systems that remove dust and other particles from large industrial plants such as cement works  </I


   New software helps treat bone disorders 

   People come in all shapes and sizes.  Some reach their adult height when they're 12 years old, while others don't spurt up until they're 15 or more. 

   People with the same chronological age can have markedly different bone ages.  Accurately assessing someone's bone age is critical in treating growth abnormalities or skeletal deformities, such as scoliosis, that require hormone treatment or surgery, says University of Calgary medical researcher Stuart Coupland. 

  UNEVEN GROWTH RATES 

   He gives an example of a 14-year-old whose left leg is growing faster than the right.  An orthopedic surgeon needs to assess how much more growth remains in the normal leg to determine how big a piece of bone to cut out of the abnormal leg to ensure they'll both end up roughly the same length. 

   Coupland, a neural physiologist who dabbles in computer programming in his spare time, has developed a software program that improves on the speed and accuracy of methods currently used to assess bone age. 

   The method favored by radiologists compares an X-ray of a hand with a compilation of X-rays of hands of known age.  It takes about five minutes, but is only accurate within a two-year period.  What's more, it indicates the age of a person's hand bones, which can vary significantly from the bone age of the same person's leg or hip, says Coupland. 

   A second method, called RWT after the three researchers who devised it 15 years ago, uses 37 measurements of the knee-joint to assess bone age of the lower extremities.  Though accurate to within six months, it takes an hour or more to complete the assessment. 

   Using a Microsoft Quick Basic language program and a MacIntosh II workstation, Coupland has computerized the RWT method to get an answer in less than 10 minutes. 

   "We had to make the RWT method practical in a clinical setting without losing the accuracy," says Coupland.  "Orthopedic surgeons say they are happy with accuracy within six months. When it's one or two years off, they're concerned." 

   Coupland used a video digitizer to transfer X-rays of knee joints on to a computer screen.  Based on the patient's gender and age, the computer decides how many of the 37 measurements of the joint have to be taken to calculate the correct bone age. 

   The program, called COMRAD, for Computerized Radiographic Age Diagnosis, leads the user through the steps to take the measurements off the screen and calculates the bone age. 

   Coupland is enhancing the program to allow users to save the picture and measurements, probably on an optical disk, for future use.  He has applied the same technology to measure abnormalities in facial and head bones.  These craniofacial distortions, which are caused by genetic syndromes, are often diagnosed visually making treatment less exact, he says. 

  TESTING IN CALGARY 

   COMRAD is being marketed by University Technologies International Inc., a technology transfer company, which is a wholly owned subsidiary of the university.  UTI marketing manager Kathy Chan said COMRAD is being tested at a Calgary hospital and plans are under way for tests in the Toronto area. 

   Coupland holds the Canadian copyright for COMRAD and has applied for U.S. copyright.  Any profit he makes is split with the university, which funds UTI.  He hopes to sell the software to various hospital radiology departments and research labs.  UTI is doing a survey to determine the appropriate market and pricing for the product.  </I


   Canadian firm has global vision for robot vacuum cleaner's launch 

   Cyberworks Inc., the tiny Canadian company causing a stir with a revolutionary robot for the contract cleaning industry, is pursuing, after just one sale, European and North American markets. 

   "By the end of the summer we're hoping to get Europe launched and then North America by the end of the year," Vivek Burhan Parkar, the 26-year-old founder and president of the Orillia, Ontario firm, said in an interview here. 

  300 UNITS SOLD 

   Cyberworks' first customer is Hudon Groep Internationale of the Netherlands, one of Europe's largest service contractors. 

   Hudon, a wholly owned subsidiary of Vendex International NV, has bought 300 Cyberworks robot vacuum cleaners for $6 million. 

   Burhan Parkar says the robot machines have a unique market advantage: "It's the first machine capable of working entirely on its own, without human programming or instruction." 

   The robot weighs about 180 kilograms - mostly batteries - and uses a sophisticated electronic vision system and artificial intelligence. 

   They are also cost effective at $20,000 each compared with up to almost eight times that amount for competing models that require constant programming, Burhan Parkar says. 

   "We have a tremendous strategic and technological lead.  That positions us as a world leader." 

   "It's a revolution in the cleaning industry," says Cees Ravesteyn, vice-president of marketing and development for Hudon, which made contact with Cyberworks by chance at a demonstration in Cologne, West Germany. 

   Hudon is now using eight of the robots in the Netherlands, Belgium and France and expects to expand by the end of the summer. 

  'CONSERVATIVE' FASHION 

   Studies of market potential in Europe indicate that Cyberworks could sell at least 5,000 units a year, Ravesteyn said. 

   While Burhan Parkar talks confidently about penetrating major markets in North America and Europe, he said Cyberworks plans to proceed in "typically Canadian, conservative fashion." 

   Burhan Parkar says he calculates it will take a minimum of two years for any competition to catch up. 
 </I

  
 
   Although virtual reality is in the development stages, a California company named VPL Research Inc. has already begun selling headgear called an "eyephone" and a lightweight, sensor-equipped glove that would allow a user to experience the sensation of picking up and moving objects that appear in a virtual-reality environment.  David Benman, VPL's sales-support engineer, said that the eyephone, which also resembles a scuba mask, sells for $11,200, while the glove costs $10,500.  A complete package, which includes the eyephone, glove and two computers manufactured by Silicon Graphics Computer Systems of Newport Beach, California, costs about $267,000.  Benman said that VPL has sold its eyephone and glove to most of the U.S. research institutes and universities that are currently conducting virtual-reality research.   </I

 <I
  Ontario looking to the future in advanced composite materials 

   Super-strong golf clubs, artificial joints and ceramic engine parts appear to have little in common.  But all are made using space-age materials. 

   These advanced materials are composites, combinations of materials offering extreme strength and lightness.  They are already being used in manufacturing, medicine, communications and aerospace industries.  And the field is becoming increasingly competitive as countries such as Japan pour billions of dollars into research. 

  CENTRES OF EXCELLENCE 

   To help Canada stay in the race, the Ontario government's nonprofit Ontario Centre for Materials Research - biggest of seven centres of excellence set up by the Premier's Council - will receive $43 million in provincial grants over five years. 

   The centre manages and promotes long-term research in advanced materials at the University of Waterloo, Hamilton's McMaster University, University of Western Ontario in London, Queen's University in Kingston and University of Toronto. 

   The two biggest applications for composites are military equipment and sporting goods, says Craig Simpson, of Ontario Hydro's research division. 

   Simpson spoke at the annual meeting of the Ontario Centre for Materials Research (OCMR) this week in Toronto and used a $150 golf-club shaft as a pointer during his presentation. 

   Simpson said the composite club was developed in Japan, where most high-tech sports equipment originates.  With the help of provincial funding and OCMR, a variety of products incorporating composites will one day be made in Ontario, he said. 

   The centre for materials research helps to fund world-class research projects and ensures that technological advances and information on new materials flows into Canadian industry.  When industry develops what the research community wants, funds also flow into corporate coffers. 

   For example, Toronto-based Electrofuel Manufacturing Co. Ltd. supplied two pieces of equipment for McMaster's ceramics research. 

   Electrofuel President Sankar Das Gupta said a high-temperature, high-pressure furnace to measure the expansion and contraction of ceramics was developed by the university and his company. 

   The furnace, which operates at 2,000 C - steel melts at about 1,400 C - costs $55,000. 

   Electrofuel also designed and built a $150,000 vacuum hot press to manufacture ceramics parts. 

   Both pieces of equipment are owned by the centre for materials research, but McMaster will have first crack at acquiring them at fair market value when the centre's funding agreement with the government is completed. 

   Getting the equipment installed and working has resulted in orders from research and development laboratories in other countries, Das Gupta said.  He expects the six-year-old company's revenues to double to $4 million in 1990. 

   Other industry members include heavyweights such as Alcan Aluminium Ltd., Imperial Oil Ltd. and Dofasco Inc., as well as smaller firms such as Ultra High Vacuum Instruments Ltd. in Burlington, Ontario, and Brampton's Colortech Inc. 

   Most OCMR initial funding went to buy equipment necessary to carry out materials research.  More than $1.2 million went to particle beam studies at Western.  McMaster's molecular beam epitaxy unit, which makes new materials for chips used in optical communications systems, cost $1.2 million. 

   The centre also funds university scientists working in: biomaterials, polymers and composites; components handling optical and electronic communications signals; metals and ceramics; the study of surfaces and interfaces. 

  COAGULATION RATES 

   Baxter Healthcare Corp. of Deerfield, Illinois, through its Mississauga-based Canadian subsidiary, is partially funding two OCMR-supported scientists working in biomedical materials at McMaster and University of Toronto. 

   One researcher is studying changes in blood coagulation rates when it comes in contact with materials in kidney dialysis machines and other equipment.  The other researcher is trying to develop a coating to protect transplanted pancreatic cells from being rejected. 

   Almost $10 million was allocated to more than 30 OCMR projects in the year ended March 31, 1989. 
 </I


 <I
  Brokers offer more market information 

   Discount brokers are offering clients a variety of electronic market-quote systems to get more information into investors' hands - and more commissions into brokers' pockets. 

   Clients at Toronto-based Marathon Brokerage can choose from market-information systems developed by Stratford Software Corp. of Vancouver, Toronto-based International Hay-Info Communications Inc. or Bell Canada's Alex home shopping service. 

   Alex, which has been operating in Montreal for more than a year, will be available in the Toronto area starting next Monday. 

   Stratford and Hav-Info provide real-time quotes.  Alex offers real-time quotes from the Montreal and Toronto stock exchanges and a 15-minute delay on the Vancouver exchange. 

  PHONE ACCESS 

   Toronto Dominion Green Line Investor Services Inc. has its own real-time quote service, which is accessible by touch-tone phone, but it also offers Alex. 

   Discounters say do-it-yourself quotes and market information are ideal for people who manage their own investments, do their own research and analysis, and only use a broker to trade securities. 

   "They take control of their own portfolios and look to us for information, not for advice or recommendations," Marathon President Paul Bates said. 

   He believes giving clients more information means they will trade more often, generating more commission fees. 

   Stratford's information system, called Suzy, has more market-related features than the others, but it doesn't cover U.S. exchanges.  The company started shipping the product this week. 

   Suzy offers quotes from four Canadian exchanges, stock indices, share price changes and volumes, a breakdown of trades on each company, portfolio updates, and news stories and press releases carried by the Globe & Mail's InfoGlobe service and StockWatch, a Vancouver publication.  It can compile lists of the most active stocks or generate daily or weekly stock-price graphs. 

   Bates said Marathon is working with Stratford to develop features that will enable clients to access their own accounts and check the balance and margin available, or get information on Marathon's services. 

   Suzy's electronic mail feature also allows users to send memos or files.  Stratford is setting up information networks on various subjects.  Already available is small business accounting, run by software consultant Richard Morochove, and entrepreneurship, run by Rick Spence, editor of Small Business magazine. 

   Hav-Info doesn't yet provide news items or graphics, but its quotes service is more extensive, covering not only four Canadian exchanges but Trans Canada options, Toronto's over-the-counter stocks, New York and American stock exchanges, and the Dow Jones industrial average. 

  SHOP-AT-HOME SERVICE 

   Alex carries quotes from the three exchanges, but is working with the exchanges and other market-quote service firms to beef up its financial offerings.  It carries a host of other information, including a shop-at-home service. 

   The three companies charge about the same for quote services, but hardware and software requirements differ. 

   The basic fee for Suzy is 20 cents a minute, plus on-line charges or tapping into the news databases and using other features.  Discounts are offered after market hours. Hav-Info charges 29 cents a minute and Alex 25 cents a minute for quotes. 

   Suzy is a software disk that costs $29.95.  To use it you need an IBM or IBM-compatible personal computer with at least 384K of ram memory and a modem.  No other software is required. 

   Hav-Info's system also runs on IBM or IBM compatible PCs with modems.  But Marathon clients who don't have a computer can buy a Hav-Info terminal with built-in modem for $599, a $200 discount from the regular price.  Communications software allowing the unit to send and receive data is free. 

   An IBM or Macintosh PC with modem, and free software provided by Bell, will pick up Alex.  Those without computers need a special terminal, which plugs into a phone jack and rents for $7.95 a month.  The first two months are free. 

   Green Line's direct quote service is free to clients who do at one trade a month through the discounter.  Non-traders get 25 free quotes a month, before a $0.25-a-quote charge kicks in. 
 </I


 <I
  Bright future forecast for document imaging 

   Document imaging is the latest weapon in the war against paper burden.  These systems scan letters, invoices and legal documents and store them in picture form on computer disk. 

   The process is intended to speed up paper flow through large organizations, said Elliot Katz, a product manager with Wang Canada Ltd. 

   Banks, insurance companies and government agencies are among the first to get into document imaging. 

   "There's massive savings available," said Paul Renaud, director of technology for SHL Systemhouse Inc. of Ottawa.  

   The snag is that document systems are expensive - $15,000 to $20,000 a workstation - because pictures require a lot of hardware.  They also take up a lot of space on disk and demand high-powered microprocessors. 

   With scanners to read documents, optical disk storage and a few high-resolution monitors, a typical entry-level network costs about $650,000.  Large systems can add robot-controlled libraries of optical disks and satellite communications links. 

   The U.S.-based Association for Information & Image Management estimates the market for image systems will be worth US$12.7 billion by 1993, making it potentially one of the strongest segments of the computer industry. 

   The two biggest players are Wang, which has sold 20 of its minicomputer-based systems in Canada, and Filenet Corp. of California, which has sold five. 

   Although suppliers are quick to compare imaging to earlier computer stars, such as data processing in the 1970s or desktop publishing in the 1980s, users haven't been as quick to lay down their cash. 

   Systemhouse's Renaud said one reason could be the generic one-system-for-all approach some suppliers have taken. 

   "There's a fair number of misconceptions about image systems.  The first one is that there's a cookie-cutter or no-brain solution for everyone." 

   Although most large organizations will admit to having a problem with paper, the solution must be tailored to specific bottlenecks in the company and build from their computer system, as opposed to scanning everything in the office, he added. 

   Mutual Life Assurance Co. of Canada uses a Filenet system to speed the processing of death claims. 

   "It has improved our service in the department, reduced the amount of paper and we no longer lose pieces of paper," said Ray Simonson, an administrative services executive with the firm. 

   But, he added, the system "is not really as useful as information coded in a database." 

   For example, a traditional database record system can track customers by sex, age, or other characteristics, which can be easily cross-referenced by computer. 

   Documents stored in picture form can't be automatically broken down into such categories. 

   And organizations that deal with paper efficiently have a hard time justifying the high cost of jumping to an electronic system. 

   But technological improvements and the recent introduction of low-cost systems for personal computers from large manufacturers such as Wang and International Business Machines Corp. are likely to change that, Simonson said. "Now we're looking again." 

   One firm hoping to jump in at the bottom end of the market is software firm Imara Research Corp. of Toronto.  Imara intends to ship a $400 imaging program that can be used on relatively low-cost standard hardware.  The program, which has yet to be named, will be available in June. 

   Imara has already done joint marketing with IBM and software giant Microsoft Corp., which are counting on new products to stimulate sales of computers using Microsoft's new operating system, OS/2. 

   "Imara's is one of the first imaging products built on standard technology that can sit on anybody's machine without spending hundreds of thousands of dollars," said David Boker, Microsoft Canada's corporate accounts manager.  "There's certainly a strong feeling about the product at Microsoft." 

   Imara Chairman Stephen Sutherland projects sales of $1.5 million by February, 1991. 

   Although Sutherland acknowledges the estimate is "really low," he's quick to compare his 15-employee firm's product to other leading personal computer programs, such as Aldus Corp.'s Pagemaker, the breakthrough desktop publishing program, and Wordperfect Corp.'s Wordperfect, the longstanding word processor of choice in many companies. 

   "The fact that there are high-end systems available, just legitimizes the need for the technology." 
 </I

 <I
   Indeed, most of the scientists working on the new technology say that they have merely crossed the frontier of an exciting new concept.  But Jacobson predicts that virtual-reality products will be available to such commercial users as architects and engineers within five years, while mass-market consumer entertainment products will be available within a decade.  But given the rate at which previous computer revolutions have occurred, virtual reality could become a commonplace technology much sooner than that. 
 </I



   UNHEALTHY BUILDINGS: Experts say indoor air can make people sick 

   Early in 1989, several federal civil servants working in a Montreal office tower began complaining about headaches, fatigue and nausea.  According to some reports, several employees fainted at their desks, recalls Tedd Nathanson, an Ottawa-based engineer with the federal department of public works.  He said that he inspected the building and discovered that automobile exhaust fumes from the underground parking garage were penetrating the offices because the owner of the building had boarded up the underground ventilation system to save money.  Although the source of that problem was unusual, concerns about air quality in the workplace are becoming increasingly common.  Indeed, almost 1,300 scientists, doctors and businessmen from 30 countries met in Toronto last week for a six-day conference aimed at improving air quality in homes and offices. 

   Indoor air quality began to emerge as a health issue during the late 1970s when builders across North America and Europe started installing sealed windows, which cannot be opened, as an energy conservation measure in almost all new office towers, hospitals and schools.  Experts attending the Toronto conference said that air delivered through ventilation systems frequently becomes contaminated with pollutants from inside and outside the building.  When workers begin complaining about headaches, fatigue and other ailments, they are suffering from what experts call "sick-building syndrome." Said Douglas Walkinshaw, an Ottawa-based consultant who led the Toronto conference: "Most people now work in airtight buildings, so it is becoming a very big issue." 

   Just how big was apparent at the meeting, where participants delivered 530 research papers on indoor-air quality.  By comparison, there were only 50 papers at the first conference on indoor air, which was held in Copenhagen in 1978 and which attracted only 230 delegates.  The combination of research and actual investigations of buildings with air-quality problems has revealed dozens of potential sources of pollution.  Said Ole Fanger, a Danish air-quality expert and organizer of the first conference: "It was always assumed that buildings were clean.  That was the big error." 

   New buildings, or structures that have been renovated or refurbished, can contain dozens of pollutants called volatile organic compounds, according to Walkinshaw.  Many of the compounds, including benzene, toluene and octane, are derived from petroleum and are used in the glues applied to upholstery, tiles and carpets.  Moisture within a building is a second source of airborne contaminants, Walkinshaw said.  Dirty humidifiers, condensation on pipes and wet carpets become breeding grounds for such micro-organisms as moulds and fungi. 

   In most sealed buildings, only about 20 per cent of the air circulating at any given time is actually fresh from outside, said Walkinshaw.  The remainder is merely being recirculated, even when it is contaminated by volatile organic compounds or micro-organisms.  Walkinshaw said that the problem can be compounded if building operators shut off their ventilation systems altogether at night or during weekends, in order to save energy - or money. 

   In some cases, poorly designed ventilation systems can lead to contaminated air.  Nathanson said that his department inspected an office tower in Ottawa recently after numerous employees complained about stale air and illness.  Public works inspectors eventually discovered that the air intake duct and the exhaust duct, both of which measured six feet by 12 feet, were side by side on the roof.  About 25 per cent of the air that was pumped out of the building through the exhaust was being sucked back in through the intake duct.  Nathanson said that the flaw was corrected by building a barrier between the two ducts. 

   Some medical experts who have studied sick-building syndrome contend that contaminated indoor air rarely poses a serious threat to an employee's health.  Sherwood Burge, a British doctor who attended the Toronto conference, said that the real problem is the comfort and productivity of office workers.  He and fellow British physician Alastair Robertson conducted a study in which they circulated questionnaires to 4,000 workers based in 47 buildings in Great Britain.  Burge said that employees in sealed buildings reported three times as many health problems, including headaches, runny noses and fatigue, as their counterparts in buildings where the windows could be opened.  Said Burge: "The symptoms are not particularly disabling." 

   According to some of the delegates, solutions to sick-building syndrome are just beginning to emerge as a result of research conducted during the past decade.  Burge said that architects and engineers should begin designing buildings that allow the occupants some measure of individual control over heat and lighting.  For his part, Fanger said that he is part of an eight-member European Community task force set up to develop new ventilation guidelines.  He added, "It will take a complete change in our philosophy and thinking about buildings."  Clearly, it will take some strong medicine to cure the sick-building syndrome that has become such a prevalent problem. 
 </I


 <I
  Re-creating reality: a new development has tantalizing applications 

   Michael McGreevy says that he will walk on the planet Venus within the next two years.  But he plans to do it without leaving his laboratory in California's Silicon Valley.  McGreevy, a 40-year-old research scientist with the National Aeronautics and Space Administration (NASA), is one of dozens of American and Japanese scientists racing to perfect a newly developed computer technology called virtual reality.  The technology involves the use of computers to create full-color, three-dimensional images of everything from molecules to planetary surfaces.  And rather than merely looking at the images, scientists are using headgear resembling scuba diving masks equipped with image-bearing screens to enter the so-called virtual realities and even manipulate their contents.  There are also potentially tantalizing applications for the entertainment industry.  Some theorists predict that people who now watch steamy TV sex videos could instead experience the sensation of participating in them. 

   Proponents of virtual reality contend that within 10 years, the technology will have a profound influence on the work of architects, engineers and urban planners.  They will be able to translate blueprints and plans for buildings, airplanes and expressways into lifelike, 3-D images that the designers or users can simulate entering to explore and change before construction begins.  Surgeons may be able to put themselves inside the human body, while other scientists will be able to explore room-sized models of molecules.  Said Robert Jacobson, associate director of the Seattle-based Human Interface Technology Laboratory, which is devoted solely to developing virtual reality: "It's a fabulous idea.  If it comes off, it's going to be remarkable." 

   But other experts predict that virtual reality could lead to the problems foreseen by Vancouver author William Gibson in his award-winning 1984 novel Neuromancer.  The book deals with a young man in a futuristic society who becomes obsessed with a virtual-reality fantasy world called "cyberspace."  Louis (Bo) Gehring, a Toronto-based computer graphics expert who is working on virtual-reality projects for the U.S. air force, said, "When you look at the impact video games have already had on kids and compare them to the quality of cyberspace, you cannot overstate the potential impact." 

   People who have sampled virtual reality describe it as a powerful experience.  David Cohn, a senior editor with the Vancouver-based magazine CADalyst, a publication for specialists in computer-aided design, said that he has entered the world of virtual reality four times.  Once, during a demonstration at a Boston hotel, he wore a headpiece that resembled a diving mask and rode a stationary exercise bike that was hooked up to a computer that produced images of outdoor scenes.  Cohn said that when he began pedalling, he felt he had actually entered the computer-generated environment.  The harder he pedalled, the faster the bike appeared to travel.  When he reached 25 m.p.h., the bicycle actually left the ground in the artificial environment and he felt as though he were flying.  Said Cohn: "It was wonderful.  I didn't want to take off the mask." 

   Still, some computer scientists dismiss the technology's entertainment potential and its addictive possibilities.  They maintain that it will be a valuable tool for professionals in several different fields.  Jacobson said that urban planners could translate proposals for expressways into 3-D virtual-reality freeways, then change routes and simulate traffic-flow patterns as they would be now or years into the future.  Said Jacobson: "You need a visceral experience in order to appreciate traffic congestion.  Numbers on paper don't capture the frustration." 

  Computer scientists and doctors at the University of North Carolina at Chapel Hill have used virtual reality to study the accuracy and impact of X-ray treatment beams used to destroy tumors in human patients.  Frederick Brooks, a computer scientist at the university, said that they generated images of internal human organs.  Doctors then simulated entering the artificial realities by putting on special headgear.  They could then watch as computer-generated X-ray treatment beams entered the body from different directions and attacked the tumor.  Brooks said that the purpose of the experiment was to determine whether doctors could aim the beams more accurately so that they would destroy the tumor without damaging other body organs. 

   Apart from its potential for improving medical treatment or the design of earthly structures, virtual reality could be a valuable tool in space exploration.  NASA's McGreevy said that he plans to create virtual-reality images of Venus using data collected by an unmanned NASA spacecraft currently travelling towards the planet.  Said McGreevy: "We will be able to re-create the surface of Venus in virtual reality and explore it almost as you would your office." 

   McGreevy said that the technology may be used on a manned mission to Mars that NASA hopes to undertake by the year 2019.  He said that after landing on Mars, astronauts could send an unmanned rover out from the spacecraft to explore the surface for miles around.  With virtual-reality images of the Martian surface, produced on the basis of previously collected data, astronauts could see in advance what kind of terrain the rover would be crossing and guide it around any hazards.  The rovers would also be equipped with video cameras, which would complement the virtual-reality images. 

   But before virtual reality can be used on an everyday basis, scientists say that they will have to greatly improve the technology for creating computer-generated imagery.  Brooks described the images of human organs generated at the University of North Carolina as "pretty crude, Saturday-morning cartoon quality."  Stephen Hines, research director at a Los Angeles company called 3-D ImageTek Corp., added that even the best color images currently lack detail or any shading. 

   According to Hines, time delays that occur when using the existing technology are another problem.  His firm's virtual-reality headgear is equipped with a sensor that informs the computer of the location and position of the user.  If he were to take three steps forward, the computer would have to produce new images to show how a building or human organ might look from his new position.  But so far, computers cannot produce color images fast enough to keep up with human movements.  As a result, the viewer sees a jerky or choppy movement in virtual reality, said Hines. 

   According to Gehring, who is working with American military and civilian scientists, the U.S. air force has developed the most sophisticated virtual-reality equipment to date in an attempt to reduce the clutter and complexity of the cockpits in modern fighter aircraft.  The instrument panels in jet fighters, said Gehring, "look like a Swiss clock shop gone berserk.  There are typically as many as 200 displays and switches."  Since 1982, the air force has been trying to develop a system that would allow a pilot to fly highly complex aircraft on the basis of an artificial reality presented to him through his headgear.  Rather than dealing with a confusing instrument panel, the pilot would be immersed in computer-generated images of the landscape beneath him and computer symbols projected into his field of vision.  The headgear would also contain 3-D sound so that the pilot could determine with greater speed and precision the position of other planes in his squadron or incoming enemy missiles.  But Gehring said that the development of such a system would not be complete until the mid-1990s at the earliest. 

 <O untranscribed text <O

   Its most prized customer has been NASA.  Benman conceded that VPL's equipment is so far capable of producing only grainy images that lack detail.  But he said that virtual reality is a captivating concept. 

 </I


  THE CLEAN CAR QUEST: Los Angeles promotes electric automobiles 

   Since they first appeared in 1888, vehicles powered by electricity have risen and fallen in public popularity.  But many experts now say that electric cars will make a comeback over the next decade as North Americans and Europeans become increasingly concerned about air quality and other environmental issues, as well as rising oil prices.  Earlier this year, Detroit-based General Motors Corp.(GM) unveiled a prototype of a sleek-looking electric car called the Impact.  Last month, the city of Los Angeles, along with a private utility, agreed to invest $8 million in the development of electric vehicles.  Their goal is to put 1,000 electric cars on Los Angeles roads by 1992, with another 9,000 by 1995.  And last week, in a joint venture with GM, Newmarket, Ontario-based VEHMA International, a division of auto-parts maker Magna International, began assembling 60 electric vans.  Two major Canadian electrical utilities and more than 20 U.S. utilities have agreed to purchase the vehicles in order to test their capabilities.  Said Jack Kerr, president of the Electric Vehicle Association of Canada in Mississauga, Ontario: "It's sort of the hush before the storm." 

   Proponents say that, for the first time in decades, a combination of political, economic and environmental forces have made electric vehicles an attractive alternative to gasoline-powered automobiles.  For one thing, Iraq's August 2 invasion of Kuwait has pushed up gas prices in Canada between two and five cents a litre.  U.S. prices have jumped by six to eight cents a litre, and international prices from 10 to 12 cents a litre.  And increased concern about air pollution has led to strict clean-air legislation, especially in such huge urban centres as Los Angeles, and a search for automobiles that do not produce harmful emissions.  Still, some experts maintain that consumers will be reluctant to switch to electric cars until researchers develop a battery capable of powering a vehicle beyond the current limit of about 160 km before a recharge is necessary.  Declared Mark Nantais, executive director of the Toronto-based Motor Vehicle Manufacturers Association: "I don't see gas going by the wayside or a long time yet because it's a very good fuel and the current technology is based on gasoline." 

   But it has not always been that way.  At the turn of the century, 38 per cent of the automobiles in the United States were powered by electricity, 22 per cent used gasoline, and the rest were steam driven.  Then, as now, electric cars were almost silent and required little maintenance.  By 1912, there were nearly 34,000 electric vehicles in use in the United States alone, compared with about 20,000 automobiles powered by internal-combustion engines.  But by the end of the Second World War, electric vehicles had almost completely disappeared from North American roads.  Motorists had given up on the vehicles because they could only be driven a maximum of about 65 km before a recharge.  As well, they could reach a top speed of only 32 km/h, while gas-powered vehicles could easily attain speeds of 140 km/h. 

   Many of the problems associated with electric vehicles during the first half of the century still have not been entirely resolved.  Although more powerful batteries are now available, even the best electric vehicles remain limited, and it takes an average of six hours to fully recharge the batteries.  As a result, most automotive experts argue that electric cars would be useful solely in urban areas.  Even then, motorists would require a network of service centres where the vehicles could be conveniently recharged.  Prolonged testing of electric vehicles has also shown that the batteries must be replaced at about 80,000 km, or once every four to five years, which could cost $1,500 for a car and up to $7,000 for a van.  However, scientists in Europe and North America are looking for alternatives to the current lead-acid batteries in order to increase the range and versatility of electric vehicles. 

   In the past decade, growing public concern over air pollution, much of it caused by internal-combustion engines, has led to renewed interest in electric cars.  According to the Washington-based Environmental Protection Agency (EPA), many U.S. cities regularly exceed the federal clean air standard of 0.12 parts per million of ground-level ozone, a gas contained in automobile exhausts and the principal component of smog.  In Los Angeles and the adjoining communities of San Bernardino, Orange County and Riverside County, where there are eight million registered automobiles, ozone levels exceeded the EPA guidelines on 127 days last year.  EPA monitoring frequently revealed ozone levels at triple the guidelines. 

   With its severe air-quality problems, Los Angeles has become one of the first major urban areas in North America to promote the development and use of electric cars.  Last year, the South Coast Air Quality Management Board, a California regional agency that establishes air-quality standards for much of southern California, ordered that average daily ozone levels in the Los Angeles basin must meet the EPA standards by the year 2010.  To address the problem, the Los Angeles department of water and power and Southern California Edison, a private utility that supplies electricity to the basin, agreed to spend $8 million over a period of five years to finance the design and development of an electric car. 

   After reviewing proposals from 19 companies, the City of Los Angeles and Southern California Edison on September 6 signed an agreement with Swedish-based Clean Air Transport, which has developed a vehicle called the LA 301.  Clean Air was founded in 1988 and is backed by British, American and Swedish investors.  Its four-passenger electric car is designed to travel at top speeds of 115 km/h for distances of between 100 km and 115 km before requiring a recharge.  Jerry Enzenauer, who is managing the program on behalf of Los Angeles, said that the car would likely sell for about $29,000 initially, but that the price would come down as more cars are produced.  Prototypes for the vehicles are currently in production at Clean Air's plant in Sweden.  Enzenauer said that both the city and Southern California Edison plan to add about 30 of the vehicles to their fleets. 

   Meanwhile, the Big Three automakers, General Motors, Ford Motor Co. and Chrysler Corp., devote what analysts regard as a minor portion of their annual development budgets to electric vehicles.  GM's Impact is based on the engineering and design of its solar car, the Sunraycer.  With a range of about 190 km, the 2,200-lb. car can maintain an average speed of 90 km/h on a highway, it can accelerate from zero to 100 km/h in eight seconds and it can reach a top speed of 120 km/h.  The car includes a built-in recharging system, and the 870-lb. battery pack, which houses 32 lead-acid batteries, can be almost completely recharged in two hours.  But GM officials refused to comment about the company's production plans or the projected cost of the Impact. 

   At the same time, the company is also working with VEHMA and has developed the electric G-Van, 60 of which will be produced in Newmarket, 25 km north of Toronto, before the end of this year.  GM has supplied the frames for the vans, while VEHMA is assembling the vehicles.  The vans have already been sold primarily to utilities in the United States and Canada, including three to Hydro Quebec and five to Calgary-based TransAlta Utilities Corp.  The utilities will use the vans primarily for demonstration purposes.  "The initial buyers are really benevolent customers," explained David Sedgwick, the general manager of Powerplex, VEHMA's research and development arm.  "They understand the cost of the vehicles isn't really competitive with internal-combustion vehicles but they want to get involved."  The vans, which will initially cost $57,500, have a range of about 95 km, with a top speed of 90 km/h, and their lead-acid batteries require about eight hours for a complete recharge. 

   Ford, too, has been working on electric vehicles during the past eight years and the company currently has 10 vehicles out in test fleets in the Washington area.  Ford developed its EV program with Fairfield, Conneticut-based General Electric Co. and several major battery manufacturers.  Since 1982, the company has spent about $23 million on the project.  John Jelinek, product information manager at Ford Motor Co. of Canada in Oakville, Ontario, says that there is a good chance the electric vehicles will be widely available to consumers early in the next century in metropolitan areas.  Ford's demonstrator electric vehicles, five vans and five cars, can travel about 160 km without a recharge to the battery.  "That's great if you're in Metro Toronto," says Jelinek.  "So we see it as being an answer in urban areas."  He predicted that electric-powered vehicles will not be significantly more expensive than gas-powered cars. 

   Chrysler appears to be making a smaller commitment to electric vehicle research than either Ford or GM, and officials with the company express less optimism about the technology than their counterparts at the other two companies.  Chrysler is participating in a joint venture with the Electrical Power Research Institute in Palo Alto, California, to develop and test five electric-powered vans.  "We're not convinced that there's a market out there," said Chrysler media relations officer Anthony Cervone.  "Is there a supplier base out there for batteries?  Is there one for electric power?  Electric vehicles seem like the end-all from an emissions standpoint, but electric companies will have to work around the clock." 

   But several Canadian, European and American companies are working to develop solutions to the roadblocks facing the vehicles.  Ford, Powerplex and the West German electrical engineering company Asea Brown Boveri are all testing sodium-sulphur batteries.  According to Sedgwick, there are 30 vehicles in Europe and Canada using the batteries, and their performance has been encouraging.  Sodium-sulphur can store three to four times the energy of lead-acid, which means that a car powered by sodium-sulphur batteries could travel about 385 km without a recharge.  But Sedgwick added that one of the problems with the batteries, which are still in the prototype stage, is that they only last a year or two. 

   Despite the growing evidence that internal-combustion engines are contributing significantly to air pollution in urban areas, most experts agree that electric-powered vehicles do not represent a quick and simple solution yet.  The range of the cars is too limited, the battery replacement costs too high, and the cars themselves are expensive.  But most advocates of electric cars argue that if the vehicles were mass-produced, they would be cost-competitive with conventional vehicles.  "We're moving in the direction of electric vehicles," said Nantais.  "But it all rests on whether the public creates that demand."  Until the auto industry produces an electric vehicle that runs as efficiently - and as inexpensively - as gas-powered automobiles, it will take some time to convince consumers that what was popular in the early 20th century is also the wave of the future.  </I

   A safety hitch: accidents threaten a federal PCB program 

   On February 1, only two weeks after technicians in Goose Bay, Labrador, began testing a mobile incinerator, a power interruption caused an exhaust fan to fail.  As a result, 18 workers were exposed to the fumes from burning polychlorinated biphenyls (PCBs), a suspected carcinogen.  Then, in mid-April, a cooling-system failure caused parts of the incinerator to melt.  A similar portable incinerator in Swan Hills, Alberta, also stopped functioning last November because of water-pressure problems.  Environment Canada officials said that none of the accidents exposed technicians to health risks.  But the mishaps threatened a program aimed at destroying PCBs that are stored in many parts of Canada.  Federal officials said that publicity about the accidents could make it difficult to find communities willing to act as test sites for the program, under which Ottawa hopes to destroy thousands of tons of PCBs by the end of 1993.  Said Steve Hart, director of Environment Canada's waste-management branch: "There is no question, the breakdowns will slow down the process." 

   Since August, 1988, when a fire in a PCB warehouse forced the evacuation of more than 3,000 people in St-Basile-le-Grand, Quebec, Ottawa has been working on plans to destroy stockpiles of PCBs that are stored at more than 3,000 government-controlled locations across the country.  To overcome objections in many communities to PCBs being destroyed locally, then-federal Environment Minister Thomas McMillan unveiled a plan in 1988 to use portable incinerators that could be temporarily installed in communities to burn PCBs in the area.  Now, federal officials say that they fear that the accidents could threaten public acceptance of the program.  Said Captain Gregory McGuire, project manager for the Goose Bay test program, which is being conducted at the nearby Canadian Armed Forces base: "One of the things we probably did wrong was to leave the impression that this would run without a hitch.  Nothing mechanical ever does."  

   Although they were never manufactured in Canada, 40,000 tons of PCBs were used across the country, mainly as coolants and insulators in electrical equipment, between 1929 and 1977.  A series of studies in the early 1970s showed a link between them and liver cancer in laboratory animals.   

  

        Locational Referencing and Geographical
Information Systems    
    DAVID V. HAWKE  
    Geographical Information Systems (GIS), commonly
subsumed within the guise of automated mapping, computer aided
draughting or land information systems, might best be
described as spatial analysis or decision support systems
(Cowan, 1988).   Their primary role is to provide analytical
facilities for the study of spatial features based on both the
feature 's locational and other characteristics.   The dual
nature of spatial data is emphasised by the duality of data
storage mechanisms found in competent GIS such that spatial
relationships (topology) are preserved by specially developed
storage methodologies while non-locational characteristics are
stored in more traditional methodologies.  
    As the technology is still in its infancy by
comparison to information technology in general, a
considerable research emphasis has been placed on the
development of appropriate methodologies for storing the
locational component of spatial features within GIS.   While
this emphasis may distract attention from the analytical
capabilities of a GIS, its importance must not be
underestimated.   Much of the analysis performed within the
GIS environment requires the use of spatial characteristics at
some stage of the analysis, even though the results may only
be presented by means of tabular or other forms of report
generation.   An inappropriate locational referencing
system, whether engendered by the data model or the physical
storage mechanism used by the software, is therefore likely to
be detrimental to successful utilisation of a GIS.  
    This paper explores the influence of locational
referencing schemes on spatial analysis, placing emphasis on
the underlying model of space and its constraints and
advantages.   The general field of spatial modelling will be
briefly reviewed to illustrate the range of strategies in use
and to provide an insight into the characteristics of
alternative views of space.  
    MODELLING SPACE  
    The underlying characteristic which distinguishes
Geographic Information Systems from other spatial data
handling technologies is the ability to analyse space.
  This requires that any modelling strategy preserves the
topological character of the data by ensuring that spatial
relationships are implicitly or explicitly maintained
(permitting, for example, assessment of nearness, adjacency or
connectivity).   Spatial modelling procedures are typically
grouped into those based on vector methodologies (utilising
coordinate data to model curvilinear features) on the one hand
and tessellated methodologies (modelling surfaces by means of
a regular or irregular mesh) on the other (Peuquet, 1984).
  Vector data models (Figure 1) assume that spatial features
may be represented as points, lines (chains of points linking
the end points or nodes of the line), or polygons (areas
bounded by linked lines).   These models focus attention on
the attributes of the data clement in the case of the point
and line, and of the bounded area in the case of the polygon.
  Strategies range from simple data structures which require
visual analysis to reconstruct the topological relationships
between features, through models utilising single (straight)
vectors to model linear features, to those using long chains
of short vectors to model irregular features.   The more
complex models maintain topology by retaining neighbourhood
data with each feature&semi; for example, identifiers for the start
and end of chains and identifiers for the two polygons a line
separates, and the linkages between lines composing the
boundaries of polygons.   Such explicit retention of
topology ensures that analysis of spatial patterns is
possible.  
    Tessellated strategies assume that space may be
decomposed into cells within a regular or irregular matrix
(Figure 2).   The most common form of tessellation is that
of the square or rectangular grid (commonly referred to as a
raster model due to the similarities between the grid and
raster display devices), although hexagonal and triangular
networks have also received some attention (van Roessel, 1988&semi;
Peuquet, 1984).   These methodologies assume that each cell
is homogeneous in terms of the theme being modelled, and
frequently suffer from problems at boundaries where a cell
must be coded by its dominant characteristic rather than
homogeneity (Figure 3).   Attempts to resolve this problem
are based on the use of matrices having an irregular cell size
or shape (for example, TIN methodologies) or cells based on
the regular and recursive decomposition of space (quadtree
methodologies&semi; see, for example, Samet, 1984).   Irregular
tessellations have been applied in a wide variety of
situations to simplify data analysis by overlay techniques
since they maintain attribute data for a single set of
boundaries.   Such representations are often described as
the 'tea-bag' model of a GIS, where attribute data for a
variety of themes are tagged to each polygon and stored
together (see, for example, Van Berkel and Williams, 1985).
  The more general or generic model assumes that each data
theme is represented by a separate graphic description and is
maintained in a separate storage location.  
    The translation of data models into physical data
storage within a GIS places further constraints on the spatial
representation utilised owing to the need to achieve accuracy
over large areas and the current trend to provide a continuous
view of space in a manner transparent to the user (the concept
of seamless space as opposed to the restrictions imposed by
map sheet boundaries).   Attempts to resolve these problems
have resulted in the development of utilities to maintain
libraries of files relating to small units of space and
include some hybrid approaches.  
  figure 1  
    One such approach uses a vector strategy to model
spatial features, but a quadtree approach to maintain
individual data files at a size ensuring optimum performance
of the system.   This is achieved by constructing a quadtree
description of the spatial extent of the data, and storing the
features contained in each cell of the tree as a vector
description.   As data is added to a file, reconfiguration
occurs automatically if a predetermined number of features is
reached, decomposing the cell (and hence file) into the four
cells of the next lower level of the quadtree.   The
quadtree provides a description of the topological
relationships between data files and provides rapid access to
neighbouring data.   Hence the spatial extent covered by an
individual graphic data file varies in response to the volume
of data available.   The user remains unaware of the
physical storage mechanism for data as queries for extents
which transcend file extents are automatically handled by the
system.  
    The alternative approach (defining a fixed spatial
extent for a file of data, akin to the fixed size of a map
sheet) requires either a careful assessment of the amount of
data to be included in each file or a manual reconstruction of
the database when individual files become unwieldy owing to
their size.  
    Problems arising through the use of particular spatial
models relate to the resolution which they can achieve (that
is, the size of the minimum spatial extent distinguishable),
the precision with which data are stored (usually defined in
terms of the number of digits retained for each element, a
function of implementation and influenced by both software and
hardware) and their accuracy (the likeness to reality, usually
defined in terms of an error margin, a function of data
collection and resolution).  
    The influence of resolution is most graphically seen
in the tessellated models where the assumption of homogeneity
of cell contents requires a cell size small enough to maintain
the assumption or alternatively a larger cell size and abuse
of the assumption.   The tradeoff associated with the
necessity to maintain homogeneity is seen in the volume of
storage required for the database.   For example, in a
square grid based structure, the increase in resolution
achieved by halving the cell size results in a quadrupling of
the data storage requirements where no compression is applied,
and increases the redundancy of the data held in the
system.  
    Problems associated with precision (numeric resolution
of the combined hardware/software implementation of the model)
constrain the size of the numeric references which may be
held, influencing vector models in particular.   Many of the
commercially available GIS packages operate on 32 bit
computers, typically limiting single precision numbers to six
or seven significant digits (providing millimetre resolution
for extents of up to one kilometre, metre resolution up to
1000km).   To overcome this problem, some vector
implementations store location in files covering a defined
spatial extent using compact coordinates to describe the
offset from the origin of the extent (file).   Other
implementations permit the use of double precision (in this
case, 64 bit) coordinates with up to 16 significant digits,
readily permitting specification of location on a global
referencing system to millimetre resolution.   The influence
of these techniques is most felt in the overhead necessary to
convert sheet coordinates to real world coordinates or to
process unnecessarily large numbers for many of the
operations.  
    Problems relating to accuracy are more diverse, but
typically relate to decisions taken at the outset of a data
collection exercise.   For example, resolution of data
capture defines the accuracy of the data model, and it is not
possible to improve the accuracy simply, without recapturing
the data.   Constraints are usually imposed by the purpose
for which the model is established, and
  figure 2  
particularly by definition of the type and scale of
cartographic products expected to derive from the data
set.  
    While the spatial data models discussed in this
section ultimately rely on an explicit specification of
location, they do not necessarily require detailed spatial
references.   Rather, procedures must be available to permit
the use of higher level referencing and its ultimate
decomposition within the model.   Both direct and higher
level systems are in common use as locational referencing
schema.    
    LOCATIONAL REFERENCING IN SPATIAL MODELS  
    Locational referencing techniques may be broadly
grouped into those which provide an explicit reference and
those providing an implicit reference (which may require
resolution to an explicit value for some purposes).
  Explicit techniques rely on the specification of location
in terms of an offset from a fixed or arbitrary origin, and
typically either consider all references to define location in
the 'plane' of the Earth 's surface or explicitly supply a
reference to the third dimension.   These systems are
dominated by coordinate systems utilising either spherical or
Cartesian geometry.   Implicit referencing systems rely on
translation of some characteristic of the system to explicit
terms, and include hierarchical systems (for example, the
street network) and grid systems where the translation is
maintained by storage of data in an order defined by the
cell 's row and column offset from the grid origin.  
      Coordinate Referencing Systems    
    Locational referencing systems which utilise a
coordinate schema are the dominant techniques used in vector
representations of space.   Coordinate specification is made
in terms of a feature 's offset from an origin in Cartesian or
spherical space and has a numeric format.  
    Graticule models are based on the spherical geometry
of the Earth and utilise latitude and longitude to define
location.   Latitude is defined by the angular offset of the
feature from the Equatorial plane while longitude is defined
by the great circle which intersects the feature and both
Poles, and is specified as the angular offset from an
arbitrary origin (the great circle passing through Greenwich,
and both Poles).   The locational reference provided by this
system is quoted in degrees, minutes and seconds of arc for
both elements, and the feature is assumed to lie on the
surface of the Earth.   Accuracy of representation using
graticule models depends on the level to which locations are
defined, ranging from an accuracy of the order of +/- 30m at
the equator if degrees, minutes and seconds are specified, to
+/- 1800m if only degrees and minutes are specified.   The
major problems involved in the use of these models relate to
the variable spatial extent defined by
    10    1&deg;     of longitude (111km
at the equator to 0km at the poles) and to the difficulty of
providing a locational reference for a point without
considerable technical difficulty.  
    Cartesian models may be applied in both a two and
three dimensional sense, although the most common use is the
two dimensional referencing of the location of features on an
assumed planar surface.   Axes of the system are normally
defined in terms of a North-South and an East-West system, and
location is typically defined by grid references which define
the offset of a feature from an arbitrary origin on both axes.
  Accuracy of representation is a function of the scale of
the system and the number of digits specified in the grid
reference.   The most common specification utilises a six
digit reference, three digits for east-west location and three
for north-south location.   Accuracy is therefore a function
of the scale at which map data are portrayed and the density
of the grid.   Where coordinate data are supplied in this
form from a primary source (for example, field survey),
accuracy is a function of the data collection procedure.  
    The most common spatial referencing technique for
locational representation is achieved by means of the
Cartesian model, since it provides a uniformity of distance
representation in all directions from all points within the
extent modelled and additionally provides uniformity of
angular representation for small areas.   The feature common
to all of these systems is that location is defined by means
of the offset of the feature from a previously defined origin.
  They therefore explicitly hold information on the
topological
  figure 3  
relationships between features which allows distance between
features and relative orientations to be determined in a
direct and straight forward manner.   Coordinate data for
use in vector GIS are most typically described in terms of
offsets in metres from the grid origin, and the accuracy
depends on the scale at which the original data were
collected.  
      Implicit Referencing Systems    
    A number of systems exist which define location in
terms of position within a spatial structure.   Some of
these systems implicitly store data on the topological
relationships between features (for example, regular
tessellations), while others (for example, the street address
system), require an expansion or conversion of the locational
reference to define topology.   The street addressing system
is an hierarchial system which specifies location within a
hierarchy made up of region (suburb), network link in the
region (street or road) and offset from an origin on the link
(property identifier).   While the model provides an ability
to define location to the resolution of land parcel or
subparcel, it requires a pre-existing knowledge of the street
network system to enable determination of the location of the
addressed parcel in physical space.   The model has few
advantages for spatial processing since there is no mechanism
to store spatial relationships within the data, except by
extending the definition of the network.   For example,
while the network definition holds data on the relationship
between adjacent links, the relationship between distant links
requires a search through the network.   Similarly, the use
of the land parcel or property as the basic addressable unit
provides information only on the sequence of occurrence of
properties, rather than their relative positions in space.
  Ultimately, spatial analysis of data held in an indirect
form requires their translation into a coordinate form.  
    Tessellated data models implicitly store topological
relationships as a function of the way individual data
elements are addressed (and in many cases, stored).   The
clearest example is that of the family of square and
rectangular gridded structures where elements are addressed by
the row and column which they occupy in the matrix, and which
are usually stored in a physical form which parallels the
addressing scheme.  
    SPATIAL ANALYSIS AND LOCATIONAL REFERENCING  
    Successful utilisation of a GIS relies on both the
availability of data in a form suitable for analysis as well
as the provision of appropriate functionality within the
software in use.   Considerable dependence is therefore
placed upon the approach taken by the GIS, which ultimately
translates into the spatial model constructed.   Problems
which result from the use of an inappropriate approach or
model are highlighted by those related to boundary definition
and scale.  
      The Influence of the GIS Approach    
    The general model of a GIS recognises the differences
between spatial and non-spatial attributes and endeavours to
store (and analyse) them in a way best suited to their
character.   In particular, it requires the storage of
topological relationships (for example, nearness, adjacency,
connectivity) to permit spatial analysis and the creation of
new spatial entities.   This approach separates the spatial
description of different data themes and provides the tools
for their integration by means of intersection or overlay.
  Tessellated models using implicit locational referencing
provide the simplest example, since the data for different
themes are stored separately but in the same relative spatial
location in the tessellation.   Topological analysis is
readily achieved since distance and directional relationships
may be derived from the location of cells of interest in the
matrix, and overlay functions operate by selection and
comparison of data for each cell from the relevant themes.
  Vector data models typically record sufficient topological
data to permit polygon overlay and assessment of spatial
relationships to be achieved rapidly and also permit
construction of new polygons as a result of analysis by
dissolving redundant boundaries.  
    A more simplistic approach, frequently described as
the 'tea-bag' model, defines space in terms of a single,
usually irregular, tessellation, with all data attributes for
each unit stored as separate fields in a combined record (for
example, the New Zealand Land Resource Inventory).   In
common with the family of tessellated models, this structure
permits overlay analysis only as a function of the data themes
stored for each unit of the tessellation.   Typically,
results of overlay analysis are presented in terms of the
original polygon boundaries, even where adjacent polygons have
similar characteristics identified by the analysis.   This
model cannot, however, permit analysis based on distance and
directional relationships without either storing the
coordinates of a reference point within each unit as a data
theme or by reconstructing the topology graphically by
plotting the data.   While the precise analytical abilities
available depend on the functionality built into a GIS, the
computational overhead necessary to derive topological
relationships where they are not readily available places
significant constraints on the usefulness of a system.  
      Problems of Scale    
    Analysis within the framework of a GIS is effectively
independent of map scale since the results are determined by
computational means.   Results may therefore be expressed to
the precision of the hardware/software combination, frequently
implying spurious accuracy.   More significantly, as far as
the system is concerned, rarely is the mechanism available for
indicating the error margin associated with a locational
reference.   Hence data acquired from 1:50,000 map sheets
are considered as accurate as (and comparable to) that from
1:1,000 sheets.   Considerable onus is therefore placed on
the operator/analyst to ensure that due recognition is given
to the error margins associated with data themes.
  Disparities of this nature inevitably arise as a result of
analyses involving anthropogenic and resource data.  
      Problems of Homogeneity of Spatial Units    
    Tessellated data models rely on the assumption that
the basic unit of the tessellation is homogeneous.   This
assumption is frequently breached at the boundaries of
irregular spatial features where the data value stored for a
unit commonly reflects the dominant characteristic, resulting
in distortion to the shape of features (Figure 3).   While
these errors may not be significant where the application is
concerned with gross detail or summary measures, it rises in
importance in cartographic applications.   Similar problems
plague the classification of remotely sensed imagery where the
variation in the sensed surface is at a finer resolution than
the sensor is capable of achieving.   Vector data models
typically avoid such problems by allowing more accurate
modelling of boundaries, although the problem of variation at
a scale approaching the resolution of the model may still
arise.   Quadtree tessellations similarly avoid the problem
by utilising small cells to model the rapid variations
exhibited at boundaries while allowing large blocks in the
centre of homogeneous regions to be modelled by a few large
cells.  
      Boundary Characteristics    
    The character of the boundary modelled in a GIS
requires attention since each of the models assumes that it
describes a sharp boundary.   While this assumption may hold
for anthropogenic data, resource data frequently provide
conflicts, since boundaries between features may be gradual.
  Despite their 'fuzzy' nature in space, they are modelled
as a sharp boundary, providing a misrepresentation of reality.
  As yet, no commercial system permits the use of 'fuzzy'
boundaries in modelling space.  
      Temporal Stability    
    Temporal analyses are readily incorporated into the
GIS structure by treating time as an additional variable such
that each data theme is representative of the time at which
the data were collected.   This is achieved either by
preserving a separate layer in the database as a snapshot
taken at a particular time, or by storing the temporal element
as an additional attribute for each feature.   While
complexities arise where a feature has only a limited temporal
span, analyses requiring temporal comparison of data are
frequently concerned with the migration of features in space
and hence with boundary movement.   The results suffer
considerably if boundary changes are artifactual, being
related to data collection mechanisms, rather than
demonstrating true boundary migration.   In the natural
resources area, such problems may arise as a function of
varying sampling strategies during data collection, but are
relatively minor in comparison to the problems associated with
social data.  
    Analyses of social data typically rely heavily on a
regular census of the population carried out by government
agencies.   The results of such exercises are reported on an
aggregated basis to preserve the confidentiality of individual
data, and are thus heavily dependent on the aggregation
mechanism utilised.   In regions of rapid change,
considerable variation in the boundaries of regions may occur
between censuses, making interpretation of temporal analyses
difficult.   Although reporting agencies recognise these
difficulties and attempt to maintain a systematic reporting
structure (for example, the New Zealand Standard Meshblock,
where the only valid boundary changes occur by subdivision
except where boundaries are eased for other purposes and the
change causes little variation in the aggregate measures), the
history of boundary change is often ill-preserved and requires
considerable effort on the part of the analyst to untangle.
  An ever increasing body of opinion, probably best
expressed by the United Kingdom report on geographic
information (Chorley, 1987), is now pressing for the reporting
of data at the finest unit of aggregation possible to permit
more ready assessment of change with time and better
conformance to other boundaries.   In the United Kingdom,
for example, pressure is mounting for the 1991 census to be
reported on the basis of the postal unit, typically comprising
as few as 14 postal delivery addresses.  
      CONCLUSIONS  
    While GIS have the potential to become a major
analytical tool in the geographer 's domain, their application
will not be free of problems.   They suffer from similar
problems to those of the quantitative revolution since it is
possible to draw data from increasingly diverse sources into
analyses without comprehension of the assumptions involved in
their collection and modelling.   The potential for analysis
of enormous data sets in short periods of time also provides
scope for disaster as a result of blind acceptance that data
modelled in a system provide a realistic view of space.  
    While many of the problems discussed in this paper are
receiving attention at present, it is essential that GIS
techniques are applied with the rigour required of other
quantitative procedures and that analyses are designed to
operate within the assumptions of the underlying data
models.      
  

        Establishing Complexity Correlations
Between Pseudocode and Program Source Code    
  STEPHEN MACDONELL and PHILIP SALLIS  
  Computer and Information Science, University of Otago
      Abstract    
      Two major problems inhibit the use of three
widely cited software complexity assessment methods.   The
first is late derivation - measurement is often performed
after the coding phase, clearly eliminating the opportunity
for useful estimation.   The second significant problem is
one of implementation dependence - due to language
differences, the valid comparison of the complexity of various
projects is often impossible.    
      This paper demonstrates the application of metrics to
pseudocode descriptions of twenty-four algorithms and
compares the results with a similar analysis of programs
derived from the pseudocode.   The correlative results
obtained are suggested to be indicative of the outcome which
could be observed from actual instances of pseudocode and
program source code.   These results are given to provide
support for the use of complexity metrics in the prediction of
final-system characteristics.   It is suggested therefore,
that data such as this can be used to obtain higher quality
and higher productivity in the systems development
process.    
        1 Introduction      
    In general, the ultimate aim of any measurement
procedure is to enable those concerned to effectively control
their environment.   In terms of software complexity
assessment, metrics are used to provide data for the control
of certain aspects of the system development process.   This
can be achieved through model development, model-based
prediction and comparison of actual and predicted results.  
    Despite its relatively short history, software
engineering research has seen the development of more than
fifty models or techniques purported to provide complexity
measurement and prediction capabilities.   This number in
itself is indicative of the importance which is often attached
to this issue and the elusiveness of a single obvious solution
to the model choice problem.  
    There are several reasons why complexity measurement
is perceived as being important.   For example, project
management can be made more effective through more
accurate allocation of time and other resources.   More
complex systems are likely to be longer in development and
thus cost more.   Programs with high complexity levels we
assume, are more susceptible to logic errors.   The early
detection of the likelihood of errors through complexity
evaluation may also help to reduce development costs.
  Furthermore, if we assume that a more complex piece of
software is likely to require more rigorous testing, the
resources required for this testing can be more adequately
defined.   If such resource data is kept over time, some
cost forecasting for this aspect of the system development
process can be achieved.   It may be, as an extension of
this process, that subsequent maintenance effort costs can
also be estimated, and in some cases reduced.  
    Despite the large number of techniques suggested, at
least two significant problems have impaired their widespread
use.   The first concerns the late derivation of analytical
results - many methods are derived from the program code
itself (DeMarco 1984, Samson et al 1987, Londeix 1987) which
means that part of the effort has been expended before the
measure has been applied.   The second problem is one of
language dependence - results obtained from systems
implemented using one language are often not comparable
with those constructed with another coding method (Arthur
1985, Shen et al 1983, Miller et al 1987).   This reduces
the generalisable nature (and hence usefulness) of the results
from one analysis   in situ   to another.    
    It seems desirable therefore, to extend the
application of the measurement techniques to
  pseudocode   descriptions of the system.
  Pseudocode is written before coding is started, so the
metrics should provide more effective and more immediately
useful estimation results.   Our assumption is that
pseudocode is written prior to program construction.
  Intuitively we are aware that many forms of pre-coding
conventions are used to develop algorithms to a point where
language-dependent structures and phrases are used by
individual programmers.   In many cases we assume, no
pseudocode is ever developed, or is perhaps developed after
the coding has commenced for documentation or other
requirements.   Furthermore, if pseudocode is developed, it
is likely to follow in some respects the syntactic conventions
of its target implementation language.   Having said
that, we also acknowledge the desire by project managers and
others for higher quality and increased productivity in the
software development process which, in implementation,
manifests itself as a need for such things as pseudocode in
order to detect and reduce logic errors as early as possible
during the development process   For this reason, we began
to explore the complexity correlation existing between
pseudocode and its corresponding program code.  
    Finding a sample of program code which had been
derived independently from a corresponding set of pseudocode
representations was a non-trivial task.   We decided
instead, to define an initial study of a selection of software
metrics applied to a known set of pseudocode and corresponding
program code.   In effect, the purpose of this study is not
to determine absolutely the correlation between complexity
measured in the sample, but rather to demonstrate how these
measures may be applied to such a sample.  
      2 Pseudocode    
    The use of pseudocode in depicting the flow of program
logic evolved partly as a response to criticism of the then
prevailing flowchart representations.   Some researchers
(e.g. see Gane and Sarson 1979) extended this concept by use
of the term 'Structured English'.   The approach generally
uses statements similar to computer programming instructions
to provide a top down structural representation of the tasks
which the system is to perform.   The representation is
conceptually language and operating environment independent,
but often closely relates to the proposed target programming
language.   Figure 1 below illustrates the general concepts
of the method.   Variants of pseudocode abound (e.g. see Orr
1981), but in this paper we will confine discussion to the
generic form as illustrated in the Figure 1 example.   This
representation follows for the most part a COBOL syntactic
orientation.  
    Flowchart representations have been criticised for
    beign    being     cumbersome, difficult and
time-consuming to follow and maintain, and for not being
entirely suited to structured programming techniques (Stern
and Stern 1980).   They are also control-oriented and
environment dependent in terms of their use of hardware
symbols.   Pseudocode has therefore been used with
increasing frequency both for the documentation of systems
and for coding assistance.   Grauer (1983) remarks that as
well as being clear and concise, pseudocode representations
are generally quick and simple to write and are more likely to
be maintained if modifications are needed.   They are
sufficiently precise to serve as a real aid in program
development, while remaining informal enough to be
understood by those not directly involved in programming
and sufficiently non-syntactic to be
implementation-independent.  
      3 Measurement Techniques    
    Three widely investigated methods of complexity
measurement have been used in this study.   They are
lines of code metrics, Halstead 's software science measures
(1977) and McCabe 's cyclomatic complexity measure (1976).  
  Figure 1: an example of pseudocode.  
  caption  
      3.1 Lines of code metrics    
    Lines of code (LOC) techniques are widely used in
quantitative software assessment (Gaffney et al 1984).
  They are based on the often valid premise that a larger
program is more difficult to comprehend than a smaller one.
  There has been some empirical support for the continued
use of these measures (Gaffney 1984, Gremillion 1984) and they
have performed very favourably when compared with other more
complicated assessment methods (Basili and Hutchens 1983, Lind
and Vairavan 1989).   Furthermore these techniques are easy
to derive and use (Samson et al 1987) and can provide
consistent, useful results if a standard definition is
constantly applied.   Two specific measurement methods have
been used in this study.   The first involves the
determination of the number of non-commentary lines of code
(NCLOC) in the pseudocode and in the program.   This is
simply the count of all the lines occurring in each
representation excluding blank and comment lines.   The
second quantification scheme (applied only to the programs for
obvious reasons) involves
    countig    counting     non-commentary
non-declarative lines of code (NCNDLOC).   In effect, this
compares just the processing code in the programs to the
respective pseudocode representations (which incorporate
only processing information).  
      3.2 Halstead 's metrics    
    This comprehensive set of quantitative software
assessment techniques was enunciated by Halstead in a theory
known as   software science  .   The approach is based
on counts of operands (variables or constants) and
operators (symbols or combinations of symbols that
    effect    affect     the value or ordering of
an operand) in the implementation (Halstead 1977).   Four
fundamental properties form the basis of all of Halstead 's
work and it is this set of counts which were used in the
experimental phase of this study:
  formulae: definitions of   n    1  ,
  n    2  ,   N    1  ,
  N    2    
  The   vocabulary   is derived from these initial
figures as:
  formula:   n   =   n    1   +
  n    2    
and the implementation   length   as
  formula:   N   =   N    1   +
  N    2      
    From the basic parameters, several other formulae
were developed.   For example, one of the primary measures
formulated was the size measure,   volume  :
    V   =   N   log  2  
  n      
    Over large samples Halstead 's theory has been
extensively validated, in the prediction of such aspects as
error occurrence, error location time and maintainability
(e.g. see Fitzsimmons and Love 1978, Hartman 1982).
  Moreover, extraction of the measures can be automated
during compilation.  
    To its credit, the overall theory has at least
attempted to assess aspects of human cognition and it is based
on a finer level of assessment i.e. the focus is on
implementation elements rather than on complete lines.  
      3.3 McCabe 's cyclomatic complexity
metric    
    Whereas the two previous techniques take a size-based
view of software, this method adopts a significantly different
approach.   It is a topological measure, with its basis in
the control flowgraph of the code.   Complexity (v(G)) is
determined by counting the number of execution paths and
the number of functional nodes in the program:
  formula: v(G) = e - n + 2p  
  formulae: definitions of e, n, p    
    It is implied through the use of this measure that
difficulty of understanding is greater when the number of
distinct execution paths is high, as each must be followed to
gain a full understanding of the total functionality of
the software.   Positive empirical support for this
technique has been provided by Hartman (1982) and Moss (1988)
among others.   The measure is easy to derive
(Jayaprakesh et al 1987, Li and Cheung 1987), experimental
criticism is sparse and control flow, neglected in the
previous techniques, is fully considered.  
      4 Empirical Work    
    It must be stated again that this work has been
carried out to demonstrate the application of metric methods
to the notion of pseudocode-source code correlation.  
      4.1 The sample    
    Consistency of style in the sample was ensured because
all of the pseudocode and program listings were taken from the
same source.   Manning 's   Advanced COBOL: A Structured
Approach   (1985) provided an ideal test-bed, in that it
contained twenty-four examples of pseudocode and
corresponding programs, all adhering to the same structure
and format.   Furthermore, the programs varied in their
length and function.  
    As the pseudocode for each example was developed from
a hierarchy chart, some degree of modularisation was used at
an early stage.   This is appropriate for the development of
well-structured final code.   An example of the
transformation from chart to pseudocode is shown in Figures 2
and 3.  
  Figure 2: Hierarchy chart: Funding report example  
  Figure 3: Pseudocode: Funding report example  
      4.2 Experimental procedure    
    The results were obtained by the authors in one
analysis session.   While these results could therefore be
considered as interim, the procedure is valid because of the
demonstrative nature of the study.   It is clear however,
that a larger group of analysts would provide greater insight
into the degree of confidence which can be placed upon these
results, together with the use of software tools such as a
program parser.   This extension to the experiment is
currently being performed by the authors and includes one
group of programmers deriving source code from the pseudocode
created by another group.  
      4.3 Comments on the counting methods    
    In the analysis of the pseudocode all verbs and
comparatives which were not part of a program-like construct
were considered to be operators.   Anything else (i.e.
names, labels etc.) were considered to be operands.   The
full stops '.' used in the program code were disregarded in
the token counts as a matter of choice.   This of course,
strengthens the likeness with pseudocode in the sample used in
this study because no full stops are used.   They are
implied however, so it seemed fairer to eliminate them from
the source-code count.   The END Procedure-name constructs
as used     inth    in the     pseudocode
representations were also disregarded, as no explicit
equivalent was used in the program code.   ORs, ANDs, AT
ENDs and INVALID KEYs were considered as distinct branches in
the assessment of McCabe 's measure.   Overloading of symbols
or names was not assessed.  
      5 Results    
    Correlation and regression techniques were used to
determine the existence of useful relationships between the
statistics obtained from the pseudocode and the programs.
  The statistical results are summarised in Table 1
below.  
      5.1 LOC metrics    
    Very strong linear correlations were evident in the
results for the lines of code measures.   This at least,
confirms the strength of the relationship between the
informality of pseudocode and the formality of source code.
  The correlation coefficient (Pearson 's product moment
coefficient   r  ) between the pseudocode and
non-commentary lines of program code was 0.94.   Between the
pseudocode and the number of non-commentary non-declarative
lines, the correlation coefficient was 0.97.   Linear
relationships were strongly evident in plots of the respective
data sets (Figure 4) and linear regression results also
reinforced this conclusion.   The R  2   statistic,
representing the explanatory power of the regression model,
was 0.88 (where the upper bound is 1) for the pseudocode
prediction of NCLOC and 0.94 for
    NCNCLOC    NCNDLOC     prediction.  
  Figure 4: Pseudocode LOC plotted against Code NCNDLOC  
      5.2 Halstead 's metrics    
    Similarly strong results were obtained in the
relationships between the pseudocode and the programs for all
four of Halstead 's basic parameters.   Correlations of 0.92,
0.95, 0.91 and 0.97 were observed in the two values for
  n    1  ,   N    1  ,
  n    2   and   N    2  
respectively.   Plots again revealed definite linear
relationships and this was also evident in the regression
results.   The R  2   levels for these
    predicitive    predictive     relationships
fall between 0.83 and 0.94.  
      5.3 McCabe 's metric    
    Further support for the predictive value of
pseudocode-based measurement was provided from the results
derived for McCabe 's measure.   A correlation of 0.99 was
achieved for this measure under the two representations.
  The data point plot (Figure 5) and the explanatory level
were similarly strong, with an R  2   of 0.97 for the
predictive relationship.  
  Table 1: Statistical results  
  Figure 5: Pseudocode cyclomatic complexity plotted against
Code cyclomatic complexity  
      6 Limitations of the study    
    Quantification of both the pseudocode and the program
code was performed manually by the authors.   This was
necessary to ensure consistent interpretation of frequently
used constructs (particularly in the pseudocode), but this
did leave the procedure open to human error and subjectivity
in counting.   Furthermore, because the purpose of the work
was to demonstrate how software metrics can be applied to
pseudocode and its corresponding source code, there is a lack
of rigour in the experimental design.  
      7 Conclusions and Future Directions    
    Notwithstanding the demonstrative nature of this
paper, based on the results obtained from the empirical work
reported here, it would appear that accurate predictive
complexity data can be obtained from pseudocode
representations of software systems.   All of the three
techniques investigated showed very strong positive linear
relationships between the two representations, so it would
seem that any of these methods could be used with
approximately the same level of accuracy.   The aim of this
study was not to validate the methods themselves but to
determine the existence of useful predictive
relationships.   If a high correlation exists between
pseudocode and source code, it would seem that prediction of
complexity at an earlier time in the systems development
process can be achieved.   This could then positively affect
the structure and content of programs thus reducing the
likelihood of errors in the code construction phase.   This,
we suggest, (see Sallis 1989) will reduce post-implementation
maintenance and therefore, improve overall productivity.  
    It is clear that as outlined earlier, more
experimental work is needed to validate the results obtained
in this study.   The experiment was conducted in good faith
but its limitations were well realised.   This study used
only COBOL programs, so more immediate work could concentrate
on determining the existence of similar relationships for
programs implemented in other languages.   In the long
term however, work should be centred around even earlier
descriptions of the system i.e. the specification of the
software.   If accurate predictive data can be obtained
directly from software specifications, the opportunities
for useful estimation clearly become far greater.  
      Overall, we are endeavouring to further the cause
of higher quality systems and greater productivity in the
systems development process, which can only, we suggest,
be achieved by reducing the cost of post-implementation
maintenance&semi; and that means reducing logic errors as early as
possible in the development process.      
  

          Quantum leap for RNZ
offices      
    Technology with the potential to create a "paperless"
radio station, and a music recording studio representing a
world first in acoustic design are two features of RNZ 's new
Auckland premises.  
        R    adio New Zealand 's new
headquarters in Auckland sit next to one of the city 's
noisiest intersections.   Yet within the cobalt blue
building not a whisper intrudes from the outside.    
    The special acoustic requirements of RNZ have been one
of the chief factors influencing the refurbishment of the
former Kodak photo processing laboratory on the corner of
Cook and Nelson Streets.   The entire building is double
glazed, with triple glazing for the Newstalk 1ZB station.
  Walls, floors and ceilings
  photo    caption  
have been specially designed to meet stringent acoustic
standards, and in the recording studio, the most demanding
environment, RNZ has achieved a world first in acoustic
design.  
    Jasmax, the interior designers for this project, have
created an aesthetically pleasing environment for RNZ
employees and the overall layout of the work space is a
quantum leap from RNZ 's first home in Durham Lane.   An
example is 1ZB (now Newstalk 1ZB), New Zealand 's oldest radio
station (it was founded in 1926) which now operates from
spacious accommodation with wide views of the city and
harbour.   No bunker-like studios here.   Especially
designed to cater for its Newstalk format, the studios provide
an extremely pleasant environment for the people who put out
the news.   Says programme director Trish Carter:
  "We feel like we 're part of Auckland now."    
    RNZ 's original accommodation in the heart of Auckland
city was built in the 1930s.   At the time it represented a
most modern example of a radio environment, but after half a
century of service it was showing its age.  
    RNZ considered leasing, but instead opted to buy the
Kodak premises, gut them and completely refurbish,
anticipating a 15-20 year commitment to the new
accommodation.   The project, an enormous undertaking
done to tight deadlines, was the most expensive single
undertaking ever made by RNZ.  
      Fortuitous choice    
    Architect Grant Coupland says the three-storey Kodak
structure - essentially a warehouse - proved a fortuitous
choice.   The floors were capable of taking heavy loads and
one floor in particular had unusually high studs - tailor-made
for the recording studio.  
    There was no time to waste, however.   RNZ took over
the building on September 1 1989.   The first control
rooms had to be in place by December.   Engineering and
Operation were responsible for the technical project
management and in view of the tight deadline opted to
prefabricate at Huntly six control rooms and eight voice
booths, each weighing between five and 10 tonnes.  
    The rooms, complete with wiring and control desks,
were inserted into the building through holes left in the
walls and "skidded" across the floor and into place.  
      Unifying move    
    The new building brings together formerly
disparate elements of the RNZ team.   Newstalk 1ZB for
example, used to have its journalists in Albert St and its
presenters and sales reps in Durham Lane.   Te Reo Aotearoa
was also housed in Albert St, having moved from
Papatoetoe.   It is now located on the first level, the
same floor as the drama studios, with the Newstalk 1ZB studios
(the former Kodak cafeteria) on the third floor, and the new
recording studio in between on the second.  
    The Newstalk 1ZB news team moved into their new
quarters on April 14.   A central control room is
surrounded by seven rooms for on-air use, five news booths and
an editing booth.   Newstalk 1ZB administration and
sales staff work on the next floor down.  
    The design priorities for the Newstalk 1ZB news room
posed special challenges for the acoustic engineers.
  Visual contact between presenters, programme staff and
journalists was deemed a priority, hence a lot of glass has
been used.  
    The walls were required to be as thin as possible.
  Brian McGettigan, RNZ acoustic expert and project 's
acoustic consultant, looked at several studios abroad and
spent many hours working on a computer model before coming up
with the chosen design.  
    The walls, which are 180 mm thick, use lead as part of
their lining.   The doors, specially designed and built
by Winstones, have magnetic seals.   The outer glass walls
are triple glazed and those between the rooms are double
glazed.   Services such as fire sprinklers and air
conditioning have been isolated.  
    RNZ has brought most of its technology with it
from Durham Lane.   However it is still among the most
advanced in New Zealand and in some cases unique.   This can
be said of Touchstone, a software-based presentation system
which is helping 1ZB move closer towards the ideal of a
paperless radio station.  
    Touchstone, from Media Touch Systems Incorporated
of Salem, New Hampshire, was bought by RNZ about five
years ago for evaluation.   It is the only one
    if    of     its kind in New Zealand and one
of few in use around the world.  
    The Touchstone system centres on an IBM PC-based
controller which performs all the audio and control switching
functions of a radio station announcer 's console.
  It stores and manipulates a schedule which is displayed
for the announcer, greatly simplifying his/her job by
replacing the keyboard with a touchscreen.  
      Touchscreen technology    
    RNZ has three touchscreens (located in the news/sports
studio, the weather/traffic studio, and the announcer 's
studio) and a master in the producer 's studio.   A full
account of everything happening in the station is available
via the screen.   Touchstone can be operated from any of
the studios, allowing RNZ to reduce the need to duplicate
such facilities as tape and cartridge machines.  
    The touchscreen switching system is triggered
literally by touching the appropriate portion of a grid
display on the screen, allowing the operator to do just about
anything from simply broadcasting a commercial to
rearranging the order of the commercials, news, weather or
inserting music.  
    Touchstone is responsible for commercials and
every other piece of presentation that happens in the radio
station.   It has allowed RNZ to design its present
equipment to suit the newstalk format (most systems are
designed for music stations).   The old control desk has
been retained however, just in case it 's needed.  
    The telephones and microphones do not operate through
Touchstone&semi; they use conventional technology.
  (Newstalk 1ZB uses a Studer hybrid telephone system
which incorporates an EvenTide digital delay facility.
  It is used for talkback programmes in the evening and for
programming calls from local and foreign correspondents).  
    The Touchstone software is used in conjunction
with Columbine radio programme scheduling software, a
Pegasus financial package and Displaywrite wordprocessing.
  The Columbine software ensures preprogrammed conditions
are met - eg, two news broadcasts don't follow one after
the other.   (The operations manager represents the hourly
radio sessions as a pie chart and this information is
keyed into the computer by the Columbine operator).  
    Columbine produces a spool file which Touchstone
captures.   A report of the day 's schedule is thus produced
and a print out of the log of the following day 's schedule is
also possible.   Late changes are made on the Touchstone
system, which also accepts commercials downloaded from the DAS
machine.  
      Storage doubled    
    Commercials are stored on disk.   The DAS machine
replaces a CUERAC.   Its storage capacity has been doubled
from the original two-and-a-half hours.  
    RNZ has made a number of enhancements to
Touchstone primarily because it won't accept the downtime
tolerated by stations in the US.   Despite the R&amp;D that has
gone into Touchstone, it is still regarded as an economical
installation.  
    Touchstone interfaces with the Electronic News
Processing (ENP) system which has completely replaced
typewriters in the newsroom.   The speed and efficiency of
ENP technology provides major advantages for an
organisation like RNZ.  
    Newstalk 1ZB transmits its own bulletins and takes
news from the network from 7 am to 6 pm.   Each reporter has
a VDU.   They enter their stories into the database, and
these are checked by a subeditor.   Those of national
interest are sent to Wellington.   Late breaking stories
can be sent to Wellington and broadcast within minutes.  
    While the station 's technology has made life easier
for those who work there, it has also become a drawcard for
new talent.   Trish Carter says people applying for
positions with the station frequently cite the technology
as a major reason for moving to RNZ.  
      Acoustic design    
    The second level of the RNZ building houses music
studios which represent a world first in acoustic design.  
    The music studios comprise two control rooms (one for
the Concert Programme 
  photo    caption  
and one for National radio) looking out onto a performing
studio.   Traditionally several studios were necessary
to record different types of music.   Because of the unique
acoustic equipment in the new studio, this is no longer
necessary&semi; it can record anything from pop to chamber
music, and drama as well.  
    RNZ acoustic expert Brian McGettigan says he worked
towards a reverberation time of 0.4 seconds and 1.5 seconds
for the two extremes, although in fact the end result is
considerably better than that.  
    The key elements in achieving this centre on two
devices: a Variable Studio Acoustics unit (VSA) designed
by ComputerSwitch of Tasmania, Australia, and an ADR unit
McGettigan designed himself.   ComputerSwitch devised a
computer control system for both types of unit.
  Installation, completed at the end of October, has
been a joint venture between RNZ and ComputerSwitch.  
      Time-saving    
    The performing studio uses 79 VSA units each
comprising 600mm x 1200mm movable blinds mounted on the
ceiling.   The modules, each of which has a microcontroller,
can be individually adjusted by a studio engineer using a
computer mounted in the audio desk.   The operator uses
a trackball, not a keyboard, and can see exactly what is
happening via the graphic display on screen.  
    Individual acoustic configurations are stored in the
computer 's memory for future reference.   The computer
controlled system saves a lot of setting up time.   It also
makes fine tuning easier and quicker.  
    The 37 ADR units, a patented RNZ device, are
triangular in shape, 600mm wide and 3m high.   They line the
walls of the studio.   The three surfaces are in turn made
up of hard Tasman oak, an absorbent foam surface (RNZ Foam
120) and a quadratic root diffuser (QRD).   Any of these
surfaces, which rotate in one degree increments, may face into
the room in any combination.  
    McGettigan says RNZ has been able to reduce the size
of the recording studio from 100m  2   to 70m  2  
without reducing the performance space&semi; the diffusing surface
allows the artists to work within 500mm of the wails.   In
conventional studios artists need to be about one-and-a-half
metres away.   This represents significant savings for RNZ.
  In addition, McGettigan believes there is a large
international market for the system.  
      Floating floor    
    The recording studio itself has been separated from
the rest of the building.   The   walls   700mm thick
walls comprise an external wooden-framed slab-to-slab wall, a
concrete block wall slab-to-slab and, inside on the floating
floor, another wooden-framed floor and ceiling.  
    The floating floor comprises a reinforced concrete
slab with a polythene break layer superimposed on the
building 's existing floor, and 150 rubber cones.
  McGettigan says that by using rubber he
    as    has     achieved an NC reading of about
11, more than   than   the NC15 he had been working
towards and 14 points below the standard control room level of
NZ25.   The fire sprinklers and air-conditioning have been
isolated.  
    The two control rooms adjacent to the recording studio
each comprise a live end and a dead end, with the production
engineer sitting in the middle who has at his or her disposal
the latest in recording equipment.  
    While the Touchstone system and the acoustic design of
the music studios are probably the most unique aspects of
RNZ 's new home from a technical point of view, other changes
have been made to provide the best in facilities and working
conditions for employees.  
    One example is the new technical workshop, which
shares the first floor with Te Reo Aotearoa, the public radio
studio for drama&semi; National radio presentation and the archival
section&semi; the workshop is open plan.  
      Integrated circuitry    
    Technical manager Robin Moor explains that the
equipment serviced in 1990 is predominantly based on
integrated circuitry, and the design caters for this with
tables and chairs rather than workshop benches, antistatic
pads for CMOS equipment and an adjacent area for plotters,
Cad PCB draft work and termination data.  
    The workshop allows for a full range of servicing
tasks for broadcasting clients, and to plan, instruct and
commission turn  -  key projects for a varied client
base.  
    The facilities now employed by RNZ in its new
headquarters make it one of the most advanced stations in
Australasia.   RNZ has made a commitment to its present site
and the standard of the building reflects that.    
  

        Maui Stage II Development Project - technical
overview    
  byline  
      The Maui gas condensate field, 35 to 50 kilometres
off the coast from Opunake in South Taranaki, was discovered in
January 1969.   Development of the field and the agreements
between the respective parties are described in the White Paper
published in 1973.  
    The White Paper envisaged that the Maui field would be
developed in two stages, Stage I being the installation of the Maui
A platform, the development of the onshore processing
facilities at Oaonui and the onshore distribution pipeline, all of
which were completed in 1979.  
    It was also intended that a Stage II development would
follow shortly thereafter with the installation of a Maui B
platform.   Stage II had an expected onstream date of 1983.
  This development was well in hand during the late 1970s when a
major change in national energy forecast caused this development to
be deferred.  
    Extensive studies carried out during 1986 examined field
performance against projected demand for gas and established
the need for an additional production platform in the Maui field to
be in place by the winter of 1993.  
    Engineering studies were initiated to define the best
development option, culminating in a development plan, basis
for design, project specification and project execution plan.
  These formed the basis on which the project has been designed
and is currently being constructed and managed.  
    Put simply, the completed Maui Stage II Development (MSIID)
project will enable the Maui owners to maintain a capacity to
continue to produce gas to meet the Maui Gas Sales and Purchase
Contract.      
  Figure 1: Maui location map  
    The MSIID project will not increase the gas production
above that of the original capacity of Maui A alone which is now
declining as the reservoir depletes.  
    The principal components of the project comprise:
  list    
      Maui B Platform    
    The Maui B platform has been positioned to recover
hydrocarbon fluids from the reservoir in the southwest lobe of the
field (See Figure 1).  
    A three well appraisal drilling programme conducted in
1986 determined the B area of the field to be isolated from the A
area in the hydrocarbon bearing zones but in pressure communication
through the underlying water zone.  
    The selected configuration for Maui B is different from the
proposals set out in the White Paper which envisaged a standalone
integrated platform supporting the wells, production facilities
and accommodation for drilling, production and maintenance
crews.  
    Maui B has been designed with simplicity in mind and
will be a satellite to the A platform.   As such it will support
a drilling equipment set for drilling the eight wells, pipe work to
manifold the production of the eight wells into the pipeline,
minimal utilities to support the production, controls and basic
safety systems which will be monitored remotely from the Maui
A platform via a microwave link and a shelter module for operations
and maintenance crew who may be stranded on the platform through
inclement weather.  
    The platform is designed to operate in a normally unmanned
mode, with personnel commuting from Maui A on a day work basis
about one visit per week.  
    The platform is to be located in 110 metres of some of the
most exposed and turbulent water on the New Zealand coast.   It
has been designed to withstand 20.6 m high waves, 45.3 m/s wind and
"strength level" and "ductility level" earthquakes of return
periods 350 and 3000 years respectively.  
    A number of support structure types were studied including
the use of concrete, but the final selection was a tubular
steel space frame which will be pinned to the seabed by twelve
1.872 m diameter piles driven some 45 m into the seabed.  
    Designed by Earl and Wright in San Francisco, the
substructure weighing some 6 500 tonnes is being fabricated by
Nippon Steel Corporation in Japan.  
    The deck structure, or topsides for the platform, is being
built of steel although it will be equipped with an aluminium
helideck, glassed reinforced epoxy
   Figure 2: Pictorial representation of the Maui II Development
project  
walkway gratings, duplex stainless steel process piping, kunifer
piping for sea  -  water service and a selection of
corrosion resistant materials in the well completions to
minimise maintenance and hence reduce the need for personnel
attendance on the platform.  
    The topsides was designed by EB Global Engineering in New
Plymouth and is currently under construction as an integrated unit
by Sembawang Engineering Limited in Singapore.  
    Drilling equipment purchased second hand in the USA is
being packaged in Singapore for installation on the topsides on
location.  
      Maui B drilling    
    To minimise facilities on Maui B the drilling of the wells
is to be carried out with the support of a drilling tender vessel
moored alongside the platform.  
    The drilling tender will provide accommodation for the
drilling crews, mud pumping and processing, cementing and a number
of utility and control functions.  
    The actual wells will be sunk from the B platform.
  Personnel will transfer between the drilling tender and the
B platform via a telescoping bridge which is designed to
accommodate the relative movement from a one year return period
storm.   The bridge and umbilicals will be disconnected if the
weather exceeds this level.  
      Maui B to Maui A pipeline    
    The untreated wellstream fluids from Maui B will be
exported to Maui A through a single 15 km long, 508 mm diameter,
two-phase flow pipeline.   This pipeline has to withstand the
closed in wellhead tubing head pressure of 220 bar and a maximum
operating temperature of 78&deg;C.  
    Hydraulic analysis of the flow regimes predict flow in the
stratified and intermittent flow regimes.   Liquid slugs are
predicted and these will be handled by high pressure receiver
vessels on the Maui A platform.  
    Water is present in the wellstream and hydrates (ice-like
substances) have been calculated to occur at temperatures less than
20&deg;C, posing risks of pipeline blockages.  
    To overcome this threat the pipeline will be buried under
a minimum cover of one metre to provide insulation to maintain
outlet temperatures above the hydrate temperature under normal
operating conditions.  
  photo    caption  
    During extended shutdown, glycol will be injected on Maui
B to prevent hydrate formation.   Should this not be possible in
time, then the pipeline will be depressurised from Maui A.  
    The well fluids can contain up to 4 mol percent of carbon
dioxide which in combination with produced water forms a
corrosive mixture.   For this reason the pipeline is internally
clad with a 3 mm thick 316L stainless steel material.   The
structural strength of the pipeline is provided by 19 mm thick
carbon steel.  
    External corrosion protection of the pipeline is achieved
by a factory-applied polypropylene coating and zinc anodes.   The
one metre of cover over the pipelines provides thermal
insulation but also prevents thermal upheaval buckling.  
    Unfortunately, the in situ soils have insufficient strength
to restrain the pipeline and as a consequence aggregates with
a particle size range of 1 mm to 60 mm will be imported from
quarries in the north Taranaki region.   It is anticipated that
between 120 000 and 200 000 tonnes of aggregates may be
required.  
    Whilst the Maui B platform will be delivered to site with
its pipeline riser already installed, this is not the case at Maui
A where a new riser has to be retrofitted with support from a
diving vessel.  
    The pipeline ends will be connected to each of the
platforms by means of hyperbaric welds which will be undertaken
from a dynamically positioned diving vessel using an underwater
habitat in which a computer controlled automatic TIG welding
process will be employed.  
      Maui A modifications    
    All processing of Maui B wellstream fluids will be carried
out on the Maui A platform.   Fluids arriving on the platform via
the pipeline will be directed to a new high pressure separator
vessel where initial gas/liquid separation will take place.  
    This vessel has been sized to accept the large liquid slugs
generated in the pipeline riser and will thus stabilise the flow
for processing.  
    The gas from the separator will be directed to the
existing low temperature separation (LTS) trains with the liquid
stream being directed to a low pressure separator where
condensate/water separation will occur.  
    Both space and weight constraints dictate that one of
the existing 12 LTS trains be removed to make room for the new
equipment, requiring extensive rerouting of cabling and
pipework as well as module floor strengthening.  
    Extensive modifications are required to the instrumentation
and control systems to accommodate the new facilities on Maui
A but also to provide remote monitoring and control of Maui B.  
    Major work is also required to provide for the installation
of the new riser plus its associated pig receiving and handling
facilities.  
    The major challenge of the work on Maui A has been to
execute the extensive modifications whilst the platform
continues in operation, producing some 90 percent of New Zealand 's
gas production.  
      Maui production station Oaonui
modifications    
    Extensions to the Maui production station facilities at
Oaonui have two main objectives.   The first is to control the
calorific value and C6+ component content of the Maui gas
stream.   The second is the provision of additional fractionation
capacity to handle the expected increase in liquids from the higher
condensate to gas ratio of the Maui B reservoir.  
      Gas specification and C6+ component
control    
    The existing plant was designed to chill the gas stream to
minus 15&deg;C in order to achieve the gas specification.
  Apart from a higher condensate to gas ratio, the Maui B
reservoir has a lower percentage of inert gases, requiring
deeper refrigeration of the gas stream to remove the heavier
hydrocarbon components.  
      An   Additional refrigeration plant has been
installed, chilling the gas to minus 28&deg;C which is the
lowest temperature consistent with the metallurgical and design
limits of existing equipment.  
    This deeper chilling will reduce the residual C6+ content
of the gas by around 50 percent, thus providing a greater liquid
yield.  
    Another major advantage of deeper chilling and consequent
removal of the heavier residual components is that surplus
liquefied petroleum gas (LPG) can now be re-injected into the sales
gas pipeline whereas in the past it often had to be flared.  
    The extra chilling has been achieved by the addition of a
secondary gas chiller in each of the existing three gas trains plus
the addition of two 3 500 kW gas turbine driven refrigerant
compressors handling the propane refrigerant.   This plant came
on stream in September of this year.  
      Additional condensate fractionation    
    An additional condensate fractionation train of nominal
capacity 18.5   l  /s against the two existing fractionation
trains each of capacity of 31.5   l  /s is being installed.
  The two existing trains are three column systems comprising a
de-ethaniser, de-butaniser and de-propaniser.  
    The new train will be a two column system consisting of a
new de-ethaniser and the conversion of a de-propaniser from one of
the existing two trains to a de-butaniser.  
    Utilities requiring upgrading as a consequence of the
modification work include:
  list  
  Diethylene glycol (DEG) is currently used for hydrate
suppression in the gas chilling system and in the condensate
pipeline from Maui A.   When the C6+ project is commissioned
mono-ethylene glycol (MEG) will be used in the gas system, it being
more suited to use at the lower temperatures.  
    An onsite glycol purification system will also be
incorporated to overcome a current salt contamination problem which
is predicted to worsen after Maui B start-up.  
    The modifications at Oaonui are expected to be complete
by the end of 1992.   As with Maui A, stringent safety measures
have been implemented to enable simultaneous construction and
operation.  
      Offshore installation    
    The offshore installation of the Maui B platform and the
pipeline are scheduled to take place over the summer months of
1991-92.  
    Working in the tempestuous Tasman Sea is arduous and
costly, which has led to the selection of some of the biggest and
most sophisticated equipment available to the industry.  
    The prime piece of equipment to be used during the
installation is the semi  -  submersible crane vessel Balder
which will be deployed by Heerema Marine Contractors Limited to
install the 15 km pipeline between the two platforms&semi; support the
installation of the 6 500 Maui B substructure&semi; drive the 12 Maui B
piles and 10 well conductors&semi; lift and place the 2 500 tonne
integrated deck onto the substructure as well as a series of
smaller lifts associated with the drilling equipment set.  
    The Balder is a giant unit some 130 metres long by 86
metres wide with facilities to accommodate up to 350 people
aboard although this will not be necessary in the case of Maui.
  It is equipped with two very large revolving cranes, one
capable of lifting 3 600 t and the other 2 700 t at their minimum
reach.  
    The larger of these two cranes will be working close to its
limits when lifting the 2 500 t Maui B topsides into place.  
    When Maui A was installed in 1975-76 it took over a year to
drive the piles.   Pile driving technology has advanced since
that time.  
    The 12 Maui B piles, each 65 m long, will be driven by a
Menck MHU 1700 hydraulic underwater hammer which has a rated
capacity of 1 700 kNm.   Each pile is expected to take less than
a day to drive.  
    Installation of the pipeline will require the
deployment of a dynamic positioned dive support vessel.
  Whilst the specific vessel has not been selected yet it will be
around 100 metres long with a dual bell saturation diving
system.  
    This unit will initially assist with the installation of
the gas riser on Maui A then carry out the hyperbaric welding of
the pipeline to the risers.   A submarine plough weighing 150 t,
will trench in the pipeline ready for burial with land quarried
aggregates.   These will be placed by the dynamically positioned
dredging vessel Trollnes I through a fall pipe and remote operated
vehicle (ROV) placement head.   Sonar systems will be used to
verify the specified thickness of overburden.  
    It is planned that the Maui B platform will be installed by
March 1992 with the placement of aggregates taking about two months
longer to complete.   Drilling is expected to begin shortly after
the B platform is installed, in April 1992.  
        Project management    
    Shell Todd Oil Services Limited (STOS) is contracted by the
owners of the Maui field - Fletcher Challenge Petroleum
Investments, 20 percent&semi; Petrocorp Offshore (No 7), 48.75
percent&semi; Shell Petroleum Mining, 18.75 percent&semi; Todd Petroleum
Mining, 12.5 percent - to operate the field and manage the project
on its behalf.  
    STOS has established a dedicated project team to
execute the MSIID project from its New Plymouth office.   The
team is largely staffed by personnel transferred from its
technical, operations and administrative functions.  
    Management systems substantially drawn from the corporate
organisation were established at the outset and applied within
the structure of a quality assurance system.  
    Safety is of paramount importance to the company and the
project and receives as much attention as quality, schedule and
cost objectives.   Commitment to safety is sought from
contractors engaged on the project, and is vigorously enforced
by the management team.  
    Safety performance targets set for 1991 are Lost Time
Injury Frequency (number of lost work cases per million manhours
worked) of 0 for the project management team and 3 for contractors.
  As at the end of July performance was recorded as 0 and 0.9
respectively.  
    The project is progressing to schedule and confidence is
high that it will be completed as planned at the end of March
1993.      
  

        A quiet revolution    
      T  here has been a quiet revolution in
electrical instrument design which challenges the traditional
philosophy of electrical calibration.   While the adoption
of new and exciting quantum effects to help define the volt
and ohm have taken up much of the time of scientists at
various national standards laboratories, equipment
manufacturers have also been busy developing new solutions to
measurement problems.  
    Only a dozen years ago, the idea of using an
electronic instrument as a laboratory standard was
regarded with suspicion.   The electronic intrusion was
tolerated only if there was a "real" instrument available to
check it against, where "real" generally meant
electromechanical.   This suspicion was perhaps justified by
some of the earlier attempts at digital voltmeters but
thankfully there has been a considerable improvement since
then.   In fact, accuracy of the best electronic instruments
of today is such that they require intercomparison directly
against the national standards.  
    At first the developments were largely a case of
applying the computing power of microprocessors to provide
convenient features for the users, rather than to enhance the
basic accuracy of the instruments.   There is no
doubting that the great advances in computer control and speed
of data acquisition met a very real demand for a great number
of applications.  
    From the calibration perspective the first significant
development was the adoption of software calibration
constants.   These allowed the readout of an instrument to
be adjusted to the true value without adjusting a variable
resistor with a screwdriver.   This now meant that an
adjustment could be made without risking the long  -  term
stability of the instrument.   However, for those of us in
the business of providing calibration reports it was
disconcerting to realise that the value of the reports was
dependent on numbers stored in memory, probably relying on
battery backup.   The ability of battery failure to destroy
an expensive meter calibration was clearly not popular with
customers.   A trim pot adjustment could be protected with a
calibration seal, the protection of a software constant
relied on the manufacturer 's implementation.   Instruments
which reported the calibration constants being used were
clearly preferable to those which hid this information.  
    The second significant development has been the
increasing use of software to directly enhance the accuracy of
the instruments.   The best meters and calibrators of
today are capable of using a minimum of externally applied
stimuli to calibrate themselves over their full operating
range.   A meter or calibrator presented with a 10V
reference is capable of internally deriving a voltage scale
from micro to kilo volts.   The extensive scaling procedures
previously carried out by skilled standards laboratory
technicians are now executed in firmware.   These techniques
can also be used to extend the usable temperature range
without loss of accuracy.   The potential benefits of
such schemes are enormous but they can lull users into a false
sense of security.   There is the obvious problem of
ensuring the reliability of all the extensive internal
switching required for these schemes.   Also it is a
sobering thought that a firmware implementation may use of the
order of 100,000 line of code and even for a high-quality
instrument the manufacturer would expect a few defects in
the code.   One manufacturer detected an obscure and
essentially benign fault which had existed in a range of
instruments for 10 years without ever being reported by users.
  Our experience is that the defects are not always benign
and can sometimes generate subtle but serious faults.  
    The task faced by a calibration laboratory is to
develop measurement procedures which cope with this new
complexity.  
    A thorough understanding of the design principles of
an instrument is a necessary starting point.   The skills
required to achieve this understanding have shifted from
traditional electrical and electromechanical principles to
analogue and digital electronic circuit design, and now to
software.   Unfortunately the firmware documentation
provided by manufacturers is minimal and so the techniques
being used in any particular instrument are not always
clear.   Traceability can only be established if there is
documented evidence of the performance of the internal
calibration process.  
    From the average user 's point of view the most likely
trap is to be confused by the multitude of terms used by
various manufacturers.   "Selfcal", "Calcheck", "Selftest",
"Autocal", may all make a great deal of sense after a few days
with the operator 's manual but in a sales brochure can lead to
unrealistic expectations.   A manager approving the
purchase of such an instrument may be surprised to learn that
the purchase price is only a small part of the cost of
ownership.   Even a "self  -  calibrating" instrument
needs to be maintained according to a strict documented
procedure with careful analysis of the wealth of information
provided by the software.   Furthermore, although the
frequency of calibration at a standards laboratory may be
reduced such a calibration   photo  
  caption  
will still be required at regular intervals.
  Of course the software analysis, if done properly, will
allow decisions on the calibration frequency to be based on
the performance history.  
    For a company conforming to the requirements of the
ISO 9000 series on quality management and quality systems, the
control of measuring and test equipment is just one of
many areas requiring solutions.   Although the company will
undoubtedly have the technical skills required for producing
its product it is unlikely to have the skills of a
measurement scientist.   Seeking advice early about
appropriate instrumentation and procedures can save money.
  It is unfortunate that often the first contact we make
with a company is one week before a quality assessor is due.
  Discovering at this late stage that acquiring traceability
to the national standards will take time and cost money, which
has not been budgeted for, is frustrating for all parties.
  With all the recent advances in instrumentation,
establishing traceability should be relatively
straightforward and painless.  
    - By Keith Jones, DSIR Physical
Sciences.      
        A shortcut to traceability?    
      T  raceability requires that an unbroken
chain of documented measurements exists linking the
measurement in question to a recognised standard held in a
national or international standards laboratory.   Traceable
measurements are almost always referenced back to such a
standard operating at a single level.   The ability to
accurately measure ratios of physical quantities is therefore
fundamental to providing traceable measurements.  
    Consider a digital multimeter (DMM) requiring
traceability.   This instrument may have dc voltage ranges
from 100mV to 1000V.   But the national standards
laboratory will maintain the volt at a single level of
either 1V or 10V.   Similarly, the 100 ohm to 1G ohm
resistance ranges of such a DMM will need to be referenced
back to the national resistance standard maintained at
perhaps 1 ohm or 10k ohm.   Techniques have to be developed
for scaling up and down from the national standard in
order to provide traceability for the DMM.  
    The problem for calibration laboratories including
the national standards laboratories is that the performance of
the latest generation of DMM 's  inconsistent use of
apostrophe   and also multifunction calibrators have
improved to the extent that new techniques are having to be
developed.  
    The highly linear response of the latest A/D and D/A
converters are perhaps the key factor to these improvements.
  To put the linearity obtained by these components into
perspective it is equivalent to an "ideal crow" only deviating
from a straight line path by 1mm in its flight from
Wellington to Auckland!   This remarkable linearity
allows such A/D convertors to themselves perform very accurate
ratio measurements.  
    An example of the techniques developed to accurately
measure voltage ratios has recently been commissioned at New
Zealand 's national standards laboratory in Lower Hutt.
  This automated apparatus measures 10V to 1V ratios to an
uncertainty of better than two parts in 10  7  .  
    The technique used is conceptually simple.   Ten
series-connected 1k ohm resistors are wired to the stable 10V
output of an electronic voltage reference.   The nearly
equal 1V potential drops across the 1k ohm resistors are
compared with a 1V standard voltage.   The ratio of the 10V
to the 1V standard voltages is then just the ratio of the sum
of the ten 1V voltages to the single 1V voltage.  
    Leakage currents can invalidate the technique but
these can be reduced to a negligible level with careful
design.   Another requirement is that the instrument used to
measure the eleven individual 1V voltages is linear and
accurate over the small voltage range around 1V that
measurements are made.   Modern DMMs are very good at
this and their linearity over small voltage ranges can be
easily verified to high accuracy.  
    Last year an electronic voltage reference, having
both 1V and 10V outputs that had been measured using this
ratio apparatus, was transported to the Australian
national standards laboratory.   Their technique for
measuring 10V:1V ratios is quite different and the agreement
obtained was pleasing verification of our technique.  
    Resistance ratio measurement poses similar problems.
  The recently completed build-up resistor allows
measurement of the ratio of resistors at the 1, 10, 100, 1k
and 10k ohm levels to an accuracy of one part in
10  8  .   It relies on a simple statistical fact that
the ratio of the resistance of N nominally equal resistors
connected in series to the resistance in parallel is close
to N  2  .   If the N resistors have a fractional
scatter in value about their mean equal to X then the ratio of
the two configurations is accurate to a fractional error of
X  2  .   Therefore, resistors matched to 100 ppm
provide a ratio accurate to 0.01 ppm.  
    The problem is to connect the resistors in parallel
without introducing any additional contact or lead
resistance.   B V Hamon solved the problem in the 1950s by
using permanent four-terminal series connection of the N
resistors and employing additional potential balancing
resistors to reduce the effect of the inevitable
interconnection resistances.  
    The DSIR build-up resistor which follows a design
developed at the Australian national standards laboratory
comprises 1000 resistors of 100 ohm nominal value that can be
interconnected in various series and parallel combinations
to give the resistors a nominal value in decade steps from 1
to 10k ohms.   Mercury pool contacts which achieve
contact resistances of less than 100 micro-ohms on a
repeatable basis are used to create the different
configurations.   The four-terminal junctions that
interconnect the main 100 ohm resistors were manufactured
using a numerically controlled mill and the copper-lead
wires hard-soldered into them using precision-machined fillets
of solder.   It is interesting to note that these junctions
achieved cross-resistance of less than 10 nano-ohms without
adjustment, whereas similar junctions made in the 1960s using
manual milling techniques required hand trimming to achieve
this accuracy: technological advance in one area flows
into others.  
    The strength of such conceptually simple and
physically accessible ratio techniques from a traceability
point of view is that the possible sources of error can be
independently measured.   An accuracy limit can then be
assigned so that these ratio standards can provide a
traceability path without the need for independent
verification.   However, the possible error sources must be
periodically re-measured to guard against such things as
increasing leakage resistance.   Even ratio standards are
seldom trusted initially without independent verification
because there is always the risk of overlooking some important
source of error.   National standards laboratories
intercompare their ratio standards for this reason.  
    This somewhat
    priveleged    privileged     position that
ratio standards occupy, in terms of traceability, may
apply to the above passive techniques which are amendable to
error calculation but not to the active techniques used in
electronic instruments.   This is not to criticise the
careful design that has gone into developing these electronic
devices, but rather reflects the impracticality of getting
inside the integrated circuits to subsequently check the
relevant error contributions.   With such electronic ratio
techniques it is easier to take the black box approach and
measure the overall response on a more frequent basis.   In
other words, conventional external calibration is more
practicable.  
    - By Laurie Christian, DSIR Physical
Sciences.    
        Where to now for standards lab?    
      A   country 's ability to make
measurements greatly affects the level of technology it
can use.   Quality systems require measurement traceability
to the national standards laboratory.   The national
standards laboratory is the organisation with the legal
responsibility to provide for uniform measures throughout
a country by maintaining physical realisations of the legal
definitions of the units of measurement.  
    Note that the national standards laboratory is not
an enforcement agency, such as weights and measures, but one
that provides authoritative physical measurement
standards for the calibration of reference instruments as
well as advising on instrumentation suitable for maintaining
traceability of measurements.  
    In New Zealand the legal units are the International
System (SI) of units as further defined through the
  Convention du Metre 1875  .   The
laboratory responsible for the base measurement units eg,
second, metre, kilogram, ampere, kelvin and candela, was the
Physics and Engineering Laboratory of the DSIR.  
    As a result of the restructuring of the DSIR in 1990,
the standards became a part of DSIR Physical Sciences.
  Next year, 1992, the DSIR will be dissolved and replaced
by a series of Crown Research Institutes (CRI) organised more
like companies than Government departments.  
    With all these changes going on at least two questions
arise: the first is where will the national standards
laboratory be located inside the new organisation, and the
other is what services will it provide?  
    The CRIs will be organised along sector lines and
the national standards laboratory will probably be in the
CRI for the manufacturing and processing sector since the
laboratory is largely industrially oriented, even though it
does provide measurement standards for all other sectors.
  At present the empowering legislation will be
required.  
    As yet no decision on this has been made, but a likely
home is under the Foundation for Research, Science and
Technology (FoRST) Act.  
    Thus, the new CRI would carry out the functions of the
national standards laboratory under contract from FoRST.
  Legislative details should be available later this
year.   Physically, of course, the standards are expected to
stay where they are, namely on the Gracefield site of the DSIR
(later the CRI).  
    Also, no staff changes are expected and so contacts
made previously will continue.   Visitors will note that the
notice board at the gate still directs them to the Physics and
Engineering Laboratory, and the staff wonder if this will
continue under a CRI!  
    Will it be "business as usual" in a CRI?   The
answer to that question is dependent on the funding available,
either from the Crown or from industry.   Over the last few
years, most areas of the DSIR have had decreased Crown
funding, and the test and calibration areas were no
exception.  
    In spite of increasing user pays charges, there is not
enough funding to continue many existing services, let alone
tackle new areas.   Most of the effort of the national
standards laboratory has been directed to ensuring the
international credibility of New Zealand 's measurement
standards, as well as providing higher accuracy
calibration services to an increasingly sophisticated
market.   This restriction of the range of measurement
services occurs at a time when industry is adopting quality
systems which require a wider range of test and calibration
services.  
    Some services have been picked up by commercial firms
where there is a sufficient volume of work for
profitability.   In other cases offshore calibration
services have been used, often at some inconvenience.
  While for a small country, such as New Zealand, a few
offshore services will always be needed for highly specialised
calibrations, it needs to be remembered that the logic of
offshore services means that industry is likely to move
offshore to be closer to its services.  
    Thus, a main reason for providing measurement services
is to encourage growth of local industry.   Improvements
should be possible under the new CRI as industry can form
joint ventures with the CRI to develop or expand the
measurement services it requires.   Crown funding is
likely to be minimal for these areas and so the initiative for
new measurement services will be with the industries
concerned.  
    - By J V Nicholas, DSIR Physical
Sciences.    
        To calibrate or not to calibrate    
      Robin Hodgson, service manager for
Communication Instruments of Wellington, looks at
calibration and the maintenance of test equipment
inventories.    
      U  sers of test equipment must ensure that
their instruments are periodically calibrated to ensure the
measurements they make are accurate and that they relate to a
national standard  .     They must also have a procedure
for ensuring the test instruments they are using are
working correctly.   However, when test equipment
inventories begin to get substantial they become expensive to
maintain, calibrate and manage.  
    Our view is that it is not necessary to calibrate
every piece of test equipment every year.   This is because
some instruments will sit on the shelf and will be
seldom used, while others simply will not require
calibration.   It is also very expensive to calibrate
each piece of test equipment every year.   The cost may
not be justified.  
    There are items of test equipment that require
calibration every year due to the nature of the instrument,
its specifications, or its usage.   Our recommendation is to
identify these test instruments, have the calibrated every
year and use them as in  -  house standards in order to
check the other, lower specified equipment against them
periodically.  
    The benefits of this are the obvious cost savings in
calibrating a smaller number of instruments and less downtime
of equipment while it is away being calibrated.
  Additionally, there is also a benefit to the technicians
using the equipment as they become more aware of the
limitations of their test instruments when they have to
periodically test them against the standards.   This may
also     shows    show     up any faults on the
equipment earlier than during an annual calibration check.  
    Of course, the disadvantage of this system is that
some test equipment must remain as "standard" and unless it is
specified for harsh environments etc it should not be used
as such.   When test equipment is in short supply this may
be a difficult task.   By buying good quality equipment this
problem is reduced significantly.  
    Another disadvantage, is that the technicians must
take time to check their test instruments before using them.
  This can be countered by the time savings made if a fault
was found with the equipment before arriving at a distant
station or at a customer 's premises.  
    A managed test equipment system such as this will
provide the required standards of instrument maintenance and
calibration, will cause people involved with test
equipment to be more aware of the instruments that they
are using, and most importantly in this economic climate,
will be an efficient and economical method of maintaining
a test instrument inventory.    
  

        A PERFORMANCE SPECIFICATION FOR THE SEISMIC
DESIGN OF THE DC HYBRID LINK, NEW ZEALAND    
  JNO COAD    *     and RD
SHARPE    **    
      *  Trans Power New Zealand Ltd.,
Wellington  
      **  Beca Carter Hollings &amp; Ferner Ltd.,
Wellington  
        SUMMARY      
    The two electrical supply and distribution systems of
the principal islands of New Zealand have been connected by a
600 MW high voltage direct current (HVDC) link since 1965.
  A project to double the capacity of this two-way link,
with its associated submarine cable and AC/DC converter
equipment, is being undertaken.  
    Strategic planning for the rest of New Zealand 's
supply requires the link to be maintained in the event of
large earthquakes near the link 's terminals, one of which is
close to the Wellington Fault in the seismically high hazard
area of Wellington.  
    The use of a performance specification for the seismic
aspects of such an international design-build tender is
unusual.   The reasoning behind the use of this form of
specification is described, along with a description of the
form it took and the problems associated with its
implementation.    
        INTRODUCTION      
    The existing DC link terminates just north of
Wellington at Haywards substation (Figure 1).   The site is
approximately 0.8 km to the west of the Wellington Fault on a
man-made platform in hills overlooking the Hutt Valley.  
    The owner of the distribution system, Trans Power New
Zealand, requires that the operation of the HVDC link should
continue in the event of a major earthquake in the Wellington
region, as such an earthquake, may have little impact on the
demand from more distant densely populated regions unaffected
by this event.  
    Studies commissioned by Trans Power identified the
maximum ground shaking likely at both the Haywards site and
the less seismically active Benmore dam site in the South
Island.  
    Because of the specialist nature of the equipment
required, there are a relatively small number of international
suppliers.   Though there have been a few such installations
in seismic regions elsewhere in the world, it was unlikely
that there would be any suitable standard equipment which
would meet the stringent New Zealand seismic loading
requirements.  
  Map :Figure 1: Location of HVDC Transmission Link  
    In order to ensure that competitive bids would be
forthcoming on a project of this large size, it was essential
that the tender documents did not exclude electrical process
solutions which might not fit conventional seismic design
approaches.  
    The principal concerns were the design of the
thyristor valves and the seismic performance of high voltage
electrical equipment mounted on porcelain insulators.  
    The thyristor valves are electrically complex and are
required to have an extremely high reliability.   This
dictated that the design be proven technology and thus
substantially the same as other existing systems.   No one
seismic solution scheme could be specified as it was known
that some suppliers would prefer to offer solutions in which
the thyristor conversion valves were hung from above, while
others had proven floor-mounted technology.   The building
at each site was to be designed principally to contain the
thyristor valves and consequently the support system dominated
the building design philosophy.   The extent of this is such
that the building must be considered as a part of the
equipment.  
    High voltage electrical equipment must be insulated
from ground and this is commonly achieved by mounting
equipment on porcelain post insulators.   Porcelain is
inherently brittle and its use contrasts with modern seismic
design principles where brittle supports are avoided where
possible.  
    These and other concerns required the specification to
be as unrestrictive as possible to potentially innovative
solutions, while ensuring the adequacy of the seismic
qualification of the complete system.  
        TWO LEVEL EARTHQUAKE APPROACH      
    Although it has been implied in seismic codes such as
New Zealand 's NZS 4203   [1]   for many years, it is
only relatively recently that designers have focused on a
two-level performance criterion to be met by structures.
  [1]  Reference        In the nuclear
industry, the concepts of an Operating Basis Earthquake (OBE)
and a higher level Safe Shutdown Earthquake (SSE) are embodied
in at least US codes.   US design rules for offshore
platforms have a lower elastic response level and a larger
ductility level.   The draft New Zealand loadings code DZ
4203 has the concept of a serviceability level and a higher
loading under which damage might occur but collapse should
not.  
    Trans Power adopted for this project similar criteria
based on performance requirements.   At a lower level
earthquake with a relatively high probability of occurrence in
the life of the complex, it was required that there be no
interruption to the conversion process.   In an innovative
move, Trans Power set the higher level requirement by
stipulating that the conversion process must be able to be
restored within five working days after a major earthquake
with a relatively low probability of occurrence.  
    The lower level (OBE) effectively meant that there
should be no damage at all under this event.   The higher
level performance criterion, labelled the Maximum Design
Earthquake (MDE), thus allowed the contractor to decide
whether to adopt a strength or ductility design approach or
provide spare equipment somewhere safe.  
    In order to prevent the building or components
collapsing catastrophically or in a brittle fashion at load
levels just a fraction higher than the MDE, the specification
called for ductile collapse mechanisms to be included.  
    This absolute requirement was later modified by
agreement to state that: a ductile collapse mechanism need
only be specifically designed for equipment with a safety
factor of less than 1.3 on brittle failure under MDE loadings,
although the concepts of capacity design must be embodied in
even this equipment.  
        LOAD LEVEL SPECIFICATION      
    The two levels of earthquake shaking, the MDE and the
lesser OBE, were specified in the form of response spectra for
each of the terminal sites, each being shown for a range of
damping values.   The larger spectra for the Haywards site
are shown in Figures 2 and 3.  
  Graph: Figure 2: Haywards Maximum Design Earthquake  
  Graph: Figure 3: Haywards Operating Basis Earthquake  
    The OBE can be seen to have a relatively greater long
period (low frequency) content than the MDE.   This reflects
the contribution of more distant earthquakes.   The response
represented by the OBE has a return period of approximately
150 years.   The MDE has a return period of 700 years.
  These levels are compatible with the design of other
overseas critical facilities but also recognise that there is
a higher probability of an earthquake on the Wellington fault
than simply indicated by the return period due to the length
of time since the last event.  
        FURTHER REQUIREMENTS      
    It was a key aim of the specification that the
complete process, and not just individual items of equipment,
was seismically qualified.   To ensure this occurred, and
knowing that very large organisations would be tendering, it
was required that the successful tenderer nominate one person
with a demonstrable understanding of seismic design principles
to take responsibility for approving all seismic aspects of
the design.  
    The successful tenderer was required to produce a
seismic design philosophy during the preliminary design phase.
  The seismic design philosophy essentially is a more
detailed agreement proposed by the Contractor that fulfils the
performance requirements of the specification.   In this
way, the experience of the manufacturer is used to define
better the capabilities of his product.   Once in place,
this document simplifies the review of seismic reports as
often only compliance with the philosophy needs to be
checked.  
    While tenderers were free to adopt any reasonable
approach to their design of parts and portions, they were of
course going to have to report on the methods they had adopted
so that these could be verified by Trans Power.   They were
given the choice of following the style of the methods
specified in the New Zealand codes NZS 4203   [1]  
and NZS 4219   [2]  , as long as they adapted certain
of the parameters to suit the performance requirement.
  [1]  reference    
  [2]  reference      
        TENDER EVALUATION      
        THE SUCCESSFUL TENDER      
    The successful tenderer was Asea Brown Boveri (ABB), a
multi-national company based in Sweden and Switzerland.
  ABB employed a New Zealand consulting engineer as designer
of the main buildings.   Base isolation of the principal
buildings and valve hall did not prove to be a viable option
and a conventional structure of reinforced concrete and steel
roof members was the accepted solution.   The relative
closeness of the responses described by the OBE and MDE
spectra meant that, for the buildings, compliance with the OBE
dictated the design.   Inherently, there needed to be only
limited ductile behaviour for the buildings to survive the MDE
of low probability.   The general level of the design
loadings did, however, require the design to consider the
question of uplift on the foundations.  
    Suspended thyristor valves (Figure 4) were offered by
ABB, with the     principle    principal    
difference between existing designs being the inclusion of a
damper at the valve base to limit displacements.  
  Diagram: Figure 4: Suspended Thyristor Valves  
        EQUIPMENT DESIGN      
    The specification allowed the contractor to
demonstrate seismic compliance by analysis, test or a
combination of both.  
    The majority of the equipment was qualified by
response spectrum analysis, though equipment with base
isolation was qualified by time history analysis.   Such
simulations were performed by generating artificial earthquake
records whose response spectra enveloped the specified
spectra.   In some situations, it was necessary to generate
floor spectra corresponding to the ground design spectra so
that the equipment mounted there could be proven to meet the
performance specification.  
    Critical parts of the performance specification were
the requirements that the analysis technique itself be
qualified and that any critical damping values above 2 % be
demonstrated by test.   ABB often chose to test complete
items of equipment where they had assumed high damping and
thus they proved both the adequacy of the damping assumption
and the accuracy of the analysis technique in the same
test.  
    The extent of such testing gave Trans Power
considerable confidence in the ABB analysis technique.   The
only specific request by Trans Power was for the performance
of a snapback test on a thyristor valve.   It was realised
that the low levels of acceleration induced by such a test
would excite only lower modes of vibration and therefore such
a test would not be adequate to qualify fully the valve.
  However, it was considered that a snapback test would be
sufficient to prove the applicability of the analysis
procedure for such a structure.  
        RESULTS OF PERFORMANCE
SPECIFICATION      
    The performance specification format has allowed ABB
to adopt a number of solutions to the seismic design problems
imposed by the onerous Haywards NME and OBE spectra.  
    The high seismic spectral response at the Haywards
site and the inability merely to strengthen off the shelf
equipment has resulted in the use of a wide variety of ductile
devices and strength solutions.  
    The approach of using ductility to avoid catastrophic
collapse is not a very familiar concept to designers in
predominantly non-seismic regions of the world.   It is
their natural tendency to want to detail for maximum strength.
  With the high level of the loads implied by the specified
response spectra, this has the potential for designers to
chase their tails as the increased material leads to higher
masses and so to higher inertial loads to be resisted.   As
has been shown many times before in New Zealand
  [3]  , electrical equipment with its international
applicability, but often inherent brittleness, offers
opportunities for simple modification to give predictably
reliable seismic
performance.  [3]  reference      
    ABB, despite their location in a predominantly
non-seismic region, have made full use of the concept of
ductile design.   In many cases they have chosen this
solution in preference to viable and available strength
solutions.   This may be due to their previous experience in
seismically active regions of the USA.  
    Because of the ABB company structure, which is based
on independent business units manufacturing different
electrical components, many different solutions to the same
generic problem have been offered and installed.   For
example, ABB Capacitors used a rocking plate design to
introduce base flexibility and damping (Figure 5) while ABB
Switchgear offered a flexible plate to achieve the same effect
(Figure 6).  
  Diagram: Figure 5: Rocking Plate Solution  
  Diagram: Figure 6: Flexible Plate Solution  
    The importance of maintaining a single co-ordinating
and approving point must be emphasised.   When such a
variety of solutions are on offer, it is imperative that each
is subjected to the same degree of analysis to ensure
consistency and compatibility at the interfaces and across the
site.   When many individually qualified items are added
together to form a system, it is the co-ordination which
ensures the complete process is also seismically adequate.  
    To some extent, this is controlled by the early
implementation of a seismic design philosophy and the
identification of co-ordinating documents.  
    Equipment, or equipment interactions, at the
interfaces between different areas of responsibility can often
be overlooked by both the contractor and the client,
especially where a performance specification is used.  
    A potentially major drawback with a seismic
performance specification is the requirement for good
communications between the client 's seismic designer and those
of the contractor.   Such contact can often be affected by
contractual difficulties.   During this project, ABB 's
Swedish resident specialist seismic engineer was readily
available to discuss ABB 's approach to any given problem and
this degree of co-operation is reflected in the high standard
of seismic engineering in the HVDC link project.  
    It was interesting to note that it can be both
difficult and onerous for the client to verify analyses
submitted by the contractor for approval of seismic design of
complex items.   Consideration would be given in future
performance specifications to strengthening the client 's right
to ask for further verification analysis and to placing more
weight on the need to demonstrate ductile collapse
mechanisms.  
          CONCLUSIONS      
    The use of a performance specification for the seismic
design aspects of an important electrical process has been
thoroughly tested by this large project.   It has resulted
in many innovative aspects of seismic design being offered to
tailor the international supplier 's standard product to the
needs of an installation in locations with high seismic
hazard.   The advantages of insisting on the successful
tenderer producing a seismic design philosophy were seen as
the project progressed.   Consideration would be given in
the preparation of any future performance specifications to
more weight being placed on a requirement for the provision of
ductile mechanisms to be demonstrated where appropriate.
  Provisions for requesting further verification analysis
would also be tightened.   The need for the contractor to
nominate a seismic engineer (acceptable to the client) to
oversee and co-ordinate all aspects of the product being
offered was considered an important part of this
specification.      
  

        NMR AS A CHEMICAL TOOL    
    Jan Coddington, University of Auckland  
    Nuclear magnetic resonance started life as a
  physicists'   orphan, somewhat maligned and
unappreciated.   By the time of the first review article in
this journal    1    , it had become the
favourite child of many spectroscopists and has continued
to develop as an essential chemical tool.
  1.  reference       Its current
growth is being driven by the demands of molecular
biologists, biophysicists and geologists as well as by
chemists.  
    This article discusses some of the current
applications of NMR.   The examples used are research
projects involving the Bruker 40OMHz instrument at the
University of Auckland (Figure 1).   Regular users of
the instrument include the Chemistry, Biochemistry, Zoology,
Botany departments and the Cancer Research Laboratory of the
University of Auckland, and the Chemistry department of
Waikato University.   Occasional users have included
researchers from the Pathology, Physiology, Cellular and
Molecular Biology departments, DSIR and commercial consulting
chemists.    
      Organic Chemistry    
    NMR is now essential to the organic chemist for any
type of structure elucidation.   Usually simple   1  H
and   13  C spectra are enough to give an indication of
the carbon skeleton and functional groups present.   Methods
for obtaining such spectra under a variety of conditions are
now well developed.   (Excellent guidebooks
abound    2    ).
  2.  reference       Combined with an
editing experiment such as DEPT (Distortionless
Enhancement by Polarisation Transfer), to distinguish CH,
CH  2   and CH  3   groups such conventional
spectra are often sufficient to resolve structural
uncertainties.   For example, Professor Con Cambie 's group
in the Chemistry department isolates a large number of very
similar diterpenes and derivatives, such as kaurene
  1  , from native plants.   The presence of a 
  diagram 1  
keto or hydroxyl group in ring A is common and is easily
determined from the   13  C chemical shifts: a C=O
occurs about 210ppm and a CH-OH at about 75ppm.   The
position and stereochemistry of the substituent is assessed by
the magnitude of the   1  H coupling constants: an
equatorial OH will allow its geminal H to show a large diaxial
coupling (  ca  . 12Hz) to any vicinal axial H, while an
axial OH will permit a 1,4-W coupling (  ca  . 1.5Hz) to
any suitable equatorial H.   The rest of the molecule can be
examined similarly since most signals are quite well resolved.
  A range of these compounds has now been investigated
and the combined data set is of predictive value.  
  diagram 2  
    Sometimes, spectra have so many overlapping
resonances and the coupling pattern is so complex that
they appear hopelessly jumbled.   New techniques   (new
pulse sequences)   have helped enormously by spreading the
information into two dimensions.   It is possible, for
example, to have just the chemical shift information in one
dimension 
  photo    caption (Figure 1)  
and the coupling network in the other.   These methods are
now the backbone of modern NMR analysis and application
monographs are appearing    3    .
  3.  reference      
    The most useful of the new experiments is undoubtedly
COSY   (Correlated Spectroscopy)   which gives a map of
the coupling partners of every hydrogen in a molecule.
  This was used extensively by Dr. Alistair Wilkins at
Waikato to determine the structure of   2  , a
constituent of commercial chlordane.   The number of
chlorine atoms present and their relative stereochemistry had
to be established.   The coupling interactions between
the hydrogen nuclei were used to do this.   Part of the COSY
spectrum is shown in Figure 2.
  figure 2  
  A correlation between two hydrogens is indicated by a
  cross peak   at a grid position corresponding to the
chemical shift (one in each dimension) of the nuclei involved.
  In this way, the whole skeleton could be traced.  
      Biomolecules    
    The appearance of 2-D techniques has given renewed
impetus to the study of biological molecules where
materials may be unstable and of limited availability.
  At the Division of Horticulture and Plant Processing,
DSIR, Dr. R. Mitchell has isolated and characterised a
phytotoxin produced in liquid cultures of a pathogenic
plant bacterium.
  diagram 3   
  The structure of tagetitoxin   3  , was
established using a combination of mass spectra and NMR.
  Several 2-D heteronuclear NMR correlation methods were
used to unequivocally connect carbonyl   13  C H
nuclei to several bonds away.   A critical piece of
information was also supplied by the observation of a
nuclear Overhauser effect   (NOE)   between the H at C7
and one of the H nuclei at C2.   This effect is the result
of dipolar couplings and is observed
    ony    only     when H nuclei are within 0.4nm
of each other.  
    Most biomolecules are immense by solution NMR
standards and further complicate observation by being
soluble only in water.   Because their concentrations are
usually not more than 1-2mM, such molecules can only be
routinely observed using high-sensitivity   1  H
spectra.   Special methods are then needed to remove the
massive H  2  O peak that otherwise dominates the
spectrum.   Even in D  2  O solution, the residual HOD
peak is huge and must be suppressed.   The overall result is
that such spectra are collected over long periods to get
useful results.   Nevertheless, many proteins, carbohydrates
and nucleic acids have been studied.  
    Some typical spectra of the relatively small protein,
bovine parathyroid hormone, are shown in Figure 3.   Many of
the lines are broad and there is considerable overlap.
  figure 3    
  However, it is still possible to obtain information about
the tertiary structure of the protein, for instance as the pH
is changed.   In particular, the aromatic region from
6-10ppm is quite well resolved.   It can be used to
determine the pK  a   of individual histidine residues
and indicate accessibility to solvent.   Any specific
conformational changes in the other aromatic amino acids are
also easily observed.   In water, the region also contains
all the NH resonances whose behaviour is characteristic of
specific amino acids and conformations of the global
protein structure.   Dr. Peter Barling in the Biochemistry
department is using this information in the study of
structure-activity relationships of the various domains of the
protein.  
    In an equally technically demanding project, a group
led by Professor Alistair Renwick is investigating the
carbohydrate sequences of several glycoprotein hormones,
using primarily 2-D NMR methods.  
    All NMR studies of biomacromolecules depend on the
exquisite sensitivity of the new spectrometers with high-field
superconducting magnets.   This sensitivity is also used to
advantage in labelling and feeding studies.   Metabolic
pathways were originally determined by feeding radioactive
  14  C-labelled precursors.   Isolation of products
was often very tedious and difficult.   It is now routine to
feed   13  C-labelled material which is NMR visible but
not radioactive.   Incorporation of the label can often be
determined without isolation of individual products.   A
whole bacterial culture, for example, may be put in an NMR
tube and, under ideal conditions, signals will only be
detected from those compounds containing the label.
  (The problem of assigning the signals to specific
compounds may not be trivial).  
    Part of the biosynthetic pathway of rhizobitoxine
  4  , another toxin from a plant pathogen, has
been determined in this way by Dr. Mitchell at DSIR.   The
pathogenic bacteria were fed aspartic acid   5  ,
which was specifically labelled with   13  C at the C4
carboxyl position.   Enhanced   13  C NMR signals,
indicating incorporation of label, were detected at the C1 and
C4 positions of rhizobitoxine.   Label was also detected at
the C4 position of hydroxythreonine   6  ,
confirming a previous suggestion that it was an
intermediate in the pathway.   In this case, the compounds
were extracted and partially purified before the NMR
experiments were done.  
  diagrams 4,5,6  
        In Vivo   Studies    
    One of the most fascinating and challenging areas in
NMR is the study of biologically viable samples.   These can
include cell cultures, perfused organs and even whole plants
and animals.   Dr. Alison Stewart of the Botany
department and Dr. Alwyn Rees of the Marine Research
Laboratory have been interested in the symbiotic
relationship between the algal and fungal components of
various New Zealand lichens.   The major metabolite is one
of several polyols, such as arabinitol, ribitol or mannitol,
and is species specific.   As such, the identification of
the compound may serve as an aid to taxonomic classification.
  Intact lichens and their methanol extracts give good
  13  C NMR spectra where the dominant peaks arise from
the polyol present.   Experiments to follow the path of the
algal photosynthetic product to the major fungal metabolite
are now in progress: the lichens are maintained in an
atmosphere of   13  CO  2   for 12 hours and then
spectra are obtained after various times.  
    Some sea anemone species are also involved in a
symbiotic relationship with algae.   There is great interest
in whether the alga supplies glycerol or lipids (fatty acid
triesters of glycerol) to its host.   Dr. Rees has
identified an anemone that can be examined by NMR: it is small
enough to fit comfortably inside a 10mm NMR tube.   Figure 4
shows the   13  C spectrum of 50 of these live animals,
surrounded by sea water.   The major peaks can be assigned
to lipid with resonances for both carboxyl and unsaturated
bonds as well as the expected methylene envelope being
observed.   Also present are peaks corresponding to glycerol
and several amino acids.   The resolution of the spectrum is
impressive since many   in vivo   samples give
generalised blobs for spectra.   The anemones did not appear
to be under stress.   A   31  P study showed normal
energy metabolism with the expected levels of ATP and the
animals resumed feeding when returned to a more spacious
environment.  
  figure 4  
    Monitoring the health of a living system using
  31  P NMR is becoming routine.   Figure 5 shows the
spectrum of an intact rat heart.   In a healthy muscle such
as this, high levels of phosphocreatine (-5ppm) and ATP (-8m
-12, -22ppm) and low levels of inorganic phosphate (0ppm, but
pH dependent) and ADP (-7, -12ppm) should be observed.
  Dr. Stuart Humphrey in the Pathology department is
interested in the use of various buffer systems to extend the
functional lifetime of hearts for use in transplants.   An
organ preparation is placed in the spectrometer, operating
at 278 K, and spectra are obtained every 30 minutes until
there is no phosphocreatine or ATP left.   At that point,
the pH of the organ has usually changed substantially,
indicating pathological dysfunction.   This is easily
followed by the chemical shift changes of the inorganic
phosphate peak.  
  figure 5  
      Inorganic Chemistry    
    These bio-organic applications have been driving much
of the current wave of software development in NMR.   Better
hardware, such as frequency synthesisers and more powerful
transmitters, has resulted from demands for easy observation
of all the magnetically active nuclei.   NMR is becoming an
increasingly important technique in modern inorganic
chemistry    4    .   4.
  reference      
    Most organometallic and coordination compounds bear
ligands containing H and C atoms, which often give simple
spectra.   The presence of a metal atom at the centre of the
coordination sphere can influence the   1  H and
  13  C spectra in several ways however.   Some metal
centres, particularly transition metals with partially filled
d shells, are paramagnetic and they have a dramatic effect
on the spectra.   The unpaired electron spins interact with
the nuclear spins so that peaks are often broadened, sometimes
to the point of disappearance, and the chemical shifts are
significantly changed.  
    A metal centre can also behave as a heteroatom,
affecting the chemical shift of nearby H and C atoms.
  For example, a hydride ligand (hydrogen bonded directly to
a metal) usually has a resonance much further upfield than the
0-10ppm range typical of organic molecules.   In the
diamagnetic ruthenium complex
RuHCl(CO)(PPh  3  )  3   (prepared by Stage
III undergraduate chemists at Auckland), the hydride ligand is
observed at -7.2ppm in the   1  H spectrum.   The
presence of phosphorus atoms in the complex results in
coupling to the hydride and the resonance appears as a
doublet of triplets.   This is good evidence for two of the
PPh  3   ligands being equivalent, indicative of a
specific geometry around the metal.   Professor Warren
Roper 's group has synthesised many new ruthenium, osmium and
iridium compounds containing both phosphine (PR  3  )
and fluorocarbon ligands.   There is extensive coupling
between   31  P,   19  F and   1  H nuclei,
leading to very complex spectra.  
    Figure 6 shows the   31  P spectrum of a
porphyrin complex prepared by Dr. Penny   Brothers'  
group.   Here, the central atom is phosphorus and it is
coupled to eight chemically equivalent hydrogens on the
porphyrin macrocycle and a further six equivalent hydrogens on
the two methoxy ligands.   The resonance appears as a
nonet of septets.  
  figure 6  
      Chemical Exchange    
    The advantage of a multinuclear spectrometer is the
ability to     chose    choose     which element
in a molecule will give the most informative spectrum.
  Often this will be the atom at the centre of the complex.
  The spectra of these nuclei will be relatively simple
since there will be only a few different chemical shifts
present and the coupling patterns should be straightforward to
interpret.  
  diagrams 7,8,9  
    Dr. Michael Taylor has been interested in the
formation of complexes between borate and polyhydroxy
compounds.   It has been established that the
B(OH)  4   is complexed by diols to give both
mono-chelated and bis-chelated anions.   1,2-Diols form five
membered ring complexes and 1,3-diols form six-membered rings.
  Glycerol is a 1,2,3-triol, which may behave as a 1,2- or
1,3-diol in its interaction with borate.   The   11  B
NMR spectrum of a 1:3 mixture of NaB(OH)  4   and
glycerol showed separate peaks for the monocyclic five- and
six-membered rings as well as the bis-spirocyclic forms
  7   and   8  .   The mixed complex 9
is also present.   The   11  B spectrum contains at
least eight peaks, since the borate anion is participating in
polyborate formation equilibria as well.   The   13  C
and   1  H spectra can be interpreted in terms of the
major species observed but they are quite complex.   This
system is obviously most easily
    studies    studied     by starting with the
less familiar NMR nucleus.  
      The most readily accessible nuclei are those which
have a nuclear spin quantum number, 1, equal to 1/2.   These
include   1  H,   13  C,   19  F,
  31  P and   119  Sn.   The last of these has
been used in another of Dr. Taylor 's projects to study the
behaviour of tin (II) halides in solution.   The
  119  Sn spectra of aqueous mixed halide solutions
consist of single lines, the positions of which depend on the
composition of the mixture and lie between those of the
separate chloride, bromide or iodide systems.   The signals
are averages of the chemical shifts of the various species
present: (SnX  n  Y  (3-n)  )(X,Y=Cl, Br, I,
n=0-3).  
    These are labile in water at room temperature on the
NMR timescale.   If these solutions are extracted with
diethyl ether and the extracts cooled to 213 K, then the
  119  Sn spectra show each of the individual species
present (see Figure 7).   The exchange rate has been slowed
down by lowering the temperature in a less
      coordination    coordinating      
solvent.   Thus the chemical shifts of all ten
trihalogenostannate (II) anions can be determined.  
  figure 7  
      Physical Interactions    
    If a suitable probe nucleus can be found, NMR is also
useful for looking at physically interacting systems as well
as more orthodox chemical exchanges.     129  Xe is
another nucleus with I= 1/2.   Xenon gas is composed of
large polarisable atoms whose chemical shift is very sensitive
to the environment.   Dr. Russell Howe has been using
  129  Xe NMR to study the structure and composition of
several zeolite catalysts which are important in hydrocarbon
refining and synthetic petrol production.   Most of the
NMR protocols described here rely on liquid samples or at
least on samples that approach homogeneity, contained in
unsealed glass tubes (see figure 8).   This is not the
case for the zeolite experiments.   The sample consists of
solid zeolite material in the bottom of a tube which is then
filled with a known amount of xenon gas and the whole sealed
off from the atmosphere.   The experiment works because the
xenon atoms behave like a liquid inside the zeolite,
interacting with each other and with the solid.   The
properties of the catalysts depend on the size, shape and
coatings of their internal pores and these are exactly the
variables to which the xenon nuclei are responsive.   The
results are spectra which give an indication of the size of
the catalyst pores, their water content and the effects of a
variety of heat treatments mimicing the operating
    conditons    conditions     under which the
zeolites are used.  
    The interaction between sweeteners and taste receptors
is also amenable to interrogation by NMR.   A surprisingly
wide range of compounds elicit the same sweet taste on the
human tongue.   These include saccharin, aspartame
  (Nutrasweet)  , potassium cyclamate, lead acetate
and chloroform as well as sucrose and most of the
monosaccharides.   No structural similarities between these
are obvious.   The taste response may be triggered by a
specific molecular conformation when interacting with the
receptor.   NMR can be used to determine the solution
conformation of the molecules and monitor any changes when
they meet the receptor.   A major restriction is the
unavailability of isolated functioning human taste buds.
  These are considered to be essentially lipid membranes and
are modelled by unilamellar phospholipid vesicles.
  Changes in the   1  H and   13  C chemical
shifts and parameters monitoring molecular constraints such as
relaxation times, are being used to examine the effect of the
liposomes on a selection of sweet compounds.  
      NMR Trends    
    These examples, while covering a wide range of
research interests, simply indicate what is possible on one
type of modern NMR spectrometer.   The subject is much
wider.   Some other important areas under intense
development are&semi; experiments giving data in three dimensions
with an inherent increase in available information,
spectroscopy of solid materials using a range of nuclei, and
magnetic resonance imaging (MRI) which can display NMR-edited
maps of living systems as an alternative to CAT scanning.
  At the moment, these appear to be separate and disparate.
  Library classifications of books in these areas certainly
finds them widely dispersed.   This reflects the enormous
range of applications rather than underlying fundamental
techniques.   It may be that NMR is really evolving as a
discrete discipline.  
  photograph      caption (figure 8)      
  

          5 Greenhouse friendly alternatives      
        5.1 Introduction      
    In this section technical alternatives to additional
coal-fired generation at Bream Bay that will produce less carbon
dioxide are examined.   It should be made clear that the choice
of an alternative is not simply a choice between the large
centralised electricity generating alternatives - hydro, gas, oil,
coal and nuclear.   These major electricity supply options
provide generating capacity in large units generally of 250 MW or
more.   Many of the other options described below provide supply
or substitute for electricity consumption in very small units and
thus offer   the opportunity to cope with demand growth in an
incremental way  .  
    There are five main classes of alternative - energy
management/conservation, increased transmission capacity, direct
use of fuels, increased efficiency in burning fuels and
substitution with "renewable" energy.   Each of these is
discussed in turn.  
    It must be noted that the Bream Bay proposal is for power
station development in two stages each of 500 MW, each stage
consisting of two 250 MW alternators.   The alternatives are,
therefore, discussed, where possible, in terms of blocks of energy
corresponding to baseload generation at 250 MW.   In other words,
alternatives that involve blocks of energy of about 1850 GWh
(corresponding to 250 MW at 100% load and 85% availability) are
sought.  
        5.2 Energy management and
conservation      
    The history of electricity in this country has so far
revolved around the development of centralised generation.   The
pattern of development reflects the natural resource opportunities
- hydro, geothermal, coal, natural gas, and imported oil.   What,
so far, decision makers have failed to recognise is the potential
for "generating" electricity by improvements in end-use technology
- sometimes referred to as the "fifth fuel".   The scale of this
potential is commensurate with the total level of electricity
supply.  
    Examples of the "fifth fuel" are computer control of
heating and ventilation in commercial buildings, and more efficient
lights that produce the same light intensity from less power.
  There are a multitude of such systems that operate at the
microscopic end of the energy chain.    
    There is enormous potential for economic electricity
"generation" from energy management.   However, the "resource" is
complex and diffuse and a detailed analysis is beyond the scope of
this report.   The Ministry for the Environment 's preliminary and
very approximate estimate of the potential for reductions from
energy management are about 77 PJ per year in energy and about 3.8
million tonnes per year of carbon dioxide.  
        5.3 Increased transmission capacity      
    The HVDC link is to be upgraded by 1992 - an action that
will help prevent the absurdity of burning coal in the North Island
while water is spilled in the South Island.   This cable is
believed to be of 1200 MW capacity, which will allow for another
600 MW to be sent north assuming that the power is available.
  The installed capacity of the Clyde station is 432 MW which
will about match the new cable 's extra capacity.  
    In the long run, however, it is not simply the installed
capacity of the generating stations and the cable that matter but
rather the relation between the river flows, the storage capacities
of the hydro lakes and the load patterns that determine how much
electricity can be sent from south to north (and vice versa).
  This is a matter for detailed modelling of the whole generation
and transmission system which is outside the scope of this study
(and is, presumably, being carried out on a continuing basis by
Electricorp).   The maximum possible assuming that Clyde comes on
stream is about an extra 500 MW peak load and perhaps 250 MW
average.   This corresponds to an annual reduction of about two
million tonnes of carbon dioxide from future North Island
generation.  
        5.4 Direct use of fuels      
    Burning fossil fuels to generate electricity incurs a high
energy penalty&semi; two thirds goes up the stack.   Gas can be burned
directly for heating at an efficiency of 60-90%, whereas if the gas
is first converted to electricity, the overall efficiency of
heating is only about 32% (including transmission losses).  
    If we assume that half of the electricity generated in
thermal stations is used for heating and that this is replaced by
direct use of the fuel at an efficiency of 66%, then a 25%
reduction in fuel consumption (and in carbon dioxide production)
would be achieved.  
        5.5 Increased efficiency in burning
fuels      
    There are a number of ways in which fuels can be burned
more efficiently to generate electricity than in the proposed Bream
Bay plant.  
    Firstly, the design of the Bream Bay plant could be
modified to a combined cycle system.   Until recently, combined
cycle generation was confined almost exclusively to gas- and
oil-fired systems because of the difficulties in firing gas
turbines from coal.   However, there has been a move to
integrated coal gasification combined cycle (IGCC) systems in which
the coal is first gasified.   Demonstration plants using this
technology are currently under construction.   Construction of
the Bream Bay plant as an IGCC system would reduce the carbon
dioxide emissions from about 8.4 million tonnes per year to about
7.0 million tonnes per year.  
    Secondly, existing power plants could be converted to
combined cycle operation.   Conversion of both New Plymouth and
Huntly to gas turbine boosted combined cycle offers the possibility
of an extra 400 MW or more of total capacity for a carbon dioxide
emission of about 650,000 tonnes per year.   In other words,
these two stations could generate 40% of the output of the Bream
Bay plant but it would be accompanied by 80% less carbon dioxide
per GWh.  
    A third possibility is the use of natural gas to upgrade
the use of geothermal fluid for power production.   This
possibility arises because New Zealand 's geothermal generating
systems are based on a supply of superheated water from which steam
is flashed to feed the turbines.   The possibilities here range
from gas-fired superheating of the flashed steam to the use of the
geothermal fluid as the preheating source for the boiler water to
a very large gas-fired system.   The marginal efficiency of the
use of the gas varies depending on the manner in which it is used
and the scale on which one wishes to generate from a particular
geothermal source, but it can always be arranged to be
significantly higher than that of a simple Rankine cycle or of a
combined cycle.   The carbon dioxide emissions would be
correspondingly lower.   A range of possibilities has been
discussed in some detail by Dobbie (1979 and 1985) and it seems
unfortunate that these possibilities have apparently been
overlooked in recent geothermal developments.  
    A fourth way of raising the efficiency of fuel use is
cogeneration.   Cogeneration is the generation of electric power
in conjunction with the production of low temperature process heat
from the combustion of high temperature fuels or by using waste
heat from industrial processes.   This has been discussed in
detail in the past (Wallace &amp; Williamson, 1984, Dobbie, 1979 and
1985).   The most recent survey by Zoellner (1990) lists 43
existing installations with cogenerating capability totalling 157
MW.  
    Because cogeneration uses the excess temperature of the
fuel combustion above that required for the process, it generates
electricity at a very high marginal efficiency (85-100%) and
therefore produces 50-60% less carbon dioxide per kWh even than new
combined cycle installations.  
    The potential for further cogeneration plant depends on the
price of electricity that the cogenerator can avoid or can realise
by selling back to the grid (these are not necessarily the same).
  In many cases the economics have been muddied because of the
difference between the price charged for bulk supply from the grid
to the supply authorities and the cost of cogenerated electricity
and the long run marginal cost of new generation.   Depending on
how these are related, one can arrive at economic cogeneration
capacity of from 150 to 500 MW and from 600 to 1200 GWh per year.
  This corresponds to from one sixth to one half of the installed
capacity and from one twelth to one sixth of the energy of the
Bream Bay plant.  
    Because of the structure of the electricity generating and
distribution industries in New Zealand it has not in the past been
easy for the two parties most concerned with cogeneration, namely
the generating group (Electricorp) and the cogenerating group
(industry), to come to suitable commercial arrangements of benefit
to both.   In fact, potential cogenerators in New Zealand have
been actively discouraged by pricing structures and other means.
  It is noticeable that in countries where there is only one
commercial boundary between the main electricity generating company
and the consumer (potential cogenerator), much more progress has
been made in the implementation of cogeneration.  
        5.6 Substitution by renewables      
    There are many "renewable" technologies for electricity
generation or substitution.   For example, Electricorp has been
carrying out investigations for wind turbine sites and also
considering wave generation for the Chatham Islands.  
    We have selected one "renewable" technology with which we
are very familiar - domestic solar water heating - for assessment.
  In what follows, a comparison of this "soft" dispersed energy
source and a thermal power station is made.  
    The technology for solar water heating is well developed
and a range of types of equipment suitable for various applications
is available.   It is clearly not convenient to try to provide
all of a household 's hot water energy from the sun since this would
require an inordinately large installation.   Experience has
shown that for New Zealand conditions it is appropriate to design
for the provision of about two thirds of the annual hot water
energy to be supplied from the solar system.   This means that a
typical solar installation should aim to provide around 2500 kWh
per year.  
    We estimate that at least 50% of New Zealand 's one million
houses would be suitable candidates for solar water heating.
  The potential for substitution is thus about 1250 GWh per
year.  
    This translates to one third of the output of the first
stage of the Bream Bay plant, about one third of the present output
of Huntly or 12 times the present output of Meremere.   In terms
of carbon emissions, this would eliminate the need for a thermal
station that would burn about 500,000 tonnes of coal per year and
emit 1.4 million tonnes of carbon dioxide per year.  
    One analysis showed that solar water beating using a system
available at that time could produce hot water at an equivalent
cost of around six cents (1985) per kWh (Wright &amp; Baines, 1986).
  This analysis has been updated to give nine cents (1990) per
kWh.   The cost would be lower with large scale production of
solar systems.  
    The cost of electricity "supplied" from a domestic solar
water heater should be compared with the marginal cost of power
  delivered   from centralised power plants.
  Estimates of marginal cost are very dependent on the
characteristics of the particular power plant under consideration.
  We estimate the   delivered   cost of electricity
from new thermal     plant    plants     such as that
proposed by CRA to be from nine to twelve cents (1990) per kWh if
the fuel can be obtained cheaply&semi; that is, the coal would need to
be purchased at about $80 per tonne.  
    However, the domestic solar hot water option barely
competes with the average cost of electricity as it is presently
sold to consumers.   In particular, the option cannot compete
with off peak domestic rates.   This problem again can be traced
to the fact that electricity is generated and transmitted by one
agency (Electricorp) that sells it to a second agency (the
Electricity Supply Authority) at some kind of average price.
  The second agency distributes the power locally and sells it to
the consumer.   These two commercial boundaries act like
"crossed" polarisers so that the generating agency that could
benefit from solar water heating and, the consumer, who under the
current arrangements would pay the capital cost of the
installation, never "see" each other.   As a result there is a
classic non-market situation where the costs and the benefits
accrue to different parties.  
    One of the advantages of solar water heating is that the
substitution releases energy in the area in which the increase in
demand occurs, and there is therefore less need for new
transmission lines from remote generating stations.   Solar water
heating also has the advantage that it can be implemented at a
number of rates since the quantum of installation is small.
  This is also a disadvantage.   At the moment the scale of the
industry is very small, so that overheads are large and costly
labour-intensive methods are used.  
    Another advantage of an industry of this kind is that it is
sufficiently flexible to respond to changing conditions.   For
example, if it turned out that for some unanticipated reason solar
water heating were not a good thing, one would not be faced with
the question of whether or not to abandon a programme into which
tens or even hundreds of millions of dollars had been sunk and out
of which nothing had come.   Abandoning a solar water heating
programme, for whatever reason, would leave one with the advantages
of the equipment already installed up to the point of cessation and
the only real loss would be the few million dollars in
manufacturing plant.  
    Solar water heating is not the only substitution industry
that should be considered&semi; however, it is an outstanding example of
a neglected opportunity.  
    It is worth noting that in the very long run it may prove
possible to incorporate solar electricity generation into the
system.   Present technology is capable of producing electricity
from solar energy either by direct (photovoltaic) methods or by
indirect (solar thermal) methods.   The largest solar thermal
installation at present is in California and has a capacity of 300
MW with a further 300 MW to be installed by 1992.   The cost of
the electricity is claimed to be eight cents (U.S.) per kWh, which
is competitive with nuclear excluding the costs of decommissioning
and fuel reprocessing (Luz Engineering, 1990).   The scale is
certainly appropriate for consideration in terms of plants like
Bream Bay.  
    The cost of solar electricity is still significantly higher
than that of conventional thermal generation.   However, if the
emission of carbon dioxide into the atmosphere is controlled or
taxed then solar thermal generation will become a much more viable
option.    
  

      2 SETTING GOALS    
    The first step in any programme to produce less waste
is a commitment to action, usually through a statement of
policy (Environmental Defense Fund, 1986).   This need not
be in detailed form, although some governments and
organisations have produced detailed policies (as will be
shown in this chapter).   Policies can often be set at an
early stage, before the details of an implementation plan have
been established.   However, it is important to define what
exactly the policy is referring to.  
    There are a number of names used to describe the
actions of producing less wastes.   Some of these are:
pollution prevention, waste reduction, waste minimisation,
source reduction, waste avoidance, and low-waste, non-waste,
clean or cleaner technology.   There is much debate about
the right words to use, and how they are defined, as will be
noted in this and the following chapters of this paper.
  Waste reduction has been chosen as the appropriate term
for this paper and is recommended for use in New Zealand.
  (It 's shorter and easier to spell than waste
minimisation.)  
    No matter what terminology is used, the ultimate goal
in waste reduction is:   To prevent the generation of
waste at its source rather than to control, treat or manage it
afterwards.    
    This goal is also known as the first principle in the
hierarchy of waste management.   It does not include waste
recycling, the treatment of wastes after they are generated,
or any action that merely concentrates the waste to reduce its
volume or dilutes it to reduce the degree of hazard.  
      Setting goals from which to develop waste
reduction strategies is critical.   All subsequent actions
undertaken by organisations or individuals should relate back
to the goals.    
      Everyone should set goals for reducing wastes&semi; it is
only through the combined efforts of each of us that waste
reduction will be achieved.    
    This chapter looks initially at the policies that have
been set by the international and regional organisations of
which New Zealand is a member.   The policies of firstly the
public sector (countries, states) and secondly the private
sector (companies, business and industry organisations) are
then discussed.   Finally, examples of some of the policies
already adopted in New Zealand are presented.   From these
examples, New Zealanders can choose those most appropriate for
their individual needs.  
        2.1 International agencies      
    New Zealand is a member of various international
organisations which are involved in matters relating to
wastes.   The two most important are the OECD (Organisation
for Economic Co-operation and Development) and UNEP (United
Nations Environment Programme).   Both organisations set
policy directions for their member countries which are binding
on members.   New Zealanders should therefore be aware of
these requirements when setting their own policies.  
      2.1.1 Organisation for Economic Co-operation
and Development    
    In 1974 the OECD Environment Committee, which oversees
all work involving environmental matters, set up the Waste
Management Policy Group (WMPG) (OECD, May 1990).   This
Group is primarily responsible for considering and developing
international policy to promote appropriate waste management.
  Since 1988 the Group has stepped up its work on waste
minimisation, including the consideration of a discussion
paper on policy options prepared by the Secretariat (OECD
WMPG, Sep 1988).   These policy options had to be flexible
to allow a variety of actions for promoting waste minimisation
to reflect the market economy approach of OECD member
countries.  
    The OECD WMPG paper set out in priority order a list
of ten goals for encouraging waste reduction (see Box 2.1).
  The first six items were considered as probably necessary,
with the final four items being of lesser importance.  
    Note that this OECD list implies that information
about the rate at which waste is being generated is of primary
importance.   Without this basic information, there is no
precise way that anyone - policy maker, waste producer or
enforcer - can determine the success, failure or trends of a
waste reduction programme, i.e. whether the goals are being
achieved.   Further information on the system proposed by
the OECD for measuring the waste generation rate is given in
Chapter 3 and Appendix II.  
    The first six items in Box 2.1 are mainly aimed at the
governments which comprise the OECD, as they require detailed
legislative and financial interventions (although they can
also be used as a guide to action by local government).
  For example, the Federal Republic of Germany has
stipulated that solvent wastes can only be recycled or
incinerated (Bailey, Aug 1989), thus controlling the disposal
of such wastes through regulations.   However, the OECD
recognises that individual countries need to select and
implement 
  photo  
  caption  
  box 2.1  
policies which are appropriate to their circumstances.  
    The OECD also developed policies during the 1980s for
the monitoring and control of transfrontier movements of
hazardous wastes (OECD, May 1990), which formed the basis of a
series of Council Acts (see section 9.1.1).   Recently, the
Waste Management Policy Group worked on a policy to reduce the
transfrontier movements of wastes.   The
Decision-Recommendation on the Reduction of Transfrontier
Movements of Waste was adopted at the meeting of the OECD
Environment Committee at Ministerial level in January 1991
(OECD, Jan 1991).  
      2.1.2 United Nations Environment
Programme    
    UNEP undertakes a wide range of work in relation to
wastes including setting policy, providing information,
promoting research and development, and encouraging training.
  Its policy is negotiated through meetings which often
involve more than 100 countries.   UNEP 's policy for wastes
is set out in the Cairo Guidelines and Principles for the
Environmentally Sound Management of Hazardous Wastes (UNEP,
June 1987).  
    The Basel Convention on the Control of Transboundary
Movements of Hazardous Wastes and their Disposal (UNEP, Mar
1989) also sets out waste policy, but as this is also an
instrument of control it is discussed in detail in Chapter 9
(see section 9.1.2).  
    The Cairo Guidelines were designed to assist
governments to develop policies for the management of
hazardous wastes (the definition used is given in section
3.1.2), and drew on experience already gained through existing
agreements and legislation.   They therefore deal more with
administrative aspects rather than technical details of
managing hazardous wastes.   However, specific references to
reducing wastes are found in two places (see Box 2.2).  
      These guidelines should be considered by all
those setting goals for managing hazardous wastes, as they
represent the consensus of a large number of
countries.    
      2.2 Regional    
    New Zealand is also a member of a number of regional
organisations, including ANZEC (Australian and New Zealand
Environment Council) and SPREP (South Pacific Regional
Environment Programme).   Although not a member, New Zealand
is also affected by the policies of the European Community
(EC).  
      2.2.1 Australian and New Zealand Environment
Council    
    The Australian and New Zealand Environment Council
(ANZEC) comprises the Australian Federal, State and Territory
and New Zealand Ministers for the Environment.   It has long
had an interest in waste management issues.   Recently it
turned its attention to reducing wastes and produced a draft
document entitled "Towards a National Waste Minimisation and
Recycling Strategy" (ANZEC, Mar 1991).   This document 
  box 2.2  
will be released for public comment.  
    The draft strategy has among its objectives the need
to change Australian production, consumption and disposal
activities to minimise waste and pollution and promote clean
production practices.  
    The strategy will be implemented in accordance with
the following principles:
&bullet;    Waste management hierarchy - waste
avoidance and reduction is the first preference.  
&bullet;    Clean production practices - to
avoid or eliminate hazardous wastes and hazardous products and
use the minimum amount of raw materials, water and
energy.  
&bullet;    Life-cycle approach - including all
aspects of resource use, waste generation, storage, transport,
treatment and disposal.  
&bullet;    Precautionary approach - working
towards longer term goals.  
&bullet;    "User pays" principle - the users
of resources pay fully for costs to society.  
&bullet;    "Polluter pays" principle - the
costs of containing or eliminating pollution are internalised
into the costs of production.  
&bullet;    Economic efficiency - actions that
promote greater waste minimisation but are not economically
viable may not be justifiable.  
&bullet;    Individual and corporate
responsibility - achievement of waste minimisation will depend
primarily on the decisions of individuals and corporate
producers and consumers, as the outcome is the sum of all
their actions.  
&bullet;    Education and awareness - concern
for waste reduction becomes an integral part of
decision-making by all in the community.  
&bullet;    Appropriate technology - the
minimum standard of waste reduction should be equivalent to
the best waste reduction achievable by proven and commercially
available technology.    
      In view of New Zealand 's close links with
Australia in trade and culture, these principles should be
considered when adopting goals for waste reduction.    
      2.2.2 South Pacific Regional Environment
Programme    
    The South Pacific region contains a major part of the
world 's oceans.   It is therefore not surprising that the
issue of marine pollution has been a part of the work of the
South Pacific Regional Environment Programme (SPREP).   One
of the causes of marine pollution is the disposal of wastes,
both domestic and non-domestic, including agricultural and
mining wastes (South Pacific Commission, Aug 1990a).   The
fishing industry and shipping also contribute towards marine
pollution.   In 1988 an integrated marine pollution
programme was established.  
    So far the programme has gathered data through
monitoring.   However, the information is insufficient to
provide an overall picture of pollutants entering the marine
environment.   Without adequate data it is difficult for
governments to take appropriate action to correct pollution
and waste problems.  
    An intergovernmental meeting was held in 1990 to
consider project proposals to be included in the 1991-92 SPREP
Work Programme (South Pacific Commission, Aug 1990b).   A
new project on marine pollution was proposed for the next two
years.   This would develop a more accurate database to
assess such pollution and provide a basis for recommendations
to prevent and control pollution.  
    The SPREP Secretariat is in the process of developing
a co-ordinated regional programme on pollution prevention or
waste reduction for land and freshwater pollution.   This is
likely to be addressed at the next intergovernmental meeting
in mid 1991.  
      2.2.3 European Community    
    The European Community (EC) was founded in 1957
through the Treaty of Rome (Haigh &amp; von Moltke, Jul/Aug 1990).
  At present it consists of 12 member countries, although
several other countries have applied to join.   The Treaty
of Rome, as amended by the Single European Act of 1987,
enshrines environmental policy among the official policies of
the EC.   In particular, Article 130r(2) lays down that
  "action by the Community relating to the environment
shall be based on the principles of preventive action,
rectification of environmental damage at source as a priority
and the principle that the polluter should pay"  
(Commission of the European Communities (CEC), Sep 1989).  
    The main institutions setting policies on wastes are
the Commission, which consists of 17 people appointed by the
governments of member countries, and the Council, which
comprises one minister from each member country (Haigh &amp; von
Moltke, Jul/Aug 1990).   The Commission has the power to
propose legislation, but only the Council may enact it.  
    In 1989 the Commission prepared a strategy for waste
management (CEC, Sep 1989), in which the prevention of waste
was the first priority.   A dual preventive strategy was
proposed:
&bullet;    Prevention by
technologies - the development of clean technologies for
industry.   Such technologies usually tend to improve
manufacturing processes generally.  
&bullet;    Prevention by products - the
preparation of ecological parameters for products aimed at the
introduction of a Community ecological labelling scheme.
  Public procurement can play a crucial role in implementing
this strategy.    This may be a quote    
    In May 1990 the Council passed a resolution on waste
policy in response to the Commission 's strategy (Council of
the European Communities, May 1990).   The resolution
welcomed and supported the strategy, and urged the Commission
and member countries   "to promote further the
development of clean technologies and clean products so as to
minimise the production of waste"  .  
    The European Community has also been considering new
product-packaging rules (Anon, Apr 1991), with a goal of
reducing the total volume of packaging produced by 10 percent
between 1990 and 2000.   The rules would prohibit the use in
packaging of harmful substances, such as heavy metals, and set
standards for the amount of recycled material in
packaging.  
      Because of its trade with the European
Community, New Zealand should ensure that its exports meet
these goals.    
        2.3 Public sector - countries and
states    
    Most industrialised countries have set in place
policies as goals for their strategies for waste management&semi;
these always include waste prevention as the first priority.
  In this section only a selection of these policies are
presented - for two of the Australian states, United States
(the federal policy plus a few of the states), Canada, The
Netherlands and the United Kingdom.   Some information on
the goals of other countries can be gleaned from information
presented on aspects of their waste reduction programmes in
subsequent chapters.  
    A study of strategies for hazardous wastes
minimisation in six countries - Japan, Denmark, Sweden,
Federal Republic of Germany, The Netherlands and Canada -
gives some information about policies in each of these
countries (Beecher et al, 1988).  
      2.3.1 Australia    
    Two Australian states, Victoria and New South Wales,
have been working on policies for reducing wastes.
  Victoria has now put in place a very detailed policy
(State of Victoria, Oct 1990), which covers implementation as
well as the goals.  
    New South Wales also includes waste minimisation in
its waste plan.  
    In 1986 the Victorian Government adopted its
Industrial Waste Strategy (Environment Protection Authority of
Victoria (EPAV), Jul 1985), which included the following
policy on waste production:  
      "Waste minimisation, recovery and reuse is
strongly preferred over disposal.   All reasonably
practicable measures should be taken by government, industry
and the community to avoid materials becoming part of the
waste stream."    
      "Strong measures should be taken to prevent
and avoid the use of industrial substances which may generate
intractable wastes, or which themselves may become intractable
wastes at the end of their life cycle."    
    The Environmental Protection Authority of Victoria
(EPAV) is implementing this strategy through waste management
policies and tools such as (Joy, Nov 1989):
&bullet;    Annual reporting of the quantity
and types of wastes generated.  
&bullet;    Licensing of major on-site waste
storage and handling facilities and off-site waste treatment
facilities.   Non-licensed premises are controlled on a
needs basis by Pollution Abatement Notices.  
&bullet;    The tracking of all prescribed
wastes from generator via transporter to treater or disposer
through a waste transport permit and certificate
system.    
    Initially this strategy was well received (Robinson,
Nov 1989&semi; Robinson, 1990) because it was the first public
commitment to waste avoidance as a priority in waste
management.   Unfortunately the strategy had limited
success, in part because of the lack of co-operation from all
sectors of industry in improving their waste management
practices.   Incidents involving wastes continued to happen.
  This led to the public losing faith in the strategy
because it perceived that the required changes had not
happened.  
    To overcome the difficulties, the Victorian Government
through the Environment Protection Authority is now using both
carrots and sticks to more effectively implement the strategy
(Robinson, Nov 1989).   Actions which have been initiated
over the past two years to encourage industry to identify and
take advantage of opportunities to minimise waste include
(Joy, Nov 1989):
&bullet;    The preparation of the Industrial
Waste Minimisation Policy.  
&bullet;    The establishment within the EPAV
of a Waste Minimisation Task Force (see section
5.2.1).  
&bullet;    The establishment of the Clean
Technology Incentive Scheme (see section 6.4.1).    
    The Industrial Waste Minimisation Policy was the first
policy to be prepared under the Industrial Waste Strategy
(Joy, Nov 1989).   The draft policy was released for an
extended period of public comment in April 1988, and came into
operation on November 1, 1990 (State of Victoria, Oct 1990&semi;
EPAV, Oct 1990).  
    The policy 's primary objective is   "... to
reduce potential hazards to human health and to the
environment posed by industrial wastes by ensuring the
generation of such wastes is minimised."     Secondary
objectives include a number of broad economic benefits
including resource conservation, reduced monitoring and
enforcement costs, and reduced infrastructure costs.  
    In assessing waste management options, the order of
preference is:
(a) waste avoidance and/or waste reduction&semi;
(b) waste reuse, recycling, and reclamation&semi;
(c) waste treatment&semi;
(d) waste disposal.  
    Principles of the policy include the adoption of an
integrated approach to the management of industrial waste, the
application of the "polluter pays" and "user pays" principles
to waste generation, treatment and disposal, and the use of
appropriate technologies.   Waste minimisation should be
considered for any new industrial projects or works early in
the project planning stage.  
      Such a detailed policy can act as a guide for
New Zealand.    
    The Victorian Government has recently developed a
Recycling and Waste Reduction Plan with the goal of a
reduction of 50 percent in waste going to landfill by the year
2000 compared to 1990 (based on weight per capita) (EPAV,
1991).   This goal is to be accomplished by a combination of
a reduction in packaging, the recycling of domestic,
commercial and building wastes, and the composting of food and
garden wastes.   After consultation on the draft plan, which
is based on a ten-point programme, legislation is to be
introduced into Parliament in 1991 with a target date for
commencement of January 1, 1992.  
    In New South Wales, the State Pollution Control
Commission (SPCC) published a discussion paper on waste
reduction and resource conservation in 1984 (SPCC, Nov 1984).
  Most of the discussion in this report centred on actions
to improve resource recovery and recycling and to prevent
littering, with only brief mention of preventing waste being
produced.   An example of waste reduction was the Industrial
Waste Exchange which was set up by the Sydney Metropolitan
Waste Disposal Authority (now the Waste Management Authority
of New South Wales) in 1977.   Reduction by industry in raw
material use was also cited.   The paper noted that
government preferred industry to take voluntary action to
achieve waste reduction and resource conservation, although
intervention might be needed if industry did not
co-operate.  
    In 1989 the approach in New South Wales was still to
use positive incentives for voluntary waste minimisation
efforts and not to rely on regulations (Cook, Nov 1989).
  Waste minimisation benefits, such as reduced costs for on-
and off-site waste treatment, lower risks for spills,
accidents and emergencies, and reduced production costs
through better management and efficiency, were believed to
provide industry with strong incentives for
self-regulation.  
    In 1990 the Waste Management Authority of New South
Wales (WMANSW) published its "Waste Planning for Industry: A
Guide" (Beck et al, Apr 1990).   This plan is based on the
hierarchy of waste management, with the prevention and
reduction of waste (called waste minimisation) being
emphasised.   The plan was seen as an essential first step
towards the minimisation of non-productive costs and the
avoidance of possible future liabilities.   Further
information about the plan is given in section 4.1.3.  
        2.3.2 United States      
    The need for waste reduction had been recognised as an
important aspect of waste management for some time in the US.
  When the Resource Conservation and Recovery Act was
amended by the US Congress in November 1984, the following
statement was included (Office of Technology Assessment (OTA),
Jun 1987):  
      "The Congress hereby declares it to be
national policy Of the United States that, wherever feasible,
the generation of hazardous waste is to be reduced or
eliminated as expeditiously as possible.   Waste
nevertheless generated should be treated, stored, or disposed
of so as to minimise the present and future threat to human
health and the environment".    
    This policy statement was supported by adding waste
minimisation provisions to the Act.  
    In 1986 the Office of Technology Assessment released
its report, "Serious Reduction of Hazardous Waste" (OTA, Sep
1986).   The following month the Environmental Protection
Agency (USEPA) delivered its report, "Minimization of
Hazardous Waste" (USEPA, Oct 1986a), to Congress, and also
issued its document on issues and options for waste
minimisation (USEPA, Oct 1986b).   Subsequently, OTA was
requested to analyse the USEPA report and to describe
  "how [US]EPA 's findings and conclusions differ from
those of OTA"   with the emphasis on
  "differences that either implicitly or explicitly
support different congressional actions"   on waste
reduction (OTA, Jun 1987).   The resulting report,"From
Pollution to Prevention: A Progress Report on Waste
Reduction", not only compared the OTA and USEPA reports but
also provided Congress with four critical policy choices.  
    In the US, environmental protection has conventionally
been improved by imposing more regulations and enforcing them
more firmly.   However, the environmental results of this
strategy are now acknowledged to have been disappointing, as
control technologies have failed to perform as expected, the
problem being compounded by human failures.   Moreover, the
conventional strategy both increased government spending and
added to the competitive disadvantage of US manufacturing
industries through high costs for pollution control.  
    Consequently the OTA has proposed that a
  "concerted national effort"   is needed to
reduce the generation of hazardous wastes and environmental
pollutants at their sources, whether they are regulated or not
(OTA, Jun 1987).   It felt that the US should acknowledge
that waste reduction is the   "environmental option of
choice"  , which also provides economic benefits.  
    The OTA felt that the lack of implementation of waste
reduction was because of   "... human, organisational,
and institutional obstacles in industry and
government"  .   These included:
&bullet;    Industry 's requirement to put
resources mainly into regulatory compliance.  
&bullet;    Government agencies focusing on
fixing the mistakes of the past.  
&bullet;    Companies most needing to implement
waste reduction usually having the worst competitiveness
problems.  
&bullet;    Potential economic benefits not
being understood or captured systematically in
industry.  
&bullet;    Using the term waste minimisation,
broadly interpreted to include waste treatment.   This was
leading to capital investment in new waste treatment
facilities such as incinerators, which can inadvertently
restrict waste reduction even though the latter offers better
environmental protection at lower costs.    
    However, the OTA report noted that nearly everyone
agreed that the use of regulations to prescribe waste
reduction is technically not feasible and administratively
impractical.   Instead the following three fundamental
policy options were offered to Congress (OTA, Jun 1987):
1.      "  Take no new action to directly
help industry to reduce waste generation."    
    This policy relied on industry efforts to achieve
waste reduction.   However, in practice waste reduction does
not usually take priority over other traditional responses to
rising environmental costs and liabilities, such as changes in
pollution control technologies, acceptance of high costs for
waste treatment and disposal, and occasionally the shutting
down of industries.   This policy also did not take into
account the obstacles to waste reduction such as the lack of
information on sources of wastes and technologies and methods
to reduce their generation.   Therefore, this option would
bring about little change to what was already
happening.    
2.    "    Institute a small Federal effort
through existing environmental statutes and regulatory
program[me]s."    
    This policy would have limited waste reduction to
certain regulated wastes and posed administrative problems for
the USEPA.   It could also have had limited credibility
because existing environmental programmes have done little
work on waste reduction.   This policy, too, might not
significantly change what was already occurring.    
3.    "    Through new legislation, establish
a separate Federal program[me] within [US]EPA to support waste
reduction and to provide national leadership.   Fund it and
State program[me]s by allocating several percent of [US]EPA 's
operating budget."    
    This policy would provide industry with Government
funded in-plant technical assistance and central sources of
information, which could lead the way for industry to adopt
voluntary, comprehensive waste reduction.   Thus industry
would learn that reducing the generation of all wastes is
technically feasible and in its own economic self-interest to
do as soon as possible.   State programmes could be funded
by a five-year seeding grant.   Moreover, increased
corporate profits from waste reduction savings could result in
increased tax revenues which would offset the cost of the
Federal programme, possibly in as little as one
year.      
    Option 3 was adopted.   On January 19, 1989, the
Pollution Prevention Policy Statement was published in the
Federal Register (Commoner, Jul/Aug 1989).   This
acknowledged that much of the USEPA 's past effort had been on
pollution control rather than pollution prevention.   By
this statement, a major reorientation of American
environmental policy was forecast.  
    Subsequently, the USEPA published its blueprint for a
comprehensive national pollution prevention strategy (United
States Government, Feb 1991).   The strategy has two main
objectives:
&bullet;    "  To provide guidance and
direction for efforts to incorporate pollution prevention
within [US]EPA 's existing regulatory and non-regulatory
program[me]s&semi;  
&bullet;  To set forth a program[me] that will
achieve specific objectives in pollution prevention within a
reasonable timeframe."      
    The problems of disposing of municipal solid wastes in
the United States have led to investigations into policies for
the prevention of household and commercial wastes (USEPA, Feb
1989&semi; OTA, Oct 1989&semi; Levenson, 1990).   A national policy
would have as its primary goal - source reduction (including
reuse) to decrease the volume and toxicity and increase the
useful life of products in order to reduce the volume and
toxicity of wastes.  
    There are two basic ways to prevent municipal waste:
1.  Manufacturers can change the design of products and
the way they are packaged.  
2.    Consumers can alter their purchasing decisions
about existing products and the way that they use and discard
products.    
    These two ways are connected in that consumer choice
alters   manufacturers'   behaviour, as has been seen in
the rise of "green consumerism".  
    The USEPA has embarked on a far-reaching programme to
implement this goal and to integrate pollution prevention into
the fabric of American society, so that it becomes an integral
part of everyone 's way of life (Reilly, Jan/Feb 1990).   It
has set a target of achieving a 25 percent reduction in the
country 's waste by 1992.   Moreover, the USEPA is
implementing its own agency-wide waste minimisation programme
as an example to others.   More information on specific
programmes is given in Chapters 4 and 8.  
    In the USA, states have the primary responsibility for
waste management.   In the past they have focused mainly on
waste treatment (primarily incineration) and disposal rather
than waste reduction.   However a number of the American
states have put in place policies on waste reduction.  
    Some of these policies are enshrined in legislation,
whereas others pursue waste reduction through executive
decisions.   Both California, in its Hazardous Waste
Reduction and Management Review Act, and Massachusetts, in its
Toxic Use Reduction Act, require producers of hazardous wastes
to set goals for reducing them (Parkinson, Jul 1990).   In
Massachusetts, after 1995 a company that achieves less waste
reduction than others in its industry may be obliged to meet
set goals, based on general results.  
    US government agencies have also set themselves goals
for reducing wastes (CMA, Feb 1990b).   For example, the US
Air Force has set a goal of reducing hazardous wastes by 50
percent by 1992.   It expects to spend US$42 million
annually to achieve this goal.   The US Navy has a more
ambitious goal of 100 percent reduction of hazardous wastes.
  It has already identified the 17 processes that generate
99 percent of its hazardous wastes.      
  

        Fire resistance of NZ concretes    
      The Building Research Association of New Zealand
(BRANZ) has been investigating the performance of concretes made
with local aggregates when exposed to fire in order to confirm or
update the fire resistance ratings currently found in SANZ
Miscellaneous Publication MP9 Fire Properties of Building Materials
and Elements of Structure.    
  byline with biography  
    MP9 specifies the minimum thickness of a concrete wall or
floor necessary to achieve a given fire resistance rating.   Fire
resistance of these types of concrete elements is usually
determined by the time taken for the average temperature rise on
the unexposed surface to exceed 140&deg;C.   The
fire resistance rating is then found by rounding down the fire
resistance to the nearest half-hour (usually).  
    The requirements in MP9 are "notional" in that they have
not been derived from fire tests of New Zealand made concretes, but
rather have been taken from overseas building codes and tests of
overseas concretes.  
    It is known that the thermal properties of concrete are
largely determined by the properties of the constituent aggregates,
in which there can be wide variation depending on the
mineralogical composition of the aggregate and its physical
location.  
    It was expected that the performance of New Zealand made
concretes could be quite different from overseas concretes
particularly as New Zealand has many aggregates of volcanic
origin.  
      Fire testing of concrete slabs    
    An initial study of the relative performance of different
aggregate type concretes was conducted by subjecting unloaded 1 m
  graph: relative performance of NZ concretes based on time to
exceed 140 &deg;C  
square vertical slabs to a standard fire test using the
diesel-fired pilot furnace at BRANZ laboratories at Judgeford,
north of Wellington.  
    The concrete slabs were 130 mm thick, reinforced with steel
bars and instrumented with thermocouples to record the
temperature rise within the concrete and on the concrete
surfaces.  
    A humidity sensor was also positioned within each slab to
monitor the drying process and indicate when the concrete was
ready to be tested.  
    Fire testing of all the slabs followed in accordance with
the ISO 834 test standard.  
    Casting of the slabs was performed by Central Laboratories,
Works and Development Services Corporation under contract to
BRANZ.   The concrete mixes generally differed only in the
geological source of the aggregate.  
    The aggregates studied were: limestone, quarried
andesite, quarried dacite, alluvial and quarried greywacke,
quarried basalt, alluvial andesite, alluvial quartz, rhyolite,
phonolite, and pumice.  
    These aggregates represented a wide range of concrete types
used throughout New Zealand, with the first six allocated a higher
priority.  
    The fire resistance achieved by each 130 mm thick slab
(determined by an average temperature rise exceeding
140&deg;C on the unexposed surface of the concrete) was
recorded and, in general, improved values of fire resistance were
indicated over those currently nominated in MP9.  
    This confirmed that the current "notional" ratings are at
least likely to be safe, and are probably conservative for some of
the aggregate types.  
    However, a limited programme of further testing is required
to examine both greater and lesser thicknesses of concrete before
definitive proposals can be made regarding changes to the current
MP9 requirements.   BRANZ will be undertaking this further
work during 1990.  
    The performance of the slabs (given as the mean time for
the temperature rise of the unexposed surface to exceed
140&deg;C), ranged from 152 minutes for the alluvial
quartz concrete to 306 minutes for the pumice concrete.  
    The volcanic aggregate showing the least fire resistance
was quarried basalt with 176 minutes.  
    The benchmark value for a 130 mm thick wall for comparison
with MP9 is approximately 138 minutes (resulting from an
interpolation between 120 mm and 150 mm thick siliceous aggregate
concrete).  
    Examination of the test results shows the possibility of
creating, say, five groups of different aggregates with
significantly different performances which may be useful in
revising current ratings.   For example: alluvial quartz and
greywacke (Group I)&semi; quarried greywacke, basalt, and dacite (Group
II)&semi; quarried andesite, phonolite, and rhyolite (Group III)&semi;
limestone and alluvial andesite (Group IV)&semi; and pumice (Group
V).  
    The performance of each group could then be based upon the
performance of the most conductive aggregate in the group.  
    During the fire testing of concrete, BRANZ was also
interested in the occurrence of spalling under standard test
conditions.  
    None was observed in any of the siliceous or volcanic
aggregates except pumice where minor surface spalling was
observed.  
    The limestone aggregate concrete, however, did exhibit
severe deterioration following the test, and after exposure to
weather, as a result of chemical changes (calcination).
  Fortunately, limestone aggregate concrete is not widely used&semi;
nonetheless its performance following a severe fire may be
questionable.    
      Conclusions    
    The initial study and test programme established the
relative performance between different types of aggregate, with
all aggregate types tested showing better performance (for the
single thickness examined) than the fire resistance ratings
currently indicated by MP9.  
    It is expected that once the test programme is
completed it will be possible to recommend reductions in the
concrete
  photo  
  caption  
thicknesses currently given in MP9 for a particular fire resistance
for most of the concrete aggregates examined.   A full report
presenting the results from the initial study will be published by
BRANZ.  
      Related Work at BRANZ    
    BRANZ has also been investigating fire engineering methods
for calculating the fire resistance of concrete members using
structural engineering principles and material properties at
elevated temperature.  
    Methods developed overseas have been available in the
literature for some time and, by preparing a background report and
technical guidance on this topic (to be published 1990), BRANZ
aims to increase awareness and encourage the use of fire
engineering calculations and rational fire design in New
Zealand.    
    photograph  
      The Hutt Estuary Bridge - A first for prestressed
concrete technology in NZ    
      It is an uncommon experience in the professional
life of an engineer to be associated with a project incorporating
new and untried technology on a significant scale.   The Hutt
Estuary Bridge, more commonly known as the Hutt Pipe Bridge, was
such a project.    
  byline  
    My personal involvement in the project from 1952 to 1954
was undoubtedly a significant milestone in my career.   The
foresight, initiative and courage of the owners, being the
combined local bodies of Lower Hutt City, Wellington City, Petone
Borough, Eastbourne Borough and Hutt County, the designer W G
Morrison Consulting Engineer and the contractor Wilkins &amp;
Davies Construction Co Ltd was an inspiration to all who
participated in the project.  
    The selection of the pipe bridge as part of the IPENZ 1990
Project to commemorate New Zealand 's engineering heritage
acknowledges both the engineering significance of the project,
being the first major prestressed concrete bridge designed and
constructed in New Zealand, and the enterprise and courage of those
who made it possible.  
    The unique features of the bridge, uncommon at the
time, included:
&bullet;  The adoption of a
post-tensioned, prestressing system with no precedent in New
Zealand
&bullet;    The engineering solutions adopted for
casting, storage, handling and erection of the 80 precast concrete
beams each spanning 105 feet and weighing 40 tons.  
&bullet;    The quality controls involved in mixing,
placing and curing of the high-strength concrete demanded by the
prestressed concrete solution  .    
&bullet;    The development and implementation of
quality controls associated with the prestressing
operations.      
        Concrete construction trends    
      Each decade of construction seems to go through
periods of adjustment where construction systems of one form or
another hold an ascendancy in popularity.   If the seventies are
broadly characterised by in situ structures with precast concrete
cladding, certainly the eighties can be classified as the
decade of precast concrete with structural frames and floor systems
being substantially precast construction.    
    The growth of precast structural systems has seen the
establishment of a review team from the NZ National Society for
Earthquake Engineering, the Concrete Society and the Cement &amp;
Concrete Association, to produce a state-of-the-art document on
good practice.  
  photo    byline  
      Whither the gay nineties?    
    If there is one thing guaranteed to ensure loss of market
share it is complacency.   For nearly 10 years I have advocated
the use of reinforced concrete shear cores with precast concrete
beam and post construction to support the floor system on
multistorey buildings.  
    Reading the various publicity information put forward by
the marketing forces for structural steel, I find ductile shear
walls in concrete with beam and post in structural steel as a
promotable item.  
    Some years ago an evaluation of timber construction was
carried out comparing a beam and post timber system stabilised
by concrete shear walls with a two way ductile reinforced concrete
frame.  
    Recent evaluation of structural concrete systems with US
consultants revealed that shear wall and gravity beam post
systems are by far the most popular form of construction.  
    The signs are that the 90s will see the development of this
form of construction in concrete as already practised in structural
steel and timber.  
    Our current developments in high strength concrete 70-80
MPa and higher can be conveniently applied to the gravity column
situation ensuring slender members result.  
    Fast erection of such systems is certainly correct.
  Some market play has been made by the promoters of structural
steel comparing beam/post steel erection times with ductile
concrete frame, in situ or precast.   By removing most of the
earthquake design elements from the concrete frame, one then starts
to be able to directly compare one system with another.  
    Steel promoters have drawn on overseas information to try
to prove a speed of erection superiority.   Information from the
UK certainly supported such claims in the early 80s since the
concrete industry 's complacency blunted any concerted efforts at
developments to improve the industry performance.  
    Within five years the roles were reversed with several
major developments that started off in structural steel having
subsequent stages redesigned in reinforced concrete - from both
cost and speed of erection viewpoints.  
    The concrete industry in New Zealand has a significant
onshore infrastructure which permits a greater degree of
flexibility during the design and construction processes to cope
with client variations.  
    Indenting of basic steelwork from overseas well ahead of
fabrication must represent a significant initial cost and early
design fix for a project.  
    There are no wonder structural materials in the
construction industry that are perfect for all building types and
configurations.   It is the responsibility of the architect and
engineer to map out the most cost effective use of a material.  
    The most successful buildings are a balanced use of the
basic building materials.   Having previously had the opportunity
to design and construct with all types of structural materials
I have seen both successful and unsuccessful combinations of
material.  
    As the degree of experience in material specialisation has
grown I naturally perceive a higher percentage of concrete
utilisation than most.  
    However all structural concrete is a composite material of
steel and concrete.   One of the country 's most successful
    cladding    claddings     for housing is a
combination of cellulose fibres (from timber) and Portland
cement.  
    The construction of concrete components depends upon steel
and timber, and hence the Cement and Concrete Association has an
active interest in the performance of these materials.  
    Whether the concrete is precast or not the matter of
forming the shapes still remains and while extruded floors require
little more than a flat plate lower surface, other components still
require to have formwork.  
    The formwork element of the total cost of a concrete
component still remains significant.   As a consequence the
need for the care and design of formwork to achieve both
satisfactory economic and finish requirements remains an
important goal.  
    Many serious defects in the finish relate to faults in
formwork.   Whilst placing and vibration of concrete are an
essential part of producing satisfactory finishes, they cannot
cover or make up for the deficiencies in the formwork into which
the concrete is placed.  
    If the formwork is correct then there is every chance of a
successful job&semi; if it is wrong then an unsightly, out of tolerance,
expensive eyesore is guaranteed.  
    With the need for the finished concrete components to
display what the owner or designer desires, it is not surprising
that the Cement and Concrete Association devotes significant time
in training and research to formwork design and detailing.  
    The material developments for the manufacturers of
proprietary formwork during the past 10 years has seen the gradual
introduction of aluminium materials in replacement of
steel.  
    Overseas occupational health laws are tending to specify
reduced weights for manhandling individual components of a
formwork or falsework system, and so aluminium will continue to
replace steel in componentry.  
    The pace of replacements will be linked to the potential
level of future building activity.   With proprietary steel frame
systems currently in stock and not in use, one is not likely to see
significant investment into re-equipping with aluminium based
frames in use overseas.  
    Within New Zealand the size of aluminium based floor
centres has at this stage been limited by a 205 mm diameter of the
forming die.   The significant use of precast flooring systems at
the moment would seem to act as a disincentive for the immediate
future development of larger aluminium based sections.  
    The contrast between the levels of in situ construction for
floors in Australia and New Zealand is significant.   The use of
in situ concrete for floors in Australia generates a whole raft of
falsework design associated with table forms.   Such systems
appear to be rarely used or necessary in New Zealand.  
    In the same way slip-forming of concrete service core walls
on multistorey development is common enough in Australian urban
construction yet comparatively rare in New Zealand.  
    Special formwork systems have been introduced at various
times into New Zealand by contractors intent on providing a special
construction solution.   One example of this was the Heitkamp
system used by Wilkins &amp; Davies Ltd for the construction of the
Ohaaki Cooling Tower.  
    Other self climbing system forms have been used on specific
projects but the potential lack of continuity of work and perhaps
significant modification costs between such work does tend to
negate their significant use in the short term future.  
    With increased load capacity of soldiers or walers from
traditional double timber sections, the whole pattern of using
form ties has changed.   No longer are there requirements for
  a   relatively close spaced low capacity load tie pattern to
suit the dual timber setup.   Instead more substantial
through-ties designed to carry much heavier individual loading are
the norm today compared to the previous decade of she-bolts.  
    In the matter of concrete pressures within the formwork the
Cement and Concrete Association have been evaluating the
pressure suggested by the UK organisation CIRIA.  
  photo    caption (Figure 1)  
    The on-site and laboratory measurements (see Figure 1) in
New Zealand confirm the general acceptance of CIRIA formulae but
also illustrate that, depending upon the height of the formwork
pour, the significant quantities of reinforcing steel required for
seismic design in building columns can result in reductions in pressure.  
    This research work was fund aided by
    Transit    Tranzit    which is how the company
spells it     NZ and is currently being reviewed prior to
full publication.  
    One feature of the use of precast concrete flooring has
been the need for falsework systems to cope with this concentrated
and eccentric loading on the building periphery.  
    There was perhaps some delay in coming to terms with the
new requirements of the loading situation leading   to   the
Department of Labour and others to require some attention to safety
aspects.  
    The realisation of the special requirements for this
falsework has tended to induce builders and contractors to
sub-contract the falsework erection to specialists to ensure that
the extra special skills now required are inputted into the job as
well as the aspects of responsibility.  
    Such specialisation has led to the greater use of scaffold
systems which, in the hands of a specialist, are perhaps easier
to erect than the heavy duty frame systems.  
    A growth of safety requirements is likely to continue.
  Local authorities are becoming tougher in their construction
bylaw ordinances with public safety in mind, with other
agencies continuing to see that worker safety in the building
industry is improved.   Hopefully, as safety is increased that
would become a recognisable factor in ACC levies.  
    The use of encapsulating perimeter climbing protection
units appears to be virtually mandatory in Australia.   However
the costs of such equipment needs to be balanced against the
potential reuse factors in New Zealand.  
    Alternative screen and barrier protection systems can be
put in place which will provide an upgrading in safety sought both
on behalf of the worker and public.  
      So whither the gay nineties?    
    There is no doubt the industry fortunes of concrete
construction are linked to the general building activity.
  Depressed activity as at present perhaps suppresses the drive
towards pursuing different techniques and materials.  
    However there is no doubt the industry is now well aware of
the opportunities and responsibilities it must take for the future
to face competition.  
    The developments during the past decade show the industry 's
capability to respond to the changing scenes, it just has to keep
it up.     

        CHAPTER THIRTEEN    
          Geothermal Generation        
      N  EW ZEALAND has a number
of geothermal fields, most associated with the Taupo vol-
canic zone which runs north-east from the central North
Island to the Bay of Plenty.   In these fields the intense
heat beneath the earth 's surface  'superheats' underground
water under pressure in reservoirs found in areas of
fractured, fissured and permeable rock.   The water does not
boil and become steam until the pressure is released.   The
task is to extract the water or steam from the reservoir and
deliver it as steam to a turbine.  
    Interest in exploiting this source of energy arose
after the Second World War, when it was becoming apparent
that sources other than hydro-electricity might have to be
looked at to satisfy the North Island 's demand for power.
  Semple stated in 1947 that the use of geothermal steam
should not be overlooked and endorsed immediate
investigations.    1       New Zealand
soon became an international leader in the exploitation of
geothermal resources.    
      Wairakei    
    The power station at Wairakei, just north of Taupo,
was the second large-scale geothermal generating station in
the world and the first to use 'water-dominated' or 'wet'
steam.    2       'Wet' steam, which is
characteristic of New Zealand fields, is produced when the
temperature gradient in the field is insufficient to convert
all the water into  'dry' steam.  
    Geothermal energy was first exploited at
Larderello in northern Italy around 1897.   At first it
was impossible to use the natural steam directly because it
was too corrosive of the metals then used in turbines&semi; instead
it was used simply as a source of heat to raise steam in a
boiler.   By 1913 a 250 kW steam-turbine-driven generator
using this method was supplying power to the ancient city of
Volterra.   These developments were familiar to engineers
and others in New Zealand.   In February 1918 the Masterton
Chamber of Commerce asked for an enquiry to see whether the
same technique could be used here.   Nothing seems to have
come of this, but others kept asking the same question.   In
1920 an Auckland entrepreneur asked the government for the
right to generate electric power by geothermal means near
Waiotapu.   Birks reported that generation would be
feasible, but on too small a scale in comparison with
hydro-electric power to be of any commercial use.   In
1924 Dr E. Marsden tried to interest Furkert in the idea after
the journal   Nature   published an article on the
use of steam in Italy.   Furkert 's reply was much the same
as   Birks'   had been - geothermal power would only be of
interest if it was essential to use steam, and in New Zealand
it was not.   Furkert visited Larderello in the late
1920s but saw nothing to make him change his mind.  
    By the 1930s some scientists had become interested in
the chemistry of geothermal water, and a large number of
shallow bores were drilled at Rotorua, Taupo, Tokaanu and
Helensville for heating and bathing.   However, few here
seem to have followed developments at Larderello in the late
1930s, when turbines that showed promising resistance to
corrosion were built.   By 1943 a 70 MW station had been
commissioned there, but it was later destroyed by the
retreating German army.   A Public Works Department engineer
who served in Italy was able to bring back a great deal of
information about what had been done there.  
    Geothermal power began to emerge as a more realistic
possibility during the period of post-war 
  map/plan  
shortages.   The Rotorua Borough Council even asked for the
Italian plant as its share of war reparations.   When
told that this was impossible, and that the State
Hydro-electric Department had no immediate intention of
developing the resource, Rotorua wanted to go it alone.
  Kissel, afraid that a makeshift drilling programme might
be undertaken, argued that any geothermal power
development should be by the state and in the long-term
interests of the system as a whole.  
    By 1947 interest had quickened after two years of
serious drought.   In June Kissel recommended that
investigations be made and trial bores sunk.   Meanwhile
local authorities kept raising the issue.   In 1948 the Bay
of Islands Electric Power Board wanted to exploit the Ngawha
geothermal field and Taupo sought a geothermal plant.   As a
result of this pressure the Commissioner of Works and
engineers visited Larderello to study the techniques used
in deep drilling and the construction of bores.   A
Geothermal Advisory Committee set up in 1949 proposed a
five-year survey of the whole thermal area from National Park
to the Bay of Plenty, but the incessant clamour for
development led to the committee recommending in November
1949 urgent investigation of the most promising area,
Wairakei.  
    Drilling began in March 1950 using light rigs that
were able to reach depths of 750 to 1,500 feet, and a camp for
50 men was set up at Taupo.   Before long it was realised
that they were dealing with something fundamentally different
from Larderello.   They were tapping superheated water that
flashed 'wet' steam when pressure was released rather than
 'dry' steam.   Vigorous investigations continued.   By
April 1951 the Ministry of Works was fairly sure that power
could be produced at Wairakei and anticipated that, as in
Italy, 
  photo  
  captions  
  cartoon  
  photo  
  caption  
deeper drilling would find dry steam at high pressure.   Two
development wells and fourteen prospecting wells had now been
drilled.  
    Cabinet approved requests for new drilling equipment
and to send three officers to Italy for first-hand study of
the drilling techniques used for handling steam at high heat
and pressure.   This experience was of considerable value,
but drilling conditions in the two countries proved to be very
different.   New and larger T12 rigs arrived in late 1952&semi;
they were able to go down to 3,000-4,000 feet and soon proved
the potential of the field, finding superheated water at up to
480&deg;C under pressure.  
    At this time there was a new twist to the story.
  Early in 1952 the United Kingdom Atomic Energy Authority
at Harwell began to consider the development of a plant to
produce 'heavy water' to be used as a  'moderator' in nuclear
reactors.   Sir John Cockcroft, the Director of Harwell,
visited New Zealand on a lecture tour that year and told the
government that momentous defence issues were at stake.
  Britain was to have its own nuclear weapons, and the real
object of the exercise was the production of plutonium rather
than electricity.   The attraction of the geothermal
resource was that nature had provided vast quantities of the
hot water needed to produce heavy water.  
    After initial discussions about other geothermal areas
Harwell advised in January 1953 that it seemed possible to
combine a heavy-water plant and an electricity-generating
plant at Wairakei, given that the former on its own would
waste more than half the available steam.   But the extent
of the resource was still an unknown quantity.   In April
1953 Davenport was on the point of recommending a 20 MW
station, but a combined plant would need more than twice as
much steam as had yet been proved.  
    In May 1953 Cabinet approved the combined plant in
principle, and in August the consultants Merz and McLellan
were engaged to report on its feasibility and cost.   The
government, which wanted to start building the station
straight away, awaited the report anxiously.   In early 1954
the British asked for six to twelve tons of heavy water a year
from 1957.   After receiving the report the Prime Minister
announced that the plant would generate 47 MW (from two 6.5 MW
high-pressure units and three 11.2 MW low-pressure units, with
the heavy-water plant interposed between them) and produce six
tons of heavy water a year as a joint venture.   In February
1955 Geothermal Developments Ltd was formed, with the New
Zealand Government and the British Atomic Energy Authority as
its shareholders.   Wairakei would produce power as a
base-load station at the end of 1958.   The State
Hydro-electric Department would buy power from the company,
and when there was no longer any need for heavy water the
plant would revert to New Zealand control.  
    However, it was by now becoming doubtful whether heavy
water had much future as a moderator, and indeed whether
moderators would be required in the next generation of
reactors.   In January 1956 the British partner withdrew
because of a relative shift in the economics of graphite- and
heavy-water-moderated reactors, combined with a doubling of
the cost of the heavy-water part of the plant.   Wairakei
became a New Zealand geothermal power project.   It was too
late for any radical redesign work, since the contracts for
the turbo-generators had been let in 1955 and fabrication was
well advanced.  
    The basic procedure developed for exploiting the field
was to drill holes down to the reservoir to depths of up to
4,000 feet.   The bores were allowed to vent into the air
for some time to discharge debris.   The holes were then
cased with a steel tube, the lower part of which was slotted
or perforated to allow the water to enter the pipe.   The
surrounding ground was extensively grouted with concrete to
stabilise it, and valves at the wellhead controlled the flow.
  The discharge from the bores goes into cyclone separators
that extract the hot water (which is more than three-quarters
of the total discharge).   The discharge enters the
separator at an angle at high speed, creating a
centrifugal action and allowing the water to drop to the
bottom and the  'dry' steam to be collected by a vertical pipe
near the top.   Waste hot water is led to two adjacent
 'silencing' towers, where its flow is split into two
contra-rotating flows.   The waste steam goes out the top
and water out the bottom and into the Waikato River via the
Wairakei Stream.   It is this waste steam issuing out of the
silencers that is the chief visual feature of the field.  
    The dry steam used for generation travels at speeds of
up to 130 miles per hour along many miles of branch pipeline
of six to twelve inches in diameter to more than twelve miles
of main 20- to 30-inch pipelines and thence to the power
station itself.   In 1972 a 42-inch low-pressure pipeline
was also installed.   At intervals along the pipelines
raised loops take up heat expansion and steam traps deal with
condensation and droplets still suspended in the steam.
  The steam then enters the powerhouse and, depending on its
pressure, drives the appropriate turbines.   High-pressure
steam joins the intermediate-pressure steam after going
through the high-pressure turbines, and intermediate-pressure
steam, after use, likewise joins the low-pressure steam
supplying the low-pressure turbines.  
    The two Wairakei stations (A and B) were sited by the
Waikato River for ease of access to cooling water.   They
were built on raft foundations because of the ground
conditions.   Large quantities of water are required to
condense the steam leaving the low-pressure turbines.
  Condensation increases the pressure differential and
allows the generation of twice the power achievable without
it.  
    An additional two intermediate-pressure machines of
11.2 MW replaced the heavy-water plant, and the planned
capacity of the station was raised to 69 MW.   Wairakei 's
machines were designed to operate variously on high (about 180
psi), intermediate (50 psi) and low (0.5 psi) steam
pressure.  
    During 1956 construction of the station and
development of the field proceeded rapidly.   The major
contract was let to International Contractors, a
combination of New Zealand, British and Swiss firms.   A
camp for 100 men was set up on the site, and some 27
production bores were established.   Work was done on
powerhouse foundations, the cooling-water pumphouse and
culverts, and on five 20-inch steam pipelines, which by 1958
extended eight miles up the valley&semi; branch pipelines were also
being built.   Up to 700 
  captions  
  photos  
were employed on the project when work was at its peak.  
    Because more steam was found in the field, a second
stage of development was proposed and two more high-pressure
11.2 MW units were included in the station, and another
adjacent station with three 30 MW mixed-pressure units (the
turbines could run at either 50 or 0.5 psi) was planned.
  These were the largest geothermal steam turbines in the
world.   This brought the total planned capacity to 190.6
MW.   It was hoped that another two 30 MW units could be
added later as Stage 3&semi; the design took this into account.  
    At this time the consultants Merz and McLellan 
  caption  
  photo  
advised that the project should proceed to Stages 2 and 3,
subject to evidence of adequate geothermal resources, and
could produce as much as 250 MW if the waste hot water could
be converted into steam.   Tenders for Stage 2 were
called.  
    By 1958 drilling for Stage 2 had begun, and the
pumphouse and powerhouse were nearly completed.   The
first unit was commissioned on 13 November 1958, but its
operation remained limited because of the extensive testing
required and early plant failures.   By March 1960 all the
Stage 1 units were producing power and the station was
generating 50 MW at peak&semi; this rose steadily as further units
were brought into service.   By 1962 the station was
achieving the high load factor that was intended.   In the
last two months of that year Wairakei produced more energy
than any other North Island station.   From 1966 (apart from
1968 when the field was partially shut down) the load factor
was a very high 85 to 90 per cent.   The remaining units
were commissioned at regular intervals, with the last 30 MW
Stage 2 unit producing power in October 1963.  
      It was hoped to use the hot water discharged in
the field for producing further steam, and techniques for
transmitting the hot water from the wellhead extraction pumps
to the power station were developed.   Water was collected
from a group of wells and kept pressurised by pumps at the
wellheads to prevent it flashing to steam before reaching the
station.   Such techniques were entirely untried for the
volumes involved.   A pilot hot-water 'flashplant' using hot
water from seven bores was eventually incorporated into the
station.   This supplied intermediate- and low-pressure
steam.  
    Even before Stage 2 was completed and the three 30 MW
machines were commissioned in 1962-63, it was clear that the
Wairakei reservoir was running down&semi; pressures and steam
production were falling off.   The commissioning of Stage 2
accentuated this problem.   The pilot hot-water flashplant
scheme was eventually commissioned successfully in July 1963,
but ran for less than a year before falling water output led
to it being abandoned.  
    Production drilling continued until the mid-1960s, and
the actual peak power output of 173 MW, still short of the
installed capacity, was achieved in 1965.   By then over 120
bores had been drilled, and the emphasis had shifted to
managing the field to sustain existing capacity.   Pressures
for the high-pressure units were allowed to drift downward
over the years, rather than a more rapid rundown at full
pressure.  
    The idea of flashing the separated high-pressure bore
water was not forgotten.   It was applied again from the
mid-1960s, first with double flash units installed at selected
high-pressure wellheads, and then with the establishment of
several semi-centralised double flash plants.   These
produce additional steam by allowing high-pressure hot water
to flash to steam twice (producing intermediate pressure)
or three times (low pressure).   In November 1982 the
process of derating the high-pressure bores to intermediate
pressure began, as pressures had fallen to a level that could
no longer sustain high-pressure generation.   The station 's
capacity was reduced to 157.2 MW as the high-pressure turbines
were decommissioned.  
    By September 1988 the Te Mihi development involving
three wells was completed.   By adding 18 MW, this restored
the station 's output to close to its capacity.   The
reinjection of waste hot water has been investigated since
1982, and in March 1991 work that will enable the reinjection
of more than half of the total discharge was approved as a
first stage.  
    Some 50 wells are currently in production, and the
station produces about 153 MW net, but maintaining this output
requires continued work.   Although Wairakei has not
produced the 250 MW that was initially hoped for in the late
1950s, its output is close to installed capacity.   It has
been a remarkably reliable performer and has the highest load
factor of any station in the country.      
  

          Lies, Damned Lies and Benchmarks    
    By Evan Torrie    
      Computer buyers are
constantly being bombarded by vendors claiming their new
machine runs the XYZ benchmark 20% faster, or that it achieves
50% more widgets/second than the competition.   Indeed,
hardly a week goes by without someone claiming their new
product is the fastest of its type in the world.    
    But how do vendors measure this "performance" which
forms the basis of their claims?   The answer lies with
tests, commonly known as   benchmarks  , which
are explicitly designed to measure the performance of a
system.  
    Just what are these benchmarks, and should we really
take any notice of them when making a decision on which
computer to buy?    
      What Are Benchmarks?    
    A benchmark is simply a method of judging the
performance of a computer product.   Typically, this
involves measuring the time taken to perform a specific set of
carefully defined tasks, and can test anything from CPU and
disk performance through to efficiency of a mouse versus a
keyboard in a word processing program.  
    Benchmarks can be split into two types: microscopic -
those which look at the components of a system in detail, and
macroscopic - those which look at the overall performance of a
system as a whole.   Traditionally, in the microcomputer
world at least, the emphasis has been on microscopic
benchmarks.   These micro benchmarks are useful for
measuring the maximum capability of a component, but they
often do not give a true indication of how the system
performs for the average user.  
    A macroscopic benchmark measures the performance
of a real application program in a particular system
configuration.   Popular microcomputer benchmarks involve
operations such as sorting files in a database,
recalculating a spreadsheet, or scrolling through a word
processor document.   Recently, a set of application-level
benchmarks for RISC workstations has been developed by
SPEC (Systems Performance Evaluation Co-operative).   Up in
the mini and mainframe environment, real application
benchmarks are the order of the day, with the oft quoted TP1
"transactions/s" benchmark for on-line transaction
processing.  
      What Is A Good Benchmark?    
    A good benchmark must be meaningful.   In other
words, it must measure something that is relevant to the
user 's requirements.   It is no use running a benchmark
designed to measure floating point performance if the
application is a string processing program.   Similarly,
running CPU performance benchmarks is not relevant if the
major application is a database, which will usually be limited
by I/O performance.  
    Secondly, the benchmark must be accurate and
repeatable.   That is, a benchmark run tomorrow should give
the same result as it does today.   This cannot always be
guaranteed, especially with benchmarks which use
random numbers, or benchmarks designed to measure variable
factors such as network performance, but the results should be
within a quoted level of accuracy for each run.  
    Thirdly, the results of the benchmark must be
verifiably correct.   Many existing benchmarks do not
actually perform any useful computation, so it is difficult to
check whether the test has performed correctly.   With the
advent of super optimising compilers, which sometimes
optimise to the point of producing an incorrect program,
this requirement is even more vital.  
    Finally, a benchmark should give similar results for
similar systems, but should also be able to discriminate
between different systems.   Many of the popular
benchmarks fail to meet this criterion because they rely on an
intermediate step between what the person who wrote the
benchmark program intended, and what actually gets
executed - namely the compiler - which translates the
high-level language source into machine executable
instructions.   Optimising compilers are especially
notorious for not doing what the author of the benchmark
intended it to do.  
      Common Benchmarks    
    One of the reasons that benchmarking remains an art
rather than a science is that there is no official standard
for benchmarks.   Recent efforts by IEEE and the SPEC group
are attempting to overcome this, but in essence, most of the
benchmarks commonly used today are de facto standards.
  These benchmarks are often poorly-defined which in turn
leaves them open to abuse by vendors.  
      MIPS, VAXMIPS and VUPS    
    One of the biggest misnomers in the benchmark world is
the concept of Millions of Instructions Per Second (MIPS).
  Just a few years ago, it was common to quote MIPS
figures as the number of   native   instructions
executed per second.   This was fine, so long as the CPUs
being compared were the same architecture.  
    However, as soon as you try to compare different
architectures, the concept of   native   MIPS falls
by the wayside.   For example, RISC (Reduced Instruction Set
Computer) CPUs have a goal of executing 1 machine
instruction per clock cycle, whereas CISC (Complex
Instruction Set Computer) chips such as the 80386 typically
take 4 or more cycles to execute their simplest
instruction.   Thus, a 16 MHz RISC chip will execute 16
million RISC instructions per second, versus the 16 MHz
80386 's 4 million CISC instructions per second.   The
naive user would naturally assume that the RISC chip was 4
times faster than the CISC chip.  
    The fundamental flaw in this argument is that each
RISC instruction is very simple, whereas the CISC
instruction is usually much more complex.   For example,
to add a register to a memory location on a typical RISC
machine requires three instructions - one to load the data
from memory into another register, one to add the two
registers together, and another to store the new value back
into memory.   The typical CISC machine, on the other hand,
takes just one instruction to do this.  
    Because of this discrepancy between the two classes of
CPUs, and indeed even between members of the same class, e.g.,
Intel 80x86 versus DEC Vax, a convention has been established
to measure all performance   relative   to a Digital
Equipment Corporation Vax 11/780 minicomputer.   Performance
is measured by running a program on the computer in question,
and then comparing the same program on a DEC Vax 11/780.
  The Vax is taken to execute at 1 MIPS, so the measure of
performance is now termed   VAX   MIPS.   Taken a step
further, more recent nomenclature has introduced the Vax Unit
of Performance, or VUP.   One VUP is taken to be the
performance seen on a Vax 11/780, and results are measured
relative to this to obtain a VUP figure for any
processor.  
      Classic Benchmarks    
    Some benchmarks have been around long enough to be
considered as classics in the world of performance
measurement.   A few of them are beginning to show their
age now, but it is still worthwhile to investigate them to see
why they are beginning to fail.  
    The   Dhrystone   test is one of the best
general-purpose tests developed so far.   It attempts to
measure performance based on a statistical analysis of typical
programs.   It consists of integer and string processing
computations, along with numerous procedure and function
calls.   However, there are no floating-point operations
because, in general, an analysis of common user programs
showed that there was little or no floating point
computation.  
    Both the   Whetstone   and
  Linpack   tests were developed to simulate
typical scientific work, and hence involve almost pure
floating point operations.   The Whetstone benchmark was
based on an analysis of almost a thousand Algol programs.
  The Linpack benchmark simulates typical matrix
computations such as matrix multiplication, and was designed
to test how well vector computers such as the Cray 1
handled vector computations.  
      Other Benchmarks    
    The   SPEC   group was formed just over a
year ago to establish a common basis for testing high
performance workstations.   The benchmarks in the SPEC
1.0 suite of tests consist of Fortran and C programs which
test CPU-intensive computation associated with scientific
and engineering applications such as electronic CAE, CASE and
science applications.   Future SPEC benchmarks will
measure I/O, memory and network performance.  
    One of the most useful benchmarks in the minicomputer
and mainframe world is the   TP1   benchmark.
  This measures on-line transaction performance on
multiuser hosts by simulating a series of banking
transactions.  
    Benchmarking in the graphics area has been extremely
limited to date.   This is surprising considering the trend
towards WYSIWYG systems at all ends of the spectrum.   One
of the problems is that graphics can mean different things to
different people.   For example, three dimensional graphics
presents an entirely different set of problems from two
dimensional graphics.  
    The problem also arises of what we are actually trying
to test.   Should we be testing throughput, i.e., how
quickly we can put a precomputed image on the screen, or
should we be measuring manipulation, i.e., how fast we can
compute that image?  
    Benchmark testing of I/O performance has also been
limited.   Much of what has been done has been too
simple, preferring to measure easily obtained figures
such as peak transfer rates rather than more realistic
situations such as random access or repeated access to
directories and file allocation tables.  
    Another factor that could command an entire article
all by itself is the issue of network performance.   As the
year of the LAN threatens to draw ever closer, networks and
network software need to be examined to determine which system
provides the maximum throughput for a particular
environment.   This can depend on many factors, from
the networking medium on the hardware side, through to the
efficiency of the network software with varying numbers of
users.  
    Many of these decisions depend on the particular
environment in which the system will operate.   Unless you
know of someone who is running an identical setup to yours, it
is almost impossible to gauge exactly how well a
particular setup will perform.  
      What Are The Problems With Micro Benchmarks
...    
    The problem with any microscopic benchmark such as the
Dhrystone benchmark is its sensitivity to factors external to
those which the benchmark is trying to test.   The ideal
micro benchmark would isolate one component of the system,
and test just that component.   Unfortunately, as
computers become more sophisticated, that ideal is
becoming harder and harder to achieve.  
    For example, the Dhrystone benchmark makes
extensive use of small string processing subroutines on fixed
length strings.   Optimising compilers have been developed
which detect that the subroutine can be "inlined" -
instead of making a subroutine call, the subroutine can be
coded directly into the calling routine.   Compilers can
detect that the strings are always the same length which
makes for more efficient code on some processors.   The
Dhrystone test is also very small, using less than 32K for its
data structures and program code on most machines.   For
machines with cache memories, this means that the Dhrystone
can fit entirely in cache, something which is not true of
most real applications.  
    Optimising compilers have certainly made work harder
for the benchmark developer.   Most benchmarks simulate late
amounts of processing by repeating a small set of
instructions multiple times.   Compilers can often detect
that rather than doing something different each time, the set
of instructions is performing the same operations and
producing the same results each time.   So instead of
executing the loop many times, the compiler will optimise the
loop down to just one repetition.   The results obtained
will remain the same, but instead of taking 10 seconds for a
million repetitions, benchmarkers will suddenly find that
their test runs in just a few microseconds!  
    A good example of how factors such as the compiler,
linker, and cache memory affect the Dhrystone benchmark
can be seen with the figures obtained from various 80386
machines.   For 16 MHz and 20 MHz machines, results range
from 1724 to 9436 Dhrystones per second.   This variation of
5.5 in performance is certainly not due to the CPU, which
differs only by a factor of 1.25 in clock speed, even though
the Dhrystone purports to measure CPU performance.  
      ... And Macroscopic Benchmarks?    
    Although macroscopic benchmarks are a much better
indication of real system performance as a whole, they
rely on measuring a "typical" application as seen by the
benchmark developer, and thus are subject to the judgement of
the developer.  
    For instance, most database benchmarks put great
emphasis on sorting large amounts of data, since this is
perceived to be a major database operation.
  However, there are many applications which require very
little sorting, and for which fast searching of an unsorted
database is more important.  
    The other problem with system benchmarks is that they
try to simulate a typical user 's operations.   Yet, because
of the difficulty in setting up a benchmark, many of these
system benchmarks operate in a small environment and then
extrapolate the figures to a large environment.  
    A real-life database, for example, may have 10,000 to
100,000 clients on record.   But most database benchmarks
work with a limited subset of records, say 1,000, and then
extrapolate the results to the larger values.   This is a
very dubious operation, since some products use algorithms
which work well with small numbers of records, but degrade
significantly when the number of records increases.
  Equally, some products designed for large applications
will use algorithms which do not perform very well with small
numbers of records, but require numbers to be above a
"break-even point" before they really start to show their true
performance.  
        Suggestions For Better
Benchmarks    
    The computer industry has realised that many of the
common benchmarks have major flaws, and are working to correct
them.   What can be done to improve the situation?  
    Benchmarks based on isolating system components
need to improve their degree of isolation and modelling of
real world applications.   This may involve rewriting
programs to eliminate any possible compiler optimisation, as
well as increasing the size of tests to reflect real system
usage.   Tests such as the SPEC suite are attempting to
accomplish this with their requirement for full
documentation of any optimisation.  
    As well as measuring the system performance as a
whole, emphasis should also move to how well a system
interacts with the user.   Simple facets of computer
operation such as installation, training and cost of
support are often neglected in favour of hardware cost by the
computer purchaser.  
      WIMP   systems, for example, place a heavy
demand on hardware, requiring fast processors, large high
resolution screens and megabytes of memory.   Hence, the
hardware for a WIMP machine usually costs much more than an
equivalent character oriented computer.  
    However, WIMP proponents have always claimed that
a windowing menu-based system increases productivity in
everyday use, user training and user support.
  Unfortunately, there has been very little qualitative
research done on the benefits of a WIMP system, and this is
one area where a lot more work needs to be done.  
      Do We Really Need Them?    
    Given that there seem to be so many problems with
existing benchmarks, is there reason to take any heed of them
when making a computer purchase decision?  
    Well, yes and no.   Nearly all of the commonly
quoted benchmarks measure only CPU performance.   If
your work involves massive computation problems, such as
scientific or technical research, then a performance
measure such as the Whetstone or Dhrystone test is
probably a pretty accurate indicator of the relative speed
of various machines.  
    However, business use places very different demands on
a system from technical use.   Databases, for example, are
usually heavily I/O oriented which requires fast disk
performance.   Similarly, the increasing acceptance of
windowing-based systems demands a fast graphics subsystem.  
    Although micro benchmarks are the most common and
popular tests, the real issue of any performance
consideration is how well the   entire   system
runs when used for practical work.   This involves not only
the components such as CPU, I/O, and graphics, but also
the human interface features of the software itself.  
    In the future real application benchmarks which
test all these factors will become commonplace.   In the
meantime,   caveat emptor   remains a valid maxim
for any computer purchaser when confronted with a
vendor-supplied benchmark.      
  

        9 OPERATING EXPERIENCE    
    While the HVDC link has had its technical problems and
consequent operating frustrations and
anxieties,    29     at least until solutions
could be found, it nevertheless continues to fulfil its
transmission objectives.  
    In 1989 dollars it has cost an average of $4.89 million
to operate and maintain for each of its 25 years.   This
includes two years when cable repairs cost $19 million and
$15.8 million respectively.   During 1988/89 its $5.75
million cost included a significant amount of upgrading, repair
and corrective work.    caption    photo  
    Since 1970, except for one year, the link has met the
system 's 91 percent annual target availability.   To ensure
the target is met, the yearly aim is 95 percent availability,
which, in fact, is required for some seven specific months each
year.   The result has been the consistent achievement of one
of the world 's highest availability and utilisation
figures, comparing favourably with even some of the more modern
thyristor links.   In doing so it delivers some 20 to 30
percent of the North Island 's electricity requirements at
around 3.758  cents/kWh less than the average cost of North
Island units generated from a mixture of hydro and thermal
power stations.  
    Since its commissioning, the link has undergone various
modifications, such as aseismic (anti-earthquake) strengthening
of the stands which support the valves and smoothing reactors,
in a steady continuing development to keep ahead of maintenance
needs and improve operational
performance.      30    
      MERCURY-ARC VALVES    
    The mercury-arc valves have been progressively modified
to reduce the number of consequential arc-backs.   The
phenomenon of arc-backs, arc-throughs, main valve commutation
failures etc occurred much as expected when the project was
completed.    caption    photo  
    However, unexpected consequential arc-backs, a serious
form of random and persistent short circuit of two or three
valves, occurred in both rectifier and inverter operation.
  In fact, their appearance on the New Zealand system was the
first anywhere, although they later began to occur elsewhere
around the world.   Modifications to reduce their increasing
frequency were begun in 1969.   By the following year 25
percent of the work had been completed, with the link
maintaining a 94.5 percent availability, comparing well
with most hydro-electric stations.  
    By 1971, 85 percent of the valves had been modified but
the performance of the others continued to deteriorate, the
department 's annual report stating that   "the overall
improvement [resulting from the modifications] in valve
performance does not appear to be as good as expected"  .
  By 1972, however, things had improved and the department
reported that while modifications had still not been completed,
partly because of new and unexpected problems with the
modifications themselves, and partly because of recent
developments which were being incorporated   "the
situation regarding valve performance is much more promising
than it has been for some time"  .    photo  
  caption  
    A year later both modification progress and the
introduction of improved maintenance techniques had reduced
transient valve incidents from 97 the previous year to 54 in
1973.   A year later the number was reduced to ten.  
    Nevertheless, as the valves aged the frequency of all
types of valves incidents occurred at an increasing rate.
  It was eventually found that the anode porcelain
characteristics influenced the rate of deterioration, more
particularly in the case of inverter operation.   New
porcelain anodes were fitted, the valve anode and grid
assemblies were modified and more stringent de-gassing
procedures introduced.   Because valve commutation under low
current conditions, particularly at valve-group starting, was
significantly influenced by stray capacitances of the group
circuits, damping circuits were included in all groups where
they were required.  
    During the first years, some valve internal components
maintenance was carried out by the Swedish manufacturers but
the department later developed the expertise and equipment to
do the work in New Zealand.  
    Most other equipment has performed as well as its AC
counterparts on the rest of the national New Zealand
electricity system.    
      TRANSMISSION LINE    
    There are strong winds, often up to severe gale force,
over the line 's North Island coastal section between Oteranga
Bay and Haywards.   These make maintenance work, such as
replacing insulator strings, the occasion for determined effort
and perseverance by line staff.   Often, there is not the
choice of waiting for better weather before carrying out urgent
remedial work both on the line and at the Oteranga Bay terminal
station.   Operational constraints and the cost implications
of an unscheduled shutdown of the link, especially at times of
high North Island demand, mean that usually the work has to be
done at short notice, whatever the conditions at the time.
  Gales can also cause difficult and sometimes dangerous
driving conditions to Oteranga Bay.   On at least one
occasion a heavy vehicle has been blown off the road and
overturned.  
    However, early maintenance experience was dominated by
insulator-fitting problems, particularly ball hook failures,
bent hanger brackets, trouble with inadequate strength and/or
crossarm bracing.   In August 1975 seven consecutive towers
collapsed and another was badly damaged in Canterbury during a
severe wind-storm.   Transmission was interrupted for five
days while the damaged section was replaced with a temporary
deviation using guyed emergency towers, a massive job with line
gangs working from dawn to dusk with the aid of an Air Force
helicopter.   The deviation was eventually replaced with
a section of new line.  
    By the late 1970s towers near Oteranga Bay were
severely corroded and another near Johnsonville was found to
have a badly cracked leg member.   The latter was replaced in
1981 and a new section of ten towers was built at Oteranga Bay
the following year.   In October 1988 two towers collapsed in
central Canterbury during a severe gale, and were replaced
by emergency towers within 32 hours with winds exceeding 160
km/h, and later by permanent structures.  
    In the early 1970s insulators began to fail due to
cracking.   Rapid wear on North Island insulator fittings was
also being caused by the high number of windy days.   By 1974
insulator cracking on the North Island transmission line
section 's positive pole  photo  
  caption  
had reached such a level that it had to be fitted with
redesigned insulators.   The work was carried out during the
1976 annual maintenance shutdown.  
    During the 1980s insulator cracking gradually
increased.   However, plans to reinsulate more of the line
were overtaken by the need to reinsulate the whole line because
of insulator porosity problems and the need to cope with the
increased operating voltage from 250 kV to 350 kV in readiness
for the upgraded hybrid link.  
    The cracking problems arose as a combination of the
relative newness of contemporary HVDC insulator technology and
the severity of the coastal marine environment.   Being
unique in these respects, they attracted world-wide attention.
  Three different causes of cracking were identified over
many years.  
    The first cause, identified in the early 1970s, was
traced to the insulator pin 's zinc sacrificial collar which
provided a shield against rapid electrolytic corrosion from DC
leakage currents.   But instead of pure zinc, the collar was
found to also contain a small amount of aluminium to help in
its casting.   The alloy 's intergranular corrosion caused the
collar to expand and apply a bursting pressure to the brittle
porcelain "rain-shed" with subsequent radial cracking.
  Collars were henceforth made from pure zinc.  
    In 1982, however, the new insulators were also found to
be cracking.   This second cause was found to be rusting of
the embedded section of the insulator pin below the sacrificial
collar with the resulting expansion force cracking the
porcelain.   The rusting itself was caused by very high
leakage currents on the line 's coastal section bypassing the
sacrificial collar and being driven deep into the pin cement.
  The best solution to this problem proved to be covering the
pin 's entire embedded length with an insulating epoxy
coating.  
    The third cause appeared in 1986 when a string of
DC-type insulators on a North Island AC line exploded.   The
insulators were found to have severe porosity which resulted in
electrical puncturing and subsequent mechanical separation by
the blast from a flashover to the tower.   The maker (NGK of
Japan) was adamant that the porosity was not a manufacturing
flaw and must have developed while the insulators were in
service, and as such was a completely new phenomenon.
  Urgent investigations found the DC line 's insulators to be
similarly affected along its whole length, the most damaged
being those subject to the greatest mechanical stress either
from line loads or stresses by pin/collar expansion.   Later
studies showed the porosity growth phenomenon to be limited to
conventional porcelain&semi; the improved alumina porcelain used by
NGK since about 1970 was not susceptible.  
    New insulators purchased for the line 's reinsulation
represent state-of-the-art HVDC insulator design.   As well
as being of improved electrical performance, they are also
proofed against the three known failures with pure zinc
collars, epoxy-coated pins and alumina porcelain and are also
fitted with sacrificial collars around the caps.  
      OTERANGA BAY CABLE TERMINAL STATION    
    Prevailing southerly conditions sweeping in from Cook
Strait bring salt-laden moisture which often forms heavy salt
deposits on insulators at Oteranga Bay and the immediately
adjacent transmission line.  
    Although some insulator-washing equipment was installed
at Oteranga Bay as part of the initial installation, during the
earlier years flashovers caused by saturated salt resulted
in automatic line-protection-initiated outages on one or both
poles.   Maintenance staff were then called urgently to wash
the insulators with equipment especially designed for the
purpose.  
    Over the years various anti-salt methods were tried
including water repellant substances such as silicone grease.
  None proved very successful.   Permanently installed
water spraying equipment of an improved type, and activated
automatically from current leakage monitors, have since
proved satisfactory for most situations.  
    Salt contamination caused other problems to Oteranga
Bay 's outdoor switchyard equipment, particularly to airbreak
switches whose many small components (springs, pivots etc)
corroded rapidly and caused a loss of contact pressure.   In
spite of regular maintenance, on several occasions this led to
contact overheating and consequent complete burn-out of the
switches.   The switches eventually became quite unreliable
and were replaced with others of a more suitable design.  
    Salt also attacked the aluminium alloy operating arms
of some of the switches which resulted in the metallurgical
phenomenon of "exfoliation".   The trouble was eliminated
with the substitution of arms using an alloy of different
composition.  
    The line 's South Island section has suffered only about
5 percent of the North 's salt-pollution problems.  
      COMMUNICATIONS EQUIPMENT    
    Although the original communications system gave
satisfactory service for over     2O    20    
years, it was replaced in 1988 after the radio valve equipment
became difficult to maintain because of wiring deterioration
and lack of spare parts.   It also suffered from an
unsatisfactory noise performance&semi; mains power was required for
its operation&semi; it had insufficient capacity&semi; and there was
a lack of flexibility when a submarine cable was taken out
of service.  
    The system was changed from two systems in tandem to
three systems with the installation of terminal equipment at
Oteranga Bay.   The change was necessary because of large
losses occurring with the mismatch in impedance between the
transmission line and submarine cable sections.  
    The new system was designed and installed by
corporation staff using power line carrier equipment from Spain
and manufactured by Dimat.   The system considerably
reduced the noise level which by modern communications
standards had become unacceptable.   This was achieved
despite the doubling of the voice-frequency channels to four
and the lowering of the transmitter power to 80 W.   The
control signalling equipment was also upgraded at the same
time, and provision made for collecting data from both terminal
stations.   The new equipment is provided with a 24 V
communications battery in case local power fails.  
      SUBMARINE CABLES    
    By international standards, the   cables'   overall
performance has been very good.   But lying in a harsh
environment and having been laid with relatively elementary
techniques compared with those available today, it was
inevitable that in places the cables would be suspended across
rocks.   At the Oteranga Bay ends of the cables,
bend-restricting armour provides some protection from this.  
    It was eight years before the first electrical fault
occurred.   When in May 1973 a fault did occur, on cable 1 at
the Fighting Bay shore joint, severe burning of the paper
insulation made it impossible to determine the cause.
  Consequently, the cable was cut back on the seaward side to
sufficiently clear the area affected by the entrance of water,
a new joint made and the cable returned to service in September
1973.  
    A similar fault developed in June 1976 on cable 3.
  Severe burning of the paper insulation again made it
impossible to determine the cause.   One possible reason was
the conductors pulling apart from the soldered joining ferrule.
  A joint-brazing technique was developed (and used also in
subsequent repairs) and the cable returned to service in August
1976.  
    A few days later a greatly more serious fault developed
on cable 1, this time in 120 m of water some 15.5 km from
Fighting Bay.   The fault was in the rigid repair joint which
had been made after the cable had "kinked" during its original
laying in 1965.  
      Photinia   was recalled from her North Atlantic
voyaging in October and spent the next five months at a
shipyard while her cable-laying gear was refitted.   After
sea trials in Scotland, using a 700 m length of scrap cable
similar to the Cook Strait cables, she left for New Zealand in
March the following year and arrived six weeks later.   The
sea trials were repeated in Cloudy Bay (near Fighting Bay).
  Then, having loaded 2000 m of cable from the spare stock
kept in Wellington,   Photinia   was moored above the
fault to a complication of anchors and buoys laid by her
support vessel   Lady Vera  , also from Britain and
specially designed to handle oil rig moorings.  
    The repair task began on 13 June.   It was a
difficult job.   On one occasion, for instance, part of the
mooring set-up failed during a storm and the entire system had
to be brought back to Wellington for modifications.  
    The repair process used was that originally attempted
during the cable-laying repair in 1965.   The work, briefly,
involved lifting the cable 10 m above the seabed on the
Oteranga Bay side in the vicinity of the fault and cutting it
with an explosive grapnel from   Lady Vera  .   The
Fighting Bay station continued to feed gas to prevent as much
water as possible from entering the cable, while the Oteranga
Bay end was lifted aboard   Photinia  , the damaged
length cut out, the cable-end capped and laid back on the
seabed.   The Fighting Bay end was then lifted on board, cut
back to good cable,joined to a new length in the ship 's hold
and returned to the seabed as the ship moved towards Oteranga
Bay.   That end was then lifted on board and attached to the
new cable by a "loop" joint.   The loop was returned to the
seabed.   Each of the joints was contained within a rigid
steel frame to both assist in re-laying and as means of
protection on the seabed.  
    By good fortune, the fault had occurred while the link
was being restored to service after a period during which it
had been out of service.   There was therefore no current
flow to burn the insulation, so the joint itself was not
seriously damaged to the extent where evidence of the cause was
obliterated.   The conductors were found to have pulled apart
from the sweated joining ferrule and there was clear evidence
of electrical breakdown of the paper insulation.  
    The repaired cable was finally returned to service on
12 August 1977 and, after discharging surplus gear,
  Photinia   sailed for Britain a week later to resume
her usual trans-Atlantic voyaging.  
    The writer experienced some of the vicissitudes of the
job during a visit to the   Photinia   for several days
while she lay in Cook Strait.   For part of the time the 
ship was buffetted by gale-force winds.   Tethered securely
to her moorings, she responded  photos  
  captions  
only with considerable difficulty to the seas which crashed
against her in sheets of flying spray.   Her mooring wires,
strained bar-tight, continuously "sang", or vibrated, in
high-pitched resonance until one snapped with such force that
the ship recoiled visibly at the sudden release from one of its
restraints.  
      The return to Wellington involved climbing down a
rope ladder into a lifeboat to be carried to   Lady
Vera  , lying several kilometres off, for the rest of the
trip to Wellington.   Although the weather had by then
moderated to a degree, the lifeboat nevertheless rose and fell
some considerable distance at   Photinia'  s side and it
was necessary to judge the moment carefully before jumping from
the ladder.   The journey to   Lady Vera   was long,
wet and uncomfortable, the seas still high enough for both
  Photinia   and   Lady Vera   to be completely
lost to sight in the troughs.   Transferring to   Lady
Vera   also required agility and fine judgement to jump to
her after-deck, which was being continually washed fore-and-aft
by breaking seas.   It was as well the writer was a former
merchant seaman!  
    Unhappily, in 1978   Photinia   was driven ashore
in Lake Michigan by 100 km/h winds and became a total loss.
  However, her crew were rescued, three at a time, by a Coast
Guard helicopter.   Lost with her, but fortunately later
recovered, was a bronze plaque presented by the New Zealand
Electricity Department in recognition of her service in
originally laying the cables and in the 1977 Cook Strait
repair.  
    In July 1980 cable 3 failed again at the Fighting Bay
shore joint although no damage similar to previous shore joint
faults was found.   The fault was repaired with the
conductors being brazed together as in the 1977 submarine
repair.  
    During the repair the cable 's Oteranga Bay shore joint
was stripped down and evidence found of differential movement
between the conductor and the cable insulation, with
significant distortion of the paper insulation.   The joint
was replaced and other joints were later found to have similar
distortions.   It was concluded, therefore, that such
differential movement may have been a significant factor in
previous insulation failures.  
  caption    photo  
    The first problem associated with the suspension of
cables over rocks occurred in October 1981 during the
re-gassing of cable 1 when, following work on an Oteranga Bay
shore joint, gas leaked from under the rubber sheath
surrounding the bend-restricting armour adjacent to the shore
joint.   This indicated a defect in the cable 's lead sheath
somewhere in the specially reinforced bend-restricting armour
section of the cable out in Cook Strait.   The problem was to
pinpoint its exact location.  
    With assistance from the nuclear sciences section of
the Department of Scientific and Industrial Research
(DSIR), a new method of locating gas leaks was developed,
followed by considerable research and development to perfect
formulae for interpreting the resulting information.   The
new method proved remarkably accurate.   With radioactivity
detectors fixed at various positions on the cable and the
injection of krypton 85 as a tracer gas, the defect was found
to be 1746 m from the shore at one end of a 26 m span where the
cable was suspended over rocks.  
    In the summer of 1982/83 the 3000t, shallow draft
cable-laying British ship   Luminence   was used to lift
and repair the cable.   The ship had carried out a similar
repair in Scotland earlier in 1982 and recently had completed
the Orkney connection with the mainland which involved
crossing the 36 km Pentland Firth, a stretch of sea similar to
Cook Strait.   Her double-sheathed hull also provided an
added safeguard while working close to rock outcrops.  
    The job itself was entrusted to the submarine cable
unit of Balfour Kirkpatrick Ltd, part of the company which had
manufactured and laid the original cables.   The work was
difficult and slow.   The cable was cut 900 m from shore and
recovered out to 2000 m with the first 900 m being pulled
ashore.   A new length was attached by way of a rigid joint
and laid back to shore.  
    During the repair process a retired Australian
metallurgist specialising in lead, Dr R C Gifkin, made an
intensive investigation into the exact reason for the lead
sheath 's failure in order to determine possible ways to avoid
such failures on the other cables.   Dr Gifkin found the
cause to be a combination of lead sheath fatigue and creep
failure where the cable touched down at the end of a suspension
point lying over rocks.  
    Diving surveys established where the cables were
suspended for more than 10 m.   Many suspensions were later
stabilised on cables 2 and 3 with mid-span concrete pyramids to
reduce cable sway during tidal current changes.   Concrete
"pillows" were also inserted under the cables where they first
touched down on rock outcrops to shift the point of suspension
to a fresh part of the cable.   Repairs were completed in
early March 1983.  
    During re-gassing a further gas leak occurred in cable
1 's lead sheath.   Radioactive tracer gas measurements
again indicated a lead sheath defect about 3210 m from Oteranga
Bay, beyond the previous repair and in the deeper and more
turbulent waters of the Terawhiti Rip.   However, repairs
at such a point would have needed a ship other than
  Luminence   and would have been substantially more
expensive.   Therefore it was decided to use cable 1 only in
an emergency in future and at half-capacity to reduce the
electrical stress and prolong its useful life as long as
possible.  
    The last major fault was in May 1988 when cable 2 's
sealing end exploded at Oteranga Bay and spilled many litres of
insulating oil in the switchyard.   This was the first
recorded failure anywhere on this type of cable
termination.  
    With cable 1 rated for only 50 percent loading as the
result of its previous failure, a repair force was urgently
assembled to restore the fault and maintain the link 's full
operation.   Skilled jointers were brought from Hong Kong and
Britain and some 90 major activities were undertaken to provide
the resources and facilities for the repair to be made in the
correct environment&semi; for instance, a 15 m high wood-clad
scaffold house was built to weatherproof the jointing area.  
    The repair was completed in two-and-a-half months, on
time and within budget.   Fortunately, half the repair period
was accompanied by unusually low water inflows in the South
Island so that the link 's full capacity was not
required.    31      
    Despite its various problems, however, the link has
been maintained on the basis of a 95 percent or better annual
availability, with close to 100 percent for about six months of
each year.   Except for the first few   years'  
operation and one or two later years when there were major line
or cable faults, this high availability has been consistently
achieved.  
    Importantly, the spare (third) cable has largely
enabled the link to continue at full capacity during cable
faults.   It was laid because of New Zealand 's remoteness,
the unavailability of repair vessels locally or at short notice
overseas.   Cook Strait 's variable weather was an important
factor, too, potentially adding further lengthy repair delays.
  The decision proved a wise one and has repaid its added
cost many times over during the past 25 years.      
  

        Coup on office floor: PCs set to take
over    
  blurb with photo  
      I  n 1975, Microsoft chief executive Bill Gates
made a prophecy - some day there would be a computer on every desk.
  This was inconceivable to most observers then - and an
interesting prophecy from someone not directly involved in
hardware marketing.  
    Today, that vision is clearly in sight.  
    Over the past 10 years, computer technology has changed
the way the world does business.   Over the next 10, that pace
will accelerate.   By 1999, it is estimated virtually every
office worker in America will use a desktop computer.   Their New
Zealand counterparts won't be far behind.  
    Office professionals have come to depend upon personal
computers for everything from word processing, to record
keeping, to data management.   The number of computer users
has grown year by year as hardware and software technology improves
and costs decrease.  
    Personal computer hardware has become more powerful with
the incredible advancement in chip technology.   Over the
past 15 years microprocessor-chip speed and memory-chip
capacity has more than doubled every 3 years.   This rate will
continue for at least 10 more years.   Today 's latest
processor chips, like the Intel 80386 and Motorola 68030,
operate at more than 10 times the processor speed in the original
MS-Dos personal computers.  
    On the software side, the Microsoft OS/2, for example, is
designed to be much more than just another version of MS-Dos.
  It extends the capabilities of the new hardware by
expanding ways people interact with their computers and how they
work together in group settings.  
    Let 's look at what some of these capabilities will mean
to an average user of the 1990s.  
    Imagine a salesperson preparing a report on a word
processor.   While working, this person refers frequently to a
variety of business and encyclopaedic sources stored on a
CD-Rom disc, inserting data and graphics from them into the
document.   A request from the boss for an up-to-the-minute
analysis of regional sales is received on electronic mail.
  The salesperson immediately calls up a preformatted spreadsheet
file and attaches it to a centralised database from a network.
  The spreadsheet automatically queries the database, brings up
the pertinent figures, and makes the necessary calculations.  
    Our salesperson can chart the numbers generated by the
spreadsheet, bring the chart with accompanying figures into the
word processor, and create a memo to send to the boss via
electronic mail.   The report can be distributed to other members
of the department on the network, so that co-workers can attach
comments and, if appropriate, make modifications.   Finally, a
publication-quality document can be printed on a shared
high-resolution printer.  
    In short, the salesperson will be able to collect
information more quickly, analyse it more accurately, and then
present it in a way that communicates more effectively.  
    This scenario illustrates three of the most important ways
  1990s'   technology will make computing more accessible and
useful to a broader range of people: a standard graphical user
interface, true multitasking, and data exchange and
integration.  
      Graphical user interface    
    The introductions of the Apple Macintosh in 1984,
Microsoft Windows in 1985 and OS/2 Presentation Manager in 1988
have forever changed the face of personal computing.
  Instead of requiring customers to learn different (often
convoluted) sets of commands for individual programs, these
graphically-oriented environments give people a consistent,
visually-oriented way to interact with computers.   The
graphical interface reduces command numbers in an application
and allows users to see immediately on screen what their documents
or spreadsheets will look like when printed.  
    The benefits of this technology are so overwhelming that I
predict within three years, more than 80% of desktop computer users
will be using a system with a graphical user interface.  
    With a graphically-oriented application, users can perform
functions that are difficult or impossible to do with
character-based applications.  
    Some of these are merging text and graphics on the
screen and in documents&semi; shrinking full pages of text to get a
sense of overall layout&semi; and using sophisticated formatting tools
that allow the creation of quality output.  
    What 's more, Windows and Presentation Manager define a
single consistent set of interface guide-lines that all
applications will share.  
    IBM has made these guidelines a part of its overall System
Application Architecture (SAA).   So more people will be able to
use more and different kinds of applications - once they know one
application, they 'll be well on their way to learning others.  
    Here at Brimaur Microsoft we receive less than half as many
support calls about these consistent graphical applications than
about character-based applications.  
    The ability to learn applications quickly and intuitively
translates into reduced training costs for corporations.  
    Although this new user interface is a change from the
inconsistent interfaces of today 's non-graphical applications, it
represents an inevitable transition.   The sooner companies
switch their users, the more time and money they will save in the
long run.  
      Multitasking    
    As office workers begin to use more applications,
they 'll discover many ways they can benefit by using those
applications together.   In the salesperson scenario above,
for example, electronic mail was running simultaneously with word
processing&semi; the database and the spreadsheet were exchanging
information&semi; and the salesperson was creating a report that
combined charts, data, and text from multiple sources.  
    Until now, the only way to bring together all these
different types of information was with an all-in-one program such
as Microsoft Works, Lotus Symphony, or Ashton-Tate Framework.
  But corporate users recognise that, as powerful as the
integration feature may be, the individual modules within those
programs fall short of what a dedicated word processor,
spreadsheet or database is capable of.  
    The model for the 1990s is integration, not at the program
level but at the system level, through the OS/2 Presentation
Manager and its consistent user interface.   Thanks to OS/2,
users can select products that fully meet their needs and then run
them simultaneously.   Because Presentation Manager
establishes a standard foundation for all applications,
people can work with applications from different vendors, moving
effortlessly from one to another.  
    One other key feature of multitasking in the 1990s is
applications will be able to share the microprocessor
simultaneously.   Which means the user doesn't have to quit one
application to run another.   Electronic mail, for example, can
operate in the background while the user works on a different
application in the foreground, alerting the user whenever mail
arrives.   Similarly, the computer can invisibly perform a
complicated spreadsheet calculation or database sort behind the
scenes while you 're writing a report on a word processor.    
      Data exchange, integration    
    Under MS-Dos without Windows, sharing data between
applications has usually been limited to the exchange of text-only
files carrying data without formatting information.   What 's
more, the way data is exchanged can change drastically from one
combination of applications to another.  
    When users work with a Macintosh system, Microsoft
Windows, or OS/2 Presentation Manager, they can move data from
one application to another by using a common  -  sense "cut and
paste" technique.   For example, they can now prepare a
graphic image in one application and then move it to another -
such as their company logo on to an electronic form used for
database entry.  
    A normal clipboard requires cut-and-paste commands be given
every time data has to be updated.   In Windows and Presentation
Manager, a special kind of paste command called Pastelink is
supported, which uses Dynamic Data Exchange (DDE) protocol to
establish permanent links between applications.   Links can
be established between applications so when any file is updated,
the information is automatically updated throughout the system.
  These links can be used, for example, to put sales data from a
spreadsheet file into a monthly report in a word processing
file, or to retrieve real-time stock quotations over the phone from
a remote source.   Because the data exchange happens at a high
level, with all formatting intact, even highly complex documents
can be constructed quickly and easily.  
      Workplace benefits    
    Behind all three of these features - graphical user
interface, multitasking, and data exchange and integration - lies
a broader vision about how people will be using computers in
the workplace of the future.  
    For example, a team collaborating on a project such as a
contract or a report will benefit greatly.   One person might
draft a document, another annotate it, and a third edit it - all
on-line.   Comments from people all over the company can be
compiled, with revisions automatically marked.   The group can
store a "library" of documents and graphics, and any member of
the group can pull in graphics or charts.  
    Along the way, the group can track who has reviewed the
document, and can even prevent some reviewers from making
particular types of changes.  
    All this can happen without those people needing to know
whether they 're accessing a mainframe, a minicomputer, or a
database residing on an independent file server.  
    Applications running on the personal computer using the
standard graphical interface will translate requests for
information into machine communications standards - particularly
SQL (Structured Query Language) - so networks using advanced
servers like those based on OS/2 departmental servers can
transparently fetch the information.  
    The goal of networking is simply to combine personal
computing benefits (rapid response, simple interface, a wide range
of software) with large-scale computing benefits (corporate
data, reliability, security) by connecting them.  
    A first-generation approach is to allow the personal
computer to be simply a terminal for running large-machine
applications.   This avoids the need for two displays, but it
doesn't simplify the user interface or allow data to migrate on to
departmental and personal machines.  
    The second-generation approach now being implemented with
OS/2 servers, and sometimes called "co-operative processing" or
"client-server architecture", allows the interface portion of
an application to run on the personal computer even though the data
is stored on another machine.  
    These features also open doors for new classes of
software.  
    New-generation programs will accommodate compound
document processing (merging text, graphics, data, and
ultimately, sound).   This will enable, routinely, production of
annual-report-quality output, and will be highly customisable
through macros.   There will be transparent access to many
different information-packed databases - department or company-wide
- without the need to know or care where the database resides.
  It will be easy to integrate information stored on
information-packed CD-Rom discs, as our salesperson did in the
scenario from the 1990s.  
    A new class of "mission-critical" programs will appear,
written internally by companies for their specialised business
needs and working together with standard applications.
  Examples include transaction processing, financial trading,
and resource scheduling.   Many of these applications will be
well-served by operating systems that support multitasking and
transparent data exchange.  
    Right now, running applications that work together is a key
feature of OS/2 and the next logical step to the 1990s.   But
soon, users will see greater movement toward "families" of
applications.   We 'll see the emergence of on-line sales
training, on-line budgets, regular use of electronic mail, word
processors that easily incorporate graphics, and much more.
  Some forward-looking New Zealand corporates will be regularly
using some of these "new look" packages by the end of the year.  
    The information age has just begun where the way a company
deals with data - making it available easily and quickly to its
employees - will be a key factor in competition.  
    This new "architecture of information" ushered in by the
personal computer revolution represents a major challenge and
opportunity for all companies.    
        DTP takes on colourful hue    
    By Eve Sinton  
      A  pplications as diverse as high-class
magazine production and giving notice of a bomb-threat, in the form
of tattered newspaper headlines pasted onto a crumpled page, call
on desktop publishing technology.   What distinguishes the glossy
magazine from the terrorist 's message is the subtle combination of
graphic design skills and appropriate computer
equipment.  
    Desktop publishing concerns the enhancement of written
communications with page design, layout and graphics, added to text
in a computer environment.   Many word processing programs
include desktop publishing capabilities such as multi-column text,
font manipulation and line drawing features, but desktop publishing
software takes the process further, placing typography and
graphic design tools within reach of anyone with access to a
personal computer.   People are the vital component&semi; without them
screens, keyboards, and clever packages achieve nothing.  
    The term desktop publishing (DTP) was coined by Apple
Computer, whose appealingly Wysiwyg Mac led its evolution, but
it is now a vague, umbrella label for a spectrum of electronic
publishing which includes many types of usage.   The technology
can be viewed on three broad levels: professional electronic
publishing, as used by newspaper, magazine and book publishers,
and advertising agencies&semi; in-house corporate communications
including legal documents, contracts, newsletters, forms, brochures
and advertisements&semi; and personal document presentation where
reports, submissions, notices and posters are enhanced with layout
tools.  
    The proliferation of PCs and page make  -  up programs
has caused alarm among professional typesetters.   Initially,
there were fears of losing business and resentment among staff as
unfamiliar computer technology was introduced into their work.
  However, the concern among professionals today is that people
who are ignorant of design skills produce an avalanche of ugly,
gimmicky documents which lack readability.  
    Although small scale desktop publishing bureaus and
in-house facilities have taken business from the professionals,
there is a trend for some of that work to come back, as customers
find amateur document production fails to meet their
requirements.   Alan Morton, managing director of Jacobsons
graphics communications in Parnell, says:     "The major
benefit of DTP for Jacobsons is we now have a greater number of
clients who have access to the technology and can appreciate the
talent and skills of our people."       Morton is
critical of the way desktop publishing has been presented as
  "boxes of promises"   by vendors, but believes
new users who rushed out to buy publishing systems are now coming
to realise the designer and typographer have been undervalued.
  He points out a computer alone will never make a designer out
of a novice.  
    The quality of low-end promotion and advertising has been
much improved by the use of DTP and enabled small-scale users to
make savings on the cost of typography and design.   The hidden
cost includes lack of impact and a failure to generate the intended
response to a promotion.     "In the growing clutter of
advertising material today 'getting noticed' is critical, and
quality always gets noticed,"   Morton says.  
    Apple Computer 's value-added retailer channels manager,
John Halliwell, says there has been a strong trend for corporations
to form in-house communications divisions.   For example, Fay
Richwhite uses Digital equipment for its banking functions and
Macintosh PCs for all in-house document presentation.
  The company has found, when dealing with Japanese
interests, it is useful to have a document processing team in the
room so agreements can be drafted, completed and signed at the
end of the negotiating session.   This procedure is favoured
by Japanese business people.  
    Computer technology was employed by some large newspaper
publishers for years before the advent of DTP, and large-scale
electronic publishing today includes integrated systems which
cover everything from display advertising and editorial layout to
classified columns, accounting, and circulation functions.
  On this scale the most appropriate selection of equipment
is critical, and there are pitfalls for companies taking advice
from computer sales people who have scant knowledge of the
publishing process.  
    According to Peter Harris, managing director of
Imagetext Publishing Systems in Auckland, the country is riddled
with people who claim to have all the answers.     "The
market has been saturated with salespeople touting fifth-grade
solutions,"   he says, and points out the recent collapse of
some PC dealers has left customers struggling without support
for the systems sold to them.  
    Harris says the benefits of integrated publishing systems
for large-scale concerns come from working smarter rather than
attempting to make savings on equipment.   Good management of
newsprint stocks, the extension of advertising deadlines, and
prompt issue of accounts all enhance revenue prospects.   The
pitfalls of DTP, he says, are exposed when users attempt to be
too clever, sacrificing comprehension and readability for pretty
shapes.  
    Professional publishing systems, while integrating
Macintosh and IBM-compatible PCs, are likely to include dedicated
workstations such as Bedford Meteor and Sun.   Each system
has strengths, and Jacobsons has found much advantage in networking
its entire range of Macs, Dos and Bedford machines so customers can
have the appropriate technology applied to different phases of
their work.   The company 's operations manager assesses each
job and directs it to the best machine.   Some processes are
still quicker if done by traditional manual techniques, so staff
are encouraged to be versatile and mix old skills with new to the
customer 's best advantage.  
    There is no disputing Apple Computer initiated DTP in its
present form, and can justifiably claim to be the industry
standard, but IBM compatibles are rapidly catching up.
  ComputerTime business consultant Craig Betts says with
Pagemaker, ScanJet Plus, Scan Gallery, and Omnipage OCR software
all running under a Windows environment, the Dos solution is on a
par with Apple Macintosh, although he concedes   "it 's still
not quite as tidy as Apple"  .   However, the cost is
significantly lower.  
    Apple 's John Halliwell attended the latest Seybold DTP
conference where he felt IBM was showing little direction as to
which of its three platforms - Windows, OS/2, or MS-Dos - it would
support in desktop publishing.     "90% of stands at
Seybold had Macs,"   Halliwell claims,   "and now
Ventura has been ported to the Mac environment we don't feel any
threat from that quarter."    
    The developers of the Next computer, according to
Halliwell, are still 18 months from having a commercial DTP
product.   When it arrives it could be a serious contender in
the field.  
    While some vendors feel the market may be approaching
saturation, Apple perceives large newspapers with annual budgets of
$50 million or more represent big sales potential.  
    Software distributors certainly anticipate continuing
demand for their products, according to Kate Francis, managing
director of Icon Software, distributors of Xerox Ventura.
    "We have seen incredible demand over the last three
months,"   she says.   Ventura has the capability to
handle very large documents, and its add-on - The Professional
Extension - provides features such as vertical justification, table
editing and scientific equations.   It also has a network server
facility.   Ventura has been purchased by large organisations
including Parliament and universities.  
    Francis emphasises the importance of good support and
training in software sales, and the necessity of good design
skills.     "Some companies expect typists to create lovely
documents with desktop publishing when they should bring in
specialists,"   she says.  
    An early convert to DTP technology for small-town newspaper
production was the   Waitomo News  , based in Te Kuiti.
  Two-and-a-half years ago the company installed a Macintosh SE
file server with two SE terminals and four Mac Plus machines.
  Its branch office in the neighbouring town of Otorohanga has an
SE linked to Te Kuiti by modem.   Software included Pagemaker,
Word and Freehand.  
      The   Waitomo News   experienced problems
when two staff who received training in the use of the system both
left six months later, and management was reluctant to pay to train
replacements.   Although the paper receives good telephone
support from its Auckland supplier, according to typesetter Agnes
Paton, it is not easy to get support people to drive down for
on-the-spot assistance.  
    The paper has upgraded its printer to a LaserWriter NTX,
replacing a slower model,   photo    caption   but the
system has never achieved the hoped-for elimination of paste-up.
  The   Waitomo News   had wanted to produce a full page
ready for print, comprising four A4 sheets, but still has to use
manual paste-up to complete the job.  
    A happier story emerges from Dene O'Brien, New Zealand
marketing manager of Foodtown.   Twelve months ago the
company bought a computer graphics system consisting of a
Macintosh IIcx, 21in high-resolution screen and LaserWriter IINTX.
  Pride of the system is a Truvel scanner, which can scan images
from actual products, such as a can of beans placed on its
flat-bed surface.   O'Brien 's staff think the system is
fantastic.  
    Foodtown uses AdMaker software to create press
advertisements, tabloids, and catalogues.   The whole process
is completed to pre-print stage without manual paste-up or layout
boards.   "    It has revolutionised the way we look at
graphics, and the staff feel they have been released from drudgery.
  The system brings out a lot of talent.     Even though
people were at first apprehensive at the introduction of the
technology, they would never go back to a conventional
system,"   he says.   O'Brien 's only regret is they didn't
opt for full colour capability.  
    Choosing the most appropriate system for a particular
function is a complex task, given the increasing number of options.
  Craig Betts of ComputerTime was asked to give an example of a
good Dos environment system for an in-house business
communications department.   He suggested a 386 computer
such as the 25MHz Hewlett-Packard RS25C, or an SX based machine.
  To this he would add a ScanJet Plus scanner and a
Hewlett-Packard 2D laser printer with a range of scalable fonts
under Type Director.   Software would include ScanGallery,
OmniPage, and Pagemaker running under Windows.  
    Imagetext recently designed an electronic publishing system
for the   Nelson Evening Mail   which included two Macintosh
IIcx 's, a Translator II to the Mail 's existing One System, a Truvel
TZ-3 greyscale Scanner, a PC running Master Planner software, a
Dash 30 52MHz 68030 fileserver and Varitype VT600W plain paper
imagesetter.   Software included Multi Ad Creator, AdWriter,
MacWrite, Desk Paint/Draw, Adobe Illustrator, and Tops Network.
  An ethernet network with Local Talk and SCSI interface was
designed to link the system together.   This represents a total
investment of over $200,000, including installation, on-site
training, and on-going support.  
    Trevor Gray of Renaissance, distributors of Aldus Pagemaker
for Dos and Macintosh systems, expects hardware prices will
continue to drop but software will not, as more improvements
and features are developed for existing applications.   While he
perceives prices as a barrier to small business customers, he
thinks cheaper hardware will encourage more purchasers and this
will flow on to maintain a demand for software.   He
    see    sees     the increasing use of colour, with
links to high  -  end colour separation machines, as the main
trend for DTP in the future.  
    The quality and price of output devices have been critical
in the evolution of DTP, and range from the Apple PostScript 300
dpi LaserWriter through to highly expensive Linotronics and
Tabloid Laser typesetters.   This has been accompanied by the
use of large, high-resolution screens.   Now input devices have
been boosted by the addition of scanners and optical character
recognition software.  
    Colour screens have been in use for some time, and now are
being joined by colour scanners and more affordable colour
printers.   The accurate definition and manipulation of
colour from on-screen editing through proof-printing to the final
product offers savings on time and frustration.  
    Paul Newport, joint managing director of Visual Solutions
in Auckland, also predicts colour as the major new thrust of
DTP through the 1990s, along with open platforms.   Open
systems can already be seen in developments such as Apple 's FDHD
drive which recognises either Dos or Macintosh format 3.5in disks
and can translate files between the two formats.  
    Many applications which run in both environments, like
Pagemaker and Microsoft Word, are able to read files of either
type, complete with formatting.  
    Desktop publishing will develop new twists with the advent
of multi-media and compact disk technology.   Reading will become
an experience mixing on-screen video images, with sound and text -
cost is the only stumbling block.   But costs inevitably come
down.   Will creative skills and tasteful production keep pace
with new technological development?      
  

      Magellan Mace Laplink III System
Sleuth    
  by Doug Casement
      R  emember the days before hard drives
became commonplace and 360Kb was a lot of storage?   The
biggest problem then was keeping floppies accurately labelled,
so data files could be found as quickly as possible.  
    Nowadays, not having a hard drive is the exception,
not the rule.   Indeed, lack of a hard drive can disqualify
you from many software packages, which need megabytes of space
to reside in.   However, while the space offered by hard
drive storage is great, keeping track of all the files stored
can be a hassle - you can get an awful lot of letters on
40Mb!  
    As ever, the software industry was not slow to respond
to a perceived demand and file management utilities have been
appearing regularly in recent years.   Such programs range
from public domain offerings with limited functionality
through to high end commercial packages such as PC Tools and
Xtree.   The latest contender in this market is Lotus
Magellan, described by Lotus as an
  "easy-to-use  photo  
data management system."    
    I wouldn't disagree with that description - Magellan
is easy to use from the moment it is installed.   Of course,
taking advantage of all of its features takes a little
practice but the essential functions are mastered in a matter
of minutes - rather than hours.  
    Apart from the software design itself, the other
factor that makes Magellan so easy to use is the superb
documentation.   In addition to the ring bound reference
manual (entitled Explorers Guide) there are two booklets -
Quick Launch and Ideas.  
    The 22 page Quick Launch booklet covers installation
and the basic operations offered by Magellan.   It is
augmented by the software itself - in addition to the
Magellan program, there are two sub-directories.   One
contains a demonstration program illustrating major features.
  The other holds practice files to avoid accidents early in
the learning curve.   There is also a nine page ReadMe file
with updated information and handy tips.  
    Installation is straightforward - the usual insert
floppy, select drive and type Install routine.   At the end
of installation, Magellan asks if you want to index your files
now or later and warns this may take some time.   It isn't
kidding - it took 35 odd minutes to index a 20Mb drive holding
approximately 17Mb of applications and data.   (It was
an elderly XT - faster machines obviously index a lot more
quickly, with 20 minutes for a 286 with 40Mb drive being an
average.)  
    However, complete indexing only has to be done once
and subsequent updates take only a few minutes, unless of
course, the hard drive data has been almost completely altered
- accidental formats do happen...  
    The other trap for young players, is that while the
index itself takes up only five to seven percent of the disk
space occupied by the files, it requires lots more room for a
temporary file while indexing.   I tried updating the index
for a 20Mb drive holding 19.5Mb of data - Magellan informed me
that there was insufficient space for the temporary index
file.   (Floppy disks can also be indexed, allowing Magellan
to perform its magic on files stored on externally.)  
    Unless there is a compelling reason not to, it makes
sense to index the files immediately - then you can get on
with the fun part - using Magellan.  
    Select the directory where Magellan is installed -
usually Magellan if the default offered by Install has been
chosen.   Type MG and Magellan loads, presenting a screen
that may well become home base for many users.  
    The top line of the screen is the Status line, which
displays program name, mode and other current information
and instructions, while the second line shows the Explore
Path.  
    Below the top two lines the screen is split
vertically, with roughly one quarter to the left of the line
and the  photo  
balance to the right.   The left hand window is the List
Window and the right, the View Window.   Underneath the
windows is an information line and below that, a Function Key
Map,  photo  
showing F key numbers and functions.  
    The basic operations of Magellan are controlled from
the 10 function keys, each of which has a dual role via the
ALT key.   F1, surprise, surprise, is the Help key - and
very good it is too!   The others are Copy, Delete, Print,
Gather, Sort, Launch, Zoom, Explore and Quit.   Hold down
the ALT key and these become Compose, Move, Mark, Rename,
Index, Tree, Macro, Options, Path and DOS.  
    In addition to the function keys, a key letter menu
can be activated by typing either the / or &gt; keys.   In the
various dialogue boxes, where Magellan asks for
information and/or instructions, you can move around using
either the cursor keys or by entering the command key letter.
  All of which makes movement fast and easy.  
    The Explorers Guide suggests that there are five
categories of activity when Magellan could be used.   These
are: viewing file contents, searching for information within
files, organising files and directories, customising
Magellan and with other programs.  
    One of Magellan 's strengths is its ability to let you
view files without opening them, so it becomes an easy matter
to scan through files and decide whether to keep them or trash
them.   Magellan has custom viewers for 1-2-3, Manuscript,
Agenda, Symphony, Word, WordPerfect, Wordstar, Multimate,
dBASE III, Displaywrite, XYWrite and ASCII files, so all
formatting commands are retained when the files are
viewed.   There is also a generic viewer for those files
that were created using programs for which a custom viewer is
not available.  
    Files can be sorted for viewing by: filename,
extension, path, size, time/ date, rank or by mark.   The
latter obviously only works after you have first marked
one or more files.   This can be done by the Mark Up or Mark
Down commands, or alternatively, as you scroll through files
in the List window, pressing the space bar
automatically marks the file the cursor was located
on.  
    Both the List window and View window can be zoomed, to
either get more information about a file 's size, directory or
creation time, or to fully view the contents of a file.
  They can  diagram   
also be incrementally adjusted using Control in conjunction
with the arrow keys.  
    Searching for information is a dream - tell Magellan
what you want and it will go find it - Lotus claim three
seconds to find a single word anywhere on a 40Mb hard drive.
  In use, Magellan was at least that fast and often quicker.
  Magellan also does fuzzy searches and will list files in
descending order for closest matches, which allows concepts to
be searched for.   The threshold for this type of search can
be specified by the user.   (Magellan can also search
network drives, RAM disks and CD-ROM disks - in fact, any
logical drive the PC can access.)  
    The Compose function allows international or
special characters to be entered in an Explore path or
dialogue box, so Magellan can search for them.  
    Organising files and directories is almost too easy -
using the Copy, Move, Rename and Delete functions, shifting
whole directories or parts of them requires only a few
keystrokes.  
    Using the Macro facility, regularly used routines can
be initiated with the minimum of key strokes and Magellan
can be customised to suit the individual.   This
customisation includes number of lines displayed - graphics
card/monitor dependent of course.   Macros can be created by
recording an operation as a macro or written directly with
keystrokes and keywords.  
    A Start Up macro can be specified that automatically
runs when Magellan is launched, switching, say, to Tree
mode, selecting a directory or listing applications ready to
open.   Magellan comes with several pre  -  defined
macros for marking files up or down, sorting by time, repeat
last Explore, exit to DOS, redraw the screen or run one of
five tutorial lessons.  
    The ability to launch applications or load a selected
file including the application that created it, makes Magellan
a very user friendly DOS Shell.   Applications can be
ordered according to frequency of use, so that if XYZ program
is used the majority of the time, it can be placed at the top
of the list - anything to cut down on the key strokes - I 'm
lazy at heart!  
    Unless there is a good reason not to, the best way to
use Magellan is to have it load automatically as the last
command in the Autoexec.bat file.   From there, you can
launch whichever program you need to use, while Magellan
retires to the background, using only 5Kb of RAM.   Exit
the application and you are back in Magellan, ready for the
next operation.   Too easy!  
    One handy facility is the Gather function, which
allows marked text to be collected from one or more files and
written to a new file.   For example, a series of
minutes might contain various references to a proposed
project.   Gather allows you to pull all those
references together and create the bare bones of a dedicated
project document file.   Gathered files are stored in ASCII
format that can be easily imported into your favourite word
processor.  
    As I mentioned earlier, the documentation is
superb.   The Explorer 's Guide is divided into Getting
Started, Tutorial, Reference and an Appendix with glossary and
information on Magellan files, program compatibility,
troubleshooting and an index.   It 's almost worth buying
Magellan just to enjoy the documentation and its light hearted
graphics.  
    In case you get stuck for inspiration, the Ideas
booklet has information on organising files, filtering and
exploring by topic, discovering databases you never knew
you had, using Magellan to pilot other programs, using it as a
program manager, hints on macro ideas and using Magellan to
back up files that have been changed since the last backup.
  Say goodbye to the terse DOS Backup command and hello to a
two key painless back up routine.   The biggest problem I
had reviewing Magellan was deciding when to stop - it would be
all too easy to keep going on about it.   Suffice to say,
it 's the best file/data management utility I have ever
used.   It now resides on the editorial computer and picking
the   Editors'   Choice was easy for once.   Magellan
well deserves it.    
            Big Time for
Newport        
      Newport Electronics have moved into the big
time by adding to their existing vast range of panel
instrumentation, two new, large size, 4-digit panel
displays - the L2Q Series and L4Q Series - the largest of
which is visible at 65 metres.    
    The L2Q Series feature digits 57mm high in a 120 x 264
x 117mm case.   The larger L4Q Series have 120mm digits in a
case 18O x 480 x 112mm.   Both have a number of mounting
options and operate using 18 analog input conditioner
cards which can be configured to directly accept the output of
most standard transducers, temperature sensors, voltage and
current inputs.  photo    
          Chemical Resistant pH and ORP
Sensors        
          "Special measurements require
advanced sensor technology.   The capability of any system
is based on the quality and accuracy of the signal
provided by the sensors but, to achieve the benefits of
advanced sensor technology, the most appropriate sensors,
transmitters and controllers must be integrated into an
effective package."        
    The above is part of a policy statement from Great
Lakes Instruments - measurement specialists, Milwaukee,
Wisconsin.   Sensor technology provides the cornerstone of
the special measurement systems developed by Great Lakes
and their latest product releases are illustrative of this,
incorporating the patented, field-proven Differential
Electrode technique for which they are well known in many
parts of the world.  
    The new releases are pH and ORP sensors, fully
encapsulated and enclosed in bodies fashioned from liquid
crystal polymer (LCP), which provides excellent   photo  
chemical resistance and meets the requirements of
every demanding industrial application.  
    They may be used with acids, bases, alcohols,
hydrocarbons, chlorinated hydrocarbons, aromatics, esters
and ketones.   Chemical resistance is also important in the
measurement of oxidation reduction potential.  
    ORP monitoring is of importance in industrial waste
disposal systems, such as chromate waste baths, cyanide wastes
from metal plating and metal treatment processes.  
    The encapsulated design eliminates moisture and
humidity problems and extends the working life of the sensor.
  The low heat distortion of LCP allows these sensors to be
used in metal fittings without risk of the leakage usually
caused by the heating/cooling cycles of other materials.  
    Two standard mounting styles are offered - a
convertible style for submersion or flow-through mounting
and a union-style mount for easy service in flow-through
installations.   Optional mounting hardware assemblies are
offered in varied materials, each including a pipe-mount
or surface mount junction box with terminal strip.  
      The patented Differential Electrode measurement
technique, developed by Great Lakes, uses two glass electrodes
- a process electrode and a standard electrode immersed in
buffer -to make pH measurements differentially.   A third
metal electrode is added to reduce ground loop problems.
  In this way, errors from contamination, such as
precipitate build-up, and electrode drift are virtually
eliminated.  
    A double junction salt bridge electrically links
the standard electrode to the process, making contamination
unlikely and, since ground-loop current bypasses the
standard electrode, the reference signal is not affected
by precipitate build-up on the salt bridge.  
    Compared to other, more conventional techniques,
this patented sensor provides measurements of greater
stability over long periods, with less down  -  time
and maintenance.  
    These pH and ORP sensors are offered with an
integral preamplifier or two-wire transmitter which provides a
4-20 mA output proportional to pH.   Each sensor also
includes a temperature sensor to compensate automatically for
process temperature variations.  
    The sensor body has a distinctive hex shape to
facilitate installation.   Threads are provided on both ends
so that the sensor may be mounted into a pipe tee or attached
to the end of a pipe for submersion applications.  
        PYRO! - Preventing Screen
Burn-in...      
      Screen-Saving Utility Offers User-Configurable
Displays, Password Protection    
    Fifth Generation Systems announced the release of
Pyro! for the PC, a screen-saving utility modelled on the
nearly omnipresent Macintosh Pyro!.   Pyro! prevents screen
burn-in and provides an entertaining display during idle
computing time.  
    Pyro! comes with four modules, including Fireworks and
Bouncing Clock (the two original modules for Pyro! Mac).
  The other modules are the user configurable "Roving
Picture" and "Message".   The program works in colour on
CGA, EGA or VGA systems and also supports monochrome
displays.  
    Pyro! can be set to "pop-up" after a user-specified
period of keyboard or mouse inactivity or through the use of a
"hot-key" combination.   Once the keyboard or mouse is
used, Pyro! will end until the next idle period.   Pyro!
also provides optional password protection.   If a user
chooses to install a password, only that password will end the
display.  
    Screen-burn is the etching in of an image that results
from a monitor being left on for extended periods of time.
  For example, a user that consistently uses one
software package will eventually find that the borders, menu
etc of that screen remain as a "ghost" image when in another
software package or even when the monitor is turned off.
  Many users also favour Pyro! because its mainly dark
display is easier on the eyes during idle computer time than a
bright screen.  
    FGS President and CEO Barry L. Bellue Sr. sees the new
product as an indicator of the convergence of the PC and
Macintosh markets.     "Three years ago, most PC users
would have seen Pyro! as a frivolity, now everyone wants it.
  We stopped trying to count the requests long ago.   The
need for a screen saver on the PC side is certainly great with
all of the paper white and expensive colour monitors but I
think that Pyro! on the PC is also a sign of the shrinking
difference between     Mactinosh    Macintosh    
and PC users."    
      "Pyro! for the PC is our most requested
product ever,"   said Dariel LeBoeuf, FGS Product
Manager.     "I must admit that this is a case where I
had a little 'Mac Envy' myself.   Most people would be
surprised to learn that Pyro! Mac 's biggest area of success is
in the corporate market, where it 's purchased in volume
through traditional channels, just like a spreadsheet or
database.   Of course, we 'll try to keep Pyro! as fun as
possible but it 's wrong to think of screen savers as
toys."    
    The memory-resident driver takes up only 5K of RAM and
works with virtually any graphic display configuration.
  Pyro! PC is available immediately for $96 excluding
GST.  
    Fifth Generation Systems, Inc. is a leading developer
of Macintosh and MS-DOS products incorporated in 1985.
  Their Macintosh products are Fastback II, Suitcase II,
Pyro!, DiskLock, FileDirector, SuperSpool, SuperLaserSpool,
and the Fastback Tape 120, a high-speed Macintosh tape backup
drive.   For the PC, FGS produces Fastback Plus, Brooklyn
Bridge, the line of Mace Utilities, Direct Access, Direct Net,
The Logical Connection and The Logical Connection
Plus.  
        FILTERS PROTECT COMPUTERS      
      An advice and supply service for filters to
protect computers and all micro  -  processor systems from
the increasing incidence of damage from mains borne
interference is announced by Cory-Wright Group Ltd.    
    The Company reports a marked increase in demand for
interference protection from all types of users.  
    Filters range from the popular single, machine
protecting cord set, 6 amp rated and conforming to
international standards, to 200 amp 3 phase and neutral units
for system protection at source.    photo      
  

          Chemophobia      
        The Fear of Chemicals.   Is it
justified?      
      The fear of chemicals, especially pesticides
and fungicides, is growing throughout the world.   But does the
available evidence support the concern?    
      S  ynthetic chemicals play a
vital role in all modern industrialised societies.   These
man-made chemicals are often essential to our quality of life and
have an important influence on our food, health, transport and
leisure activities.   Without them our lifestyles would
change dramatically.   They are an integral part of our lives&semi;
from what we brush our teeth with to how we communicate and how we
combat disease.  
    Most of us take them for granted, even the ones we use
every day.   But chemophobia - the fear or aversion of chemicals
- appears to be on the increase throughout the western world.
  For many people, chemophobia equates to the effects of one
particular group of chemicals&semi; synthetic pesticides.
  'Pesticides' is a general term which includes insecticides,
herbicides, fungicides and other chemicals used to control insect
pests, weeds and diseases and parasites of plants and animals.  
    Pesticides are a cornerstone of modern agricultural
production throughout the world and New Zealand is no exception.
  For the last 40 years they have played a very important part
in allowing New Zealand farmers and orchardists to increase
production to high levels and to maintain profitability.  
    In the absence of pesticides meat, wool and dairy
production would all be significantly lower, and the labour
requirements of agricultural production substantially higher.
  We could not consistently produce apples, kiwifruit and
other horticultural crops to the high quality standards demanded
by our major overseas markets without the judicious use of
pesticides.   Pesticide use in NZ agriculture has unquestionably
provided enormous direct and flow-on benefits to both producers
and society as a whole.  
    There is also a risk with pesticides.  
    Many of them are highly poisonous to non-target organisms,
and if used carelessly pose a threat to livestock, people, and
beneficial insects.   Their use must therefore be carefully
controlled to ensure that any potentially harmful
side-effects, such as pesticide residues on the food we eat and in
the environment, do not exceed acceptable levels.  
    In New Zealand, and many other countries, pesticide use is
regulated by statute.   Use is no longer permitted of most of
the first generation pesticides&semi; those which were highly toxic or
very persistent in the environment such as DDT.  
    Pesticides are often given a bad image, particularly among
the more sensationalist elements of the media.   Urban dwellers
especially tend to be more aware of the potentially harmful
effects of pesticides than of their benefits.   A major USA
study which investigated health hazards in the food eaten by
Americans found that the main hazards, in descending order of
importance, were:
    Microbiological contaminants of food, such as
bacteria.  
    A nutritionally unbalanced diet  
    Environmental contaminants  
  photo  
    Natural poisons in food  
    Pesticide residues  
    Food additives    
    When the researchers compared the media coverage given to
these dietary hazards they found that most media attention was on
pesticide residues, followed by environmental contaminants and
food additives.   Natural poisons had virtually no coverage&semi; but
in fact these poisons are a greater hazard than most man-made
chemicals.  
    Examples of natural poisons which occur in foodstuffs are
mycotoxins produced by fungal contaminants.     Aspergillus
flavus   for example is a widespread fungus which commonly
occurs in many parts of the world as a preharvest contaminant of
peanuts and grains.   This fungus produces a variety of
mycotoxins including aflatoxin B1, a naturally occurring
chemical which is among the most potent carcinogens known.  
    In the USA, aflatoxin testing of peanuts used in food
processing is mandatory, and similar measures are proposed for
maize.   In NZ, the maximum allowable concentration of aflatoxin
in foodstuffs is 15 parts per billion.   (See Table
One).  
    The ergot alkaloids are another example of naturally
occurring poisons in food.   Ergot is associated with the fungus
  Claviceps   which grows on wheat, other cereals, and
grasses.   Consumption of products made from contaminated
cereals, or animals grazing affected pastures, results in ergotism
- which can be fatal to humans and livestock alike.   Use of
fungicides can reduce the incidence of Claviceps and hence the
risk of ergotism - one of many examples where the use of
pesticides can result in a healthier diet.    
    The bad image which pesticides are often given by the
media reflects little more than the fact that newspaper sales and
television ratings are more likely to be boosted by stories
which focus on the spectacular and emotive.   Many people will
respond more readily to the fear engendered by pesticide residues
on their food than to the far greater health risks inherent in a
nutritionally unbalanced diet.  
    We eat more fat than any previous human generation.
  That high-fat diet is strongly linked to excess weight,
coronary heart disease, gall stones, adult-onset diabetes, and
many of the most common forms of cancers.   Yet we often prefer
to ignore the dangers of our own over-indulgence and concentrate
instead on issues for which someone else can be held responsible.
  Pesticide residues are an easy target.  
    Are pesticides a health hazard in New Zealand?  
    Chemophobia arises partly from the belief that the use of
pesticides is taking a heavy toll on our health and environment.
  In addition to pesticide residues on our food, there is a
potential risk from direct exposure to pesticides through spray
drift for example, and from the build-up of pesticide residues in
soil and water.   How much pesticide contamination of our food
and environment is occurring?  
    New Zealand 's existence as a food exporter depends on our
ability to produce high quality, high return products which meet
the needs of consumers in our major overseas markets.   Our
export products are required to meet stringent quality standards
to enter these markets, including pesticide residue tolerance
levels.   Maximum residue limits (MRLs) on food are typically of
the order of one to 25 parts per million for individual
pesticides, in some cases less than one part per million.  
    Pesticide residue MRLs are set by using toxicological data
obtained for animals - divided by a large safety factor of 10 to
10,000 to allow for uncertainties when extrapolating to humans - 
  table one  
to derive an acceptable daily intake&semi; the level of residue which
if ingested by humans daily over an entire lifetime, will not
result in an appreciable health risk.   This is then compared
with the potential daily intake derived from food consumption
data and actual crop residues found in field trials using maximum
application rates and minimum with-holding periods.  
    The MRL is then set based on the worst-case crop residue
as long as the potential daily intake does not exceed the
acceptable daily intake.   No MRLs can be set unless the
toxicological consequences of the residues in the foodstuff are
insignificant.  
    In fact, because MRLs are based on worst-case field data,
actual residues are generally well below the set limits.   Some
pesticides can cause cancer when fed in large doses to animals
such as rats and mice, but small traces found in foods which meet
MRLs do not produce these symptoms and are not known to cause
health problems.  
    Hence New Zealand 's primary foods, grown to export
standards, are not known to pose any significant health risks.
  For most of these foods no distinction is made in pesticide
use between produce which is exported or consumed within NZ.
  In the case of kiwifruit for example, it is not until the
fruit passes over the grading table that some is rejected for
export - on such grounds as shape and size - and diverted to local
consumption.  
    Monitoring by MAF chemists of export fruit crops over the
last decade indicates that pesticide residues very rarely exceed
the tolerance levels set in major markets.   Pesticide use on NZ
fruit crops is high by standards for domestic USA but low by those
of the UK and Japan.   Nevertheless, the non-persistent nature
of most modern pesticides means that there is not a
correspondingly large build-up of residues on fruit.   Only
the last one or two applications before harvest contribute
significantly to total pesticide residues.  
    Pesticide residues have been monitored less intensively in
NZ on produce which is grown solely for domestic consumption, such
as local market fruit and vegetables.   Monitoring which has
been done, by DSIR and the Department of Health, has revealed no
obvious health risks from pesticide residues in New
  Zealanders'   diets.   MAF, DSIR and the Health Department
are currently conducting more detailed monitoring of residues in
domestic foodstuffs.  
    It can be concluded from the extensive export monitoring
and limited domestic monitoring which have been conducted to date
in NZ that pesticide residues in food are not 'a major health
risk'.   This monitoring also shows that, overall, there is
a good degree of compliance with spray recommendations for
particular crops and no evidence of widespread misuse of
pesticides by farmers, orchardists, spray contractors or
other pesticide applicators.  
    These residue data are in accordance with extensive food
surveys conducted in other westernised countries with broadly
similar patterns of pesticide use to our own.   In the USA, UK
and Sweden, for example as in NZ, pesticide residues in the diet
are generally well below current health guidelines with at
least 50% of samples having no residues detectable.  
    There is likewise no scientific evidence that direct
exposure to pesticides in spray drift is a major health risk to
New Zealanders.   It is true that not all spray reaches the
target, which is generally soil or the foliage of a particular
crop or weed.   Most spray drift generated during pesticide
application enters the true vapour phase and is dispersed in
the general atmosphere in extremely dilute form.   Research
conducted by MAF and the Agricultural Engineering Institute showed
that depositable drift from orchard airblast sprayers was very
low.  
    Research is continuing in this area, with financial
support from the New Zealand Kiwifruit Marketing Board, to
document more fully the dispersal of pesticides from orchard
spraying.   The only scientifically documented cases in NZ which
have demonstrated harmful effects of spray drift have involved
herbicides affecting non-target plants, generally from aerial
application.   Crops such as grapes and tomatoes are extremely
sensitive to phenoxy herbicides (hormone sprays) and can be
affected at doses as low as 0.1 to 1.0% of a full spray rate.
  Exposure to the very low concentrations in spray drift is
likely to be well below levels which are known to be significant
threats to human health.  
    New Zealand has an international image as a "clean,
green" producer of top quality agricultural and horticultural
products, but is this image threatened by the build-up of
pesticide residues in our environment?   The available
evidence suggests that the answer is generally no.  
    Extensive monitoring conducted by MAF chemists, for
example, has revealed that pesticide residue levels in the Manukau
Harbour are well below internationally accepted health guidelines
and often non-detectable.   No pesticide residues have been
detected in groundwater from the Auckland or Canterbury regions.
  Research coordinated by the Ministry for the Environment on
the herbicide 2,4,5-T did not find any 2,4,5-T or dioxin (TCDD)
residues in sheep meat from high-use properties.  
    However, the levels of DDT and its breakdown products are
still relatively high in some NZ soils.   This persistent
organochlorine insecticide was widely used on NZ pastures to
control grass grub and other insect pests, from the early
1950 's - when little was known about the environmental persistence
of some pesticides - to the late 1960 's when the use of DDT on
grazed pastureland was banned.   It will take many more years
before DDT residues disappear completely from some South Island
dryland farms, although the levels which now occur are not known
to represent a human health hazard.  
    The fact that there has been no repeat of these problems
in NZ since DDT was banned indicates the advances which have been
made in pesticide registration and use.   Modern pesticides are
biodegradable and do not persist in the environment like the older
organochlorines.  
    They also tend to be more active and can therefore be used
at lower application rates than older pesticides.   Synthetic
pyrethroid insecticides, for example, are generally applied at
rates of grams per hectare, compared to kilograms per hectare
for older insecticides.   There is also a marked movement in
the agrochemical industry towards the production of
"environmentally soft" pesticides such as insecticides which do
not harm the natural predators of insect pests.  
    The available scientific evidence indicates that the
health of New Zealanders is not significantly threatened by
pesticide residues in our food and environment, or by exposure to
spray drift.   It must be emphasised that in these cases,
science deals in probabilities based on the available
knowledge.   The public desire for absolute assurances
cannot be met&semi; but this should not lead to the dismissal of
assurances based on informed judgement of the risks.  
    Regardless of the scientific evidence and its
interpretation by experts, there remain some people with a sincere
belief that any exposure to pesticides, no matter how low, is
dangerous to human health.   The views of these people must be
considered in the political arena, where value judgements are
made in relation to the desired degree of balance between the
benefits and risks of pesticides.  
    It must also be emphasised that NZ has no grounds for
complacency regarding the risks associated with pesticide use.
  Current increases in the monitoring of pesticide residue
levels on the food we eat and in our environment must be
maintained to ensure that any incipient problems are detected
before they become unmanageable.   There is also a need for more
education of pesticide users, and for improvements to
methods of disposal of surplus pesticides and containers on
and off farms, orchards and home gardens.  
    Although there is no scientific evidence that pesticide
use significantly threatens the health of New Zealanders,
chemophobia and pesticide residues have become key issues in
the future of our export food and fibre industries.   Advances
in analytical techniques make it possible to routinely detect and
measure quantities of chemicals such as pesticide residues at
extremely low levels (parts per billion and lower).  
    Most pesticides can now be detected at much lower
concentrations than the levels which are known to be hazardous to
human health.   This has made it possible for importing
countries to set tolerance levels for pesticide residues at
correspondingly low levels, in order to establish a wide safety
margin between tolerance and hazardous levels, and/or in response
to political pressure from the growing environmental awareness
and anti-pesticide lobby in many western countries.  
    Market acceptance has become a separate issue from
health risk.   As environmental concerns continue to grow,
countries such as NZ which export food to the affluent nations
of the northern hemisphere are at risk of losing markets if
they cannot meet extremely low pesticide residue tolerance levels.
  Further reductions in these tolerance levels and in the
range of acceptable pesticides in key markets will heighten
the threat to the access of our primary exports.  
    In recognition of the trend towards lower tolerance level
for pesticide residues, organisations such as MAF and DSIR are
making a substantial investment in research on the
development of control methods for pests, weeds, and plant
diseases which are less reliant on pesticides.   These
approaches include biological control, pest and
disease-resistant plants, integrated pest management and
organic growing.   Considerable progress has been made,
particularly for pasture pests.  
    Alternatives to pesticides and methods of reducing
pesticide use are now well established in NZ pastoral farming.
  Examples of control methods which have been developed by
MAF researchers since 1975 and are now used widely by farmers
include: Biological controls (predatory beetles and wireworms)
to control Australian soldier fly&semi; Ryegrasses containing
endophytic fungi which confer resistance to Argentine stem weevil
and black beetle&semi; Farm management practices (grazing,
avoidance of cultivation) to control grass grub and porina
caterpillar&semi; Selective grazing pressure and reduced insecticide
applications to control lucerne flea&semi; Reduced insecticide
applications and pasture management to control cattle ticks&semi;
Simple population monitoring techniques for black field cricket to
target insecticide applications.  
    These control methods have made an important contribution
to the 63% decline in insecticide use on NZ pastures during the
1980 's, although the economic downturn of farming during this
period also contributed to this trend.   Research is
continuing on the development of alternatives to chemical
pesticides for control of pasture pests&semi; e.g., MAF/Monsanto
research on a biological insecticide for grass grub based on a
naturally occurring bacterium, and MAF/DSIR research on biological
control of Argentine stem weevil.  
      Development of satisfactory alternatives to pesticides
is more difficult in horticulture than in pastures, because of
consumer preference for high quality, unblemished fruit and
vegetables.   This cosmetic factor demands a very high level of
pest control, which is often unattainable by any method other than
the use of insecticides.   The use of insecticides in
horticulture is increasing.   Kiwifruit, for example, is a host
plant for pests such as scale insects and leafroller
caterpillars.  
    Most NZ kiwifruit growers currently apply seven or eight
insecticide sprays on a calendar basis during the growing season,
to prevent serious fruit contamination of these pests.  
    The most immediate prospect for reducing this
relatively high chemical input is the development of a monitoring
system for scale insects, to allow insecticidal sprays to be
applied only when sufficient pests present to warrant treatment.
  This approach is currently being investigated by joint
MAF/DSIR research funded by the New Zealand Kiwifruit Marketing
Board.   It may also be possible to use pheromone traps under
development by DSIR to monitor leafrollers on kiwifruit, as
has been achieved for apples.  
    These approaches may reduce insecticide inputs, but
will not eliminate them.   Substitution of the currently used
insecticides with 'environmentally soft' materials which
affect only pests may also be possible.   One such
insecticide, based on the   Bacillus thuringiensis   which
is active against leafroller caterpillars, is already registered
for use on kiwifruit in NZ.  
    Other research, such as evaluation of other potential
biological control agents, identification of pest-resistant
kiwifruit cultivars, and development of organic growing
methods, may eventually allow the use of insecticides on kiwifruit
to be reduced further, but these are long term possibilities.
  For the next few years at least, production of high
quality kiwifruit for export will continue to rely on the use of
insecticides.  
    Tolerance of a low level of fruit contamination by
pests in major overseas markets, rather than the current nil
tolerance, and/or a high price premium for organically
grown produce, may be necessary before it is possible to
greatly reduce or eliminate insecticide use on kiwifruit and other
NZ horticultural crops grown for export.  
    Most advances in biological control and integrated pest
management have been made with insect
    pest    pests     and mites.   In comparison,
the development of alternatives to pesticides for the control
of weeds and plant diseases is in its infancy.   Some progress
is being made, e.g., MAF research on biological control of
plant diseases with antagonistic micro-organisms, and DSIR
research on biological control of weeds with insects.   At
present, herbicides make up about 60% of the NZ pesticides market,
compared to 24% for fungicides and 12% for insecticides.  
    Research on integrated pest management strategies, and
alternatives to pesticides such as biological control, often
has a high risk of failure.   This type of research requires
large investments with uncertain outcomes.   Despite the
substantial investment in this area of research in NZ and
worldwide it is unrealistic to expect cost-effective alternatives
to pesticides to be available for the control of all major pests,
weeds and diseases, at least within the next 10 to 20 years.  
    It must also be recognised that integrated pest
management usually involves the transfer of more complex
information than simple calendar-based spraying and is therefore
more difficult to implement without specialist consultancy
services.   Growers are often reluctant to move away from
calendar-based spraying, unless they receive significant,
direct economic benefits from integrated pest management.  
    The political impact of chemophobia and other
anti-pesticide views is a major issue in the future of NZ 's
primary export industries.   Market access depends on our
ability to meet pesticide residue tolerance levels.   These
tolerance levels may become more stringent than those which
currently apply in our major markets, even if current levels are
already well below the levels which are known to provide a
significant health risk.  
    The market access issue has given high priority to
government and producer board investment in research on the
development of control methods such as biological control and
integrated pest management.   The success or failure of this
research may decide the long-term future of some of NZ 's export
crops.  
    The trend towards less use of chemical controls should
also relax any environmental pressure - real or imagined -
which may result from the use of pesticides.   Nevertheless, it
is likely that control of many pests, weeds and diseases will
continue to rely largely or entirely on pesticides for at
least the next 10 to 20 years.      

  

        PC waters looking a little
muddier    
      T  EN years ago this month, IBM gave birth
to the personal computer and spawned a billion-dollar
industry.  
    And at the Wellington Show and Sport Centre last week,
one of the side-products of the birth - the personal
computer exhibition - attracted droves of the faithful who
swarmed to pay homage to the PC.  
    IBM was there with its notebook computer - yet to be
officially launched in New Zealand.   But IBM 's delivery is
late.   There seemed to be notebook computers on every
second stand at PC   '91  .  
    IBM was also showing off its RS/6000 reduced set
instruction computing workstation - on the same stand and
sitting right alongside a workstation from Sun
Microsystems.   Workstations?   Sun?   Risc?   Who
would have dreamed of such things 10 years ago?  
    In the tumultuous decade since the arrival of IBM 's
PC, the world has been turned upside down.   But in August
1991, instead of clarity and understanding about where PC
development is now and where it is heading, there is
only confusion.  
    IBM is there in the thick of it all, hitting the
headlines with its alliance with arch-rival Apple Computers,
and its attempts to maintain a stranglehold on the PC
market through the introduction of new architectures, the
development of new operating systems, and liaisons with just
about anybody     one     you care to name.  
    In actual fact, it was not IBM that really gave birth
to the PC, but 
  photo  
  caption  
it was IBM 's involvement that changed the world - because it
saw and seized an opportunity in the business market.
  Today the waters are a little muddier.  
    The evolution of the PC can be traced to 1965 with the
Beginners All Purpose Symbolic Instruction Code (Basic) - one
of the first simple high-level programming languages for
non-programmers.   Microsoft can trace its roots back to
Basic.  
    In 1966 Robert Noyce and Gordon Fairchild formed
Integrated Electronics, or Intel.   They eventually
created the microprocessor destined to be the brain of the IBM
PC.  
    The first PC was called the Kenbak 1 - developed by
John Blankenbaker of Boston in the United States and
launched in 1971.   It offered 256 characters of memory and
cost US$750 (NZ$1300).   Forty were sold.   Later that
year Intel introduced the 4004 chip, the world 's first
microprocessor.    
    By 1975 things were hotting up.   Two thousand Intel
8080-based PCs, called the Altair, were sold.   Bill Gates
and Paul Allen formed Microsoft as a software writing
partnership.   Their first project was to write a
version of Basic for the Altair.   In December the first
retail PC store - the Byte Shop - opened in Mountain View,
California.  
    On April Fool 's Day in 1976, Steve Jobs and Steve
Wozniak incorporated Apple Computer in dual California
headquarters - a garage and a spare bedroom.   First year
sales of the Apple II in 1977 totalled $2.5 million.  
    Intel 's 8086 and 8088 microprocessors emerged during
the next couple of years, as did Wordstar, the first
high-selling PC word processing program, and Visicalc for the
Apple     11    II     - the first user-friendly
PC-based spreadsheet software.  
    IBM was getting worried.   In July 1980 it made an
unpublicised decision to develop a PC.   The endeavour
was codenamed "Project Chess" and the processor called Acorn.
  Not a moment too soon.   A month later Ashton-Tate was
founded and launched Dbase II, the first high-selling
database management software for PCs.  
    April 1981 saw Adam Osborne 's Computer Corporation
introduce the Osborne 1, the first portable computer at 27
pounds.   The machine was bundled with Wordstar and
Supercalc - a precursor of things to come.  
    But in August of that year came the key contribution
from IBM.   Suddenly the PC won instant credibility with
corporate America.   IBM 's first effort ran Microsoft MS-Dos
1.0, Intel made the 8088 chip, and third parties made the
software, the disk drives and the printers.   But it was
IBM 's involvement that mattered.  
    It was a watershed year.   A few months later
Microsoft and IBM signed their first software agreement.
  Apple went public and made many early employees instant
millionaires.  
    From that moment the PC never looked back.   At the
end of 1982 the total value of software shipped that year was
estimated at $1 billion and Time magazine named the computer
its "Man of the Year".  
    It wasn't long before other companies started to
emulate the IBM model, using Intel chips and the Dos operating
system.   Companies such as Compaq were quick to emerge
offering glossy expensive machines, while elsewhere the
invasion of the clone was beginning.   Low-cost lookalikes
flooded out of Asia in the 1980s, eating into IBM 's market
share.   And nowhere did they appear to be more
successful, on a per head of population basis, than in New
Zealand.  
    Over the decade IBM was not generally first with the
latest.   Its first attempts to enter the home market, and
produce a portable computer and local area network, were
disastrous.  
    Frequently reactive rather than proactive, IBM decided
to tackle the clone problem with the introduction in 1987 of
microchannel architecture and the OS/2 operating system.
  It was greeted less than enthusiastically - demanding
a whole new set of applications.   Problems with
traditional partner Microsoft and the rise of the
workstation initiated by Sun Microsystems have created more
headaches for IBM.  
    This year has been another watershed one.   The
emergence of the Ace consortium was the first oddity - an
agreement between Compaq, Digital, Microsoft, Mips, Santa Cruz
and others to develop new hardware and software
standards.  
    IBM was quick to react.   In an agreement that once
would have seemed bizarre, Apple and IBM agreed on a single
software and hardware platform for their next generation
personal computers and workstations, built round an
advanced version of the RS/6000 risc chip.   Apple will
license its Macintosh software technology to IBM, which will
incorporate it into its workstations and mainframes, and
has set up a company to finish development of Apple 's
object-oriented programming-based Pink operating system
running on Intel processors.  
    What does it all mean?   Nobody really knows - apart
from the two clear conclusions that IBM, as usual, is
attempting to dominate the market, and its rosy
relationship with Microsoft is badly tarnished.   The race
is on for the next generation of personal computers.   The
starters in the first race - Intel, Apple, IBM, and Microsoft
- are still there, jostling for position.   What will
the PC market of 2001 look like?   At this stage, it 's
anyone 's guess.    
        Legal worries put imaging on
backburner    
    NEW ZEALAND companies are highly interested in
electronic imaging but are also worried about the inadequacy
of the law for admitting electronically-stored images as
evidence.  
    These are some of the findings of a survey by Deloitte
Ross Tohmatsu of 73 organisations from an original mailout of
112.  
    Fifty six per cent of those surveyed were information
systems managers who were able to identify only six
imaging vendors active in the New Zealand market
place.  
    A weighty 63.6 per cent of the sample raised legal
problems, with the acceptability of imaging systems
evidence a big, while 9.1 per cent were not bothered about
this issue.  
    While 65 per cent of those sampled were either
planning investigations or thinking about electronic imaging,
27.4 per cent were putting the issue off till next year while
6.8 per cent did not see imaging as relevant to their
organisations.  
    The main reasons managers cited for adopting
electronic imaging were better customer service (38 per cent)
and improved access to customer files (20 per cent).  
    The biggest impediment to adopting imaging were seen
to be: high cost (36 per cent) and other higher priority
projects (30 per cent).  
    More than 45 per cent of those
    survey    surveyed     had budgeted
more than $10,000 for investigating imaging next year.  
    Consultant Michael Konnoth says the survey 's high
response rate indicates a lot of interest for imaging.  
    He says while many of the objections to imaging
projects are other higher priority projects this will not
always be the case and he says companies buying personal
computer equipment to support new standards such as
Windows 3.0 will find they are already installing the
capability to handle imaging.  
    He says gradually the perceived high price of imaging
will not reflect the true cost of technology as equipment such
as optical jukeboxes become desktop items.  
    But Mr Konnoth declined to guess how many installed
imaging sites there would be within a year.  
    Mr Konnoth and Norm Hosken, a consultant to Deloitte,
say there will be a considerable amount of work for
consultants from imaging - particularly in workflow
planning.  
    Mr Hosken says workflow planning goes to the heart of
any business and vendors are keen to see consultants do this
work while they concentrate on installation
issues.    
        Wang looks forward to more Mips
sales    
    WANG strategic relations manager John Hawkins says the
company expects to sell 90 Mips computer systems this year
and is looking forward to the emergence of the Advanced
Computing Environment systems for further sales next year.  
    The Ace is a consortium of more than 60 companies
initiated by Mips, Compaq, Digital, Microsoft and the
Santa Cruz Operation to replace the Intel-based personal
computers of the 1980s with a reduced instruction set-based
architecture designed by Mips in the 1990s.  
      Mips'   Australia managing director Rob Byrne
says the environment will offer both Sco Ace/Unix and
Microsoft 's NT operating system on either Mips risc or
Intel 8086 family processors.  
    The Ace/Unix will be a combination of Digital 's
Ultrix, Open Software Foundation 's Motif, Sco 's Unix,
    Mip 's      Mips'       risc/OS and AT &amp;
T 's system V.4.   Both operating systems and processors
will provide backward compatibility to MS-Dos applications
through compatible software or emulations putting up to 45,000
applications at the disposal of the users.  
    Mr Byrne says the new systems - based on the Mips
R4000 processor to be launched next year - range from the
middle of the personal computer price/performance curve -
including laptops - into larger server systems.  
    He says the 64-bit R6000 will easily outperform the
32-bit Intel architecture system and competition in
fabrication between companies including Samsung, Sony and
Seimens will drive down the price.  
    The R4000 chip is designed to be flexible with support
for symmetric multiprocessing and fault tolerance built
in.  
      "  From 1986 to 1987 Mips was
effectively getting its own act together.   From 1987
till April when Ace was announced we were fighting the chip
war with the other risc chips.   Now we feel the chip war
has been won and we have to win the systems war.     For
that we 'll need great partners,  " Mr Byrne says.  
    However, he says Mips will always be a low-profile
company, concentrating on design work and also providing
chips and systems through resellers such as Wang.  
    Mr Hawkins says in the United States, Wang has
rebadged Mips product.   It has decided against this
option in New Zealand because of the credibility the Mips name
carries in a market that shows more interest in open systems
than the United States.  
    Mr Byrne says partners with high market share in New
Zealand such as Wang are attractive sales and support agents
though this is shared with Eagle Technology via Prime.  
    Mr     Byrnes    Byrne     says he does
not expect much to come from the Apple-IBM alliance if it
is signed later this year and he says any products that do
emerge will not begin to enter the market till 1993.  
    Mr Hawkins says     IBM    IBM 's    
alliance with Wang is to use IBM 's risc technology for its
  "open office"   imaging and
office automation development, not for
  "commercial"   or database
engine systems.  
    He says Wang 's original VS family will be
progressively   "opened"   with
non-proprietary technology such as memory and will attract
customers where specific applications are required and
  "openness"   is not so highly
prized.   Examples include the financial and legal
markets.  
    Mr     Byrnes    Byrne     says Mips enjoys
an advantage over other risc
    systems    system     manufacturers in
that it has never targeted the technical workstations market
and therefore concentrates on the
  "commercial"   market.
  He says while some companies offer low-cost risc they
cannot match Mips on service support and
performance.    
        Rising Sun comfortable with workstation
trends    
    SUN Microsystems, buoyed by strong regional and
international financial results, is unperturbed by the
prospect of the workstation becoming a commodity item.  
    Regional marketing director Dr Tony West says low
prices will stimulate the market and allow Sun to continue
dramatic growth in workstation shipments.  
    He says Sun is a cash-rich company with US$834 million
(NZ$1.45 billion) in hand and, unlike some competitors, does
not have financial problems.   Revenues for Australia and
New Zealand were up 61.5 per cent during the past last year to
A$96 million (NZ$131 million), with New Zealand accounting for
$25 million.  
    Resellers sold $45 million worth of products last year
compared with $15 million the previous year.   Compounded
growth in the five years since Sun began operations in
Australia and New Zealand is more than 100 per cent.  
    According to IDC Research figures, Sun has doubled its
Australian Unix market share to 40 per cent.   Unix
workstation market share has increased from 38.4 to 53.2 per
cent and unit growth rose from 1700 to more than 4000.
  Staffing levels have increased 57 per cent to 172,
including 25 in New Zealand.  
    The parent company 's revenues shot up 31 per cent
to US$3.22 billion and net income increased more than 70 per
cent to $190.3 million.   Fourth-quarter net income and
revenues increased 35 per cent on the previous year.  
    The 2 1/2-year-old New Zealand subsidiary, under
managing director Don Sykes, still reports into Australia, and
Dr West says a change is unlikely because of the support
Australia can give.  
    Commenting on the IBM-Apple alliance, Dr West says it
will be the death-knell of the Motorola 88K and the Intel
i860 reduced instruction set computing chips.
      "Mips and Sparc are the only games in
town."      
    The Ace consortium is   "too little, too late.
    They 're not decisive enough about stating what
  photo  
  caption   
they 're going to do  "  .   There is still confusion
about the future of OS/2 and the IBM-Microsoft relationships
and   "the more confusion there is, the more we 'll forge
ahead"  , Dr West says.  
    Sun expects to retain a dominant position in the
Sparc market through its low manufacturing costs and
value-added components.   Two out of three risc
processors shipped last year were Sparc.  
    Dr West says Sun expects to ship 200,000 units this
year and he believes there will be a volume explosion during
the next two or three years.  
    Only the IBM-Apple alliance stands a chance of
catching Sun, he says, dismissing other competitors for being
overpriced or poorly marketed.   Key sales factors will
be the low-cost and ease of use - where Dr West admits there
is still work to be done - value-added applications, and low
manufacturing costs.   The Sun hardware has become far
less complex and the move to Unix V.4 later this year should
help sales.  
    Multiprocessor capacity is not far off, enhancing the
efficient I/O throughput architecture.  
    Superscalar Sparc chips of around 80-100 million
instructions a second are arriving soon from Texas
Instruments and LSI Logic.  
    New colour low-end machines, replacing the IPC and
SLC, are now available in New Zealand.    
          Crystal driver a winner for Lower Hutt
maker    
  photo    caption  
    LOWER Hutt company Solid State Equipment has won
orders to supply NASA, Atomic Energy of Canada and Texas
University with a device evocatively described as a
"crystal driver".  
    The high-technology tool is used to determine the
viscosity of liquids (including metallic glasses) when
they undergo chemical or temperature changes.  
    Costing US$20,000 (NZ$35,000), the device is used in
conjunction with a personal computer for programmed
experiments over periods of time.  
    Originally conceived by the Department of Scientific
and Industrial Research 's physical engineering laboratory
in the 1970s, the machine won the affection of visiting United
States scientist Steve Carpenter.   He was impressed with
the system 's ingenuity and cheap price.  
    Redesigned by Solid State co-managing director
Neil Poletti, the early versions featured dials and knobs
requiring the device to be constantly monitored during
an experiment.  
    However, the latest version of the machine is now
based around a Motorola MC68H11 microprocessor
programmed in the Forth Language and is controlled by a PC
application programme written in C++.  
    Mr Poletti says the machine constitutes something
of a niche market in that nobody else in the world makes it
and so far sales have been through word of mouth in the
scientific community.  
    The potential market, while inherently limited, is
not necessarily small and could include every university and
relevant research institution in the world.   A smaller and
cheaper crystal driver is planned for industrial
scientists.      
  

          THE CLYDE RESERVOIR      
    The worrisome aspect of the Clyde dam reservoir is the
fact that there had been so little seismic activity in the
plate block area in recent times.   This had led project
engineers and Electricorp to continue to describe the River
Channel fault as   "inactive."     This lack of
activity might mean that any "stored energy" that is there had
not been relieved naturally for some time.   Whereas Benmore
was not built directly on top of an active fault, the Clyde dam
was.   Any large increase in seismicity might increase the
dangers of dam failure a hundred-fold or more because of
this.  
    Being some 64 metres high, the Clyde dam would hold
back a considerable amount of water - up to three times the
amount held in Wellington harbour.  
    Due to the extensive area of the lake catchment a large
quantity of silt and bedload might reduce the life of the
reservoir and dam to under 60 years, which was the average life
of gravity dams.   The power station would then have to
operate as a "run of river" station due to the loss of storage.
  This was one of the factors looked at when the water rights
were being considered.   That was, would it be the best use
for the water?   The Roxburgh dam was moving swiftly towards
being a "run of river" station due to serious silting in the
gorge and would operate only so long as the intakes were clear
of this silt and debris.  
    One wonders if the bottom sluice (used to take out silt
and floodwaters) in the Clyde dam would block up as quickly as
Roxburgh 's did after only 15 years in operation.   The finer
(flocculated) cohesive material had a tendency to form density
currents which would move generally through the reservoir
towards the dam.   Larger rock particles formed deltas at the
head of the reservoir or in arms around the lake edges.  
  cartoon  
    Any thoughts Electricorp engineers might have of
dredging out the reservoir behind the Roxburgh dam might well
be futile because the very fine silt was some 6 - 8 metres deep
there and would simply flow from upstream to replace that which
was excavated near the dam.   The silt was so fine that, near
the shores, it was like walking in cotton wool.   There was
also the problem of where to put it, if it was successfully
excavated.  
    One of the problems with dams of any type was not so
much with the finer, or even coarse material entering turbines
and causing damage but the air bubbles that went in with it,
affecting water densities and pressure down the draught tubes.
  This could be an indirect cause of "water-hammer," as
anyone with faulty plumbing in their house would know.   On
the scale that a dam experienced it, despite the operation of
surge tanks, it could be very damaging and would shorten the
effective life of the dam.  
    There was a large amount of the finer material in the
sides of the Clyde reservoir because it had been subjected to a
grinding movement due to landslides and glacier action over a
period of many thousands of years.   Also, the schist, when
ground down, was much finer than the sand that made up
sandstone, greywacke or most other types of rock.   Due to a
fairly dry climate now and earlier afforestation and also to a
relative lack of wind, much of this finer material remained in
place.  
    Around the lake sides and bottom there was a fear of
water percolating down through the crushed schist to
tectonically stressed rock at depth following the filling of
the reservoir.   However, despite inadequate data, the risk
of this happening to any devastating effect might not be
large.  
    But since the dam itself was positioned right over the
faults the effects could be much more devastating because it
simply wasn't known what stresses had built up in the ground
and fractured rock, especially further down in that fault.
  These stresses could be suddenly released if a large
earthquake occurred in the region of the reservoir.   Quite
often, the fact that an active fault hadn't moved could make it
more hazardous than if it just recently had.  
    For this reason, the Rodgers Creek Fault just north
west of Oakland, San Francisco was feared as a site where
violent movement might take place - about a 90 percent chance
within the next 30 years.   This probability had been
determined from some 300 seismometers placed along the fault.
  As mentioned  earlier, there were no seismometers in the
Clyde/Cromwell area.    
        DAM SAFETY      
    Geophysicist Dr W. Smith had said that occasionally
deep earthquakes have been known to cause damage in New
Zealand, but in estimating earthquake risk the main attention
was drawn to the shallow activity (such as near Clyde).   In
its submission to the Wheao canal Committee of Enquiry in 1983,
(the canal collapsed soon after it became operational) the
Geological Society of New Zealand listed as its most important
recommendation the fact that planning teams (for proposed dams,
canals etc) include an engineering geologist from the very
onset of developing the project.   This was not done at
Clyde.  
    Some "lay people" and residents pointed out some of the
potential effects of the Clyde Scheme F. project on the terrain
and the potential effects of the countryside on the dam, but
were ignored because they were not expert engineers.   There
was only one thing worse than not taking due note of what
concerned non-experts had to say and that was having too few
experts of your own and ignoring them anyway or relying on
not-so-expert experts such as had been employed on the Clyde
dam scheme over the years - and still are.  
    When Gerald Lensen spoke about the insecurity of the
site while still employed by the DSIR, he said that the DSIR
wouldn't guarantee the site for a high dam and that the
Government would have to accept full responsibility.  
    The NZ Geological Society noted that the geological
complexity of New Zealand created a high potential for
difficulties with ground engineering works and it was concerned
that major capital works were being undertaken without any
requirement for adequate geological advice.   This was
certainly the case with Clyde.  
    Dam failure occurs to one dam for every 100
constructed.   About 300-400 dams are completed each year
throughout the world.  
    Dams were much more vulnerable to failure immediately
after or even during construction than after they had been
operating for a time.   The most frequent failures occurred
within seven years after construction.   After the useful
life of the dam had ended (averaging 60 years or more of
operation) the risk of failure 
  cartoon  
increased again dramatically.   Forty of the world 's 15,000
dams in existence (outside China) failed in the period from
1945 to 1980.  
    One of the Waipori dams showed a crack during an earth
slide during a flood in 1984&semi; displaying "disaster by degrees,"
not from a particular earthquake although the area around
Waipori was faulted from the Maungatuas through to Waitati.
  Worldwide, the safety record put Waipori (an arch dam) at a
lesser risk than a gravity dam such as at Clyde.   Lower
gravity dams were at greater risk than higher ones, mainly
because they were more susceptible to "tumbling  "  .  
    One of the risks of any sort of gravity dam was from
water percolating through at the sides or underneath.   Or,
in the case of the Clyde dam, water possibly entering
faulted rock near the mobile joint.   If there was
insufficient water pressure to keep the mobile joint closed
if the reservoir was only half filled, it might be like your
bathroom tap, left dribbling - sooner or later the water would
work its way round the washer and the tap would never turn
completely off.  
    In 1987 problems with movement of the Clyde dam
powerhouse construction had been encountered.   This was
because of settlement due to the large weight of concrete
poured for the foundations and the movement of rock occurred
with the excavation of the adjacent diversion channel.   With
grouting, the sections of the powerhouse were secured to the
rock beneath.   In unstable ground like this the alignment of
turbines could be badly affected by movement within the rock if
such movement continued after construction or was aggravated by
the earthquake swarms that would accompany the filling of the
reservoir.   Already the Ohau A powerhouse and nearby Pukaki
had been affected by Plate Block tilting and this could be a
warning of larger earthquakes to come there.  
    Already, even though there was no water yet behind the
dam, serious cracking has begun in the power  house and
dam itself.   At the 136 level (metres above sea level)
painters who were painting the ceilings and floor complained
that the paint wasn't drying and some of it was peeling.
  They had to get the workforce to drain the drainage tunnels
(the powerhouse goes far below the tail water level).   There
was a real fear that the enormous pressure of water when or if
the dam was filled, would make this seepage considerably
worse.  
    There were cracks opening up in the lower generator
floor near the No 1 and 3 generators.   These had been filled
with expandable epoxy cement after the edges of the cracks had
been "cosmetically" straightened.   These gaps were about
20mm in places and represented the divisions of the highly
reinforced blocks of the powerhouse as it sat on the schist
"pad" partly over the main river fault and directly on "fault
21." There was movement there originally when the foundations
were laid (see earlier).  
    There were cracks in the walls of the drainage
galleries (lower generator floor, No.3) and the drainage
gallery which drained the draught tubes area.   One was a
particularly bad crack several millimetres wide and with
considerable amount of calcification forming around it.   It
was being monitored carefully as were most of these recent
cracks.  
    One of the problems encountered was the cracking of the
electrical conduits running throughout the structure, through
the concrete.   Water had permeated into these pipes and
caused electrical shorts and malfunctions.   It 's presence
showed that the cracks forming in the blocks might be serious.
  It was a problem which is very rare in concrete dam
constructions - particularly one, like the Clyde dam, which
wasn't even filled and operating!  
    There would be a problem of alignment for the
generators, draught tubes and penstocks if these cracks widened
even without the additional heavy pressure of water from a
completely filled reservoir.   One shudders to think what
might happen when water from a filled reservoir came thundering
down the spillways.   It might be like jumping up and down on
a jelly!  
    There was a very serious and recent (June 1990)
compression crack to the right of the penstocks and on the
generator pad near the stilling basin (below the spillways).
  It was near the shaft built early on to investigate the
shears beneath the spillway blocks.   Concrete was cracking
and breaking away as the two huge blocks were forced towards
each other.   This crack had to be seen in context of the
crest to toe cracks on   the   reservoir side of the dam
(mentioned elsewhere), to the left (facing it) of the mobile
joint.  
    It appeared that the entire block or blocks in between
these two major cracking episodes might be sinking and tilting
left towards the river channel fault hole.   It had to be
remembered that the amount of concrete "pulling" the structure
down at that point was over 300,000 cubic metres.   Again, to
the extent that this problem might be aggravated after the dam
was filled could only be conjectured but was bound to cause
critical problems for leakage generally and possible calamity
even if carefully monitoring and evasive engineering steps were
taken.  
    The compression crack was right on the junction of the
River Channel Fault and the Road 6 fault which branched off 30
degrees to the right in an upstream direction.   The upstream
rock section had been inching downstream long before the dam
was even mooted.   This section was directly upstream of the
mobile joint and included the fault hole itself.  
  cartoon  
    Although clever engineers had devised a compressed rock
"blanket" and grout curtain on the upstream side of the dam
right up to the mobile joint to keep water from seeping into
the concrete-plugged fault hole they relied on an inflexible
concrete pad under the stilling basin on the other side.   It
appeared that water may have seeped down into the fault hole
through cracks in or around it when water was released into the
stilling basin.   The fault material (pug) may have been
re-watered since the river channel was diverted and the hole
dug out and re-filled with concrete in 1984/5.  
  cartoon  
    If the dam is commissioned water would soon be
pressuring all sections of the structure and what this would do
to the existing, recent serious cracks hardly bears thinking
about.   As Gerald Lensen said about the problem with the
slides in the gorge,   "they 're just a smokescreen - the
dam is the real problem."    
    There was a problem being monitored (June 1990) with
pump correction - to keep the level of the stilling basin low
in case it backed up against the concrete plug in the mobile
joint and pushed it out through the other side.  
    There were various other design and engineering bungles
including the one concerning the roof of the powerhouse.   It
was covered with aluminium roofing but unfortunately, due to
the extremes of temperature in Central Otago, began to shear
and open up at the bolt holes.   This let in water to the
powerhouse below causing damage to fixtures and equipment.
  The aluminium was dismantled and steel roofing put in place
instead.   This caused problems because the steel beams and
struts weren't strong enough to carry the load.   Thus
another correction to a correction was made.  
    Because of the design changes, the dam had been moving
towards the powerhouse very slightly since it was completed and
this problem was largely countered in 1984 and 1985 by using
huge jacks at the bottom blocks of the dam.   That stopped
the movement apart from the upper blocks which continued to
move for a time.   No further trouble was anticipated with
this problem.   This was a different problem to the
compression cracks near the stilling basin mentioned
earlier.  
    A near disaster was nearly caused when water began
flooding through the lower coffer dam - nearly leading to its
collapse, loss of life and millions of dollars of damage.
  It was caused by project engineers directing a dredging
contractor to remove material from the river bottom close to
the bottom of the lower coffer dam.   The resultant flood
swamped the pumps and only through strenuous and desperate work
with the heaviest equipment on the site - blocking the flow of
the river, was a calamity averted.  
    Another calamity was averted when the engineer, who had
been trying to straighten the drop bars on the gate on the
river side of the diversion sluice let water     in
to    into     the sluice.   Unfortunately, he didn't
check to see if workers were in there (there were only two -
the others had left earlier) working in a special bulkhead,
cleaning out and repairing the tunnel which had suffered damage
and scouring over a period of time because of the delay in
filling the reservoir.   The workers escaped somehow after
water threatened to rise quickly over their heads.  
    Another example which did not involve safety but
resulted in sheer waste was the $380,000 refit of the Koe-Ring
digger and excavator especially to take out the coffer dam
after the completion of the diversion sluices.   It was never
used for that purpose anyway and was used only for one hour for
other purposes before being abandoned.   It reminded one of
the days of the old National Party 1950 's and 1960 's when the
farmers   "had it so good"   that you would hear of
a farmer buying an expensive post hole digger, digging half a
dozen holes with it then parking it to rust away at the corner
of a paddock.  
    Another equipment extravagance was the concrete shed
erected near the Electricorp offices near the dam, costing
several hundreds of thousands of dollars, especially to house a
small pickup truck and a lawnmower.   The plush Electricorp
offices on the site had undergone a continual metamorphosis as
planners couldn't make up their minds what to use them for&semi; a
South Island HQ, mere site office or whatever.  
    Safety problems in dams or other large works are
generally met with unquestionable reliance on the
  builders    "    '       abilities to
engineer workable solutions, says Gerald Lensen.   An
alternative description we suggest is "Bureaurocracy Under
Momentum" ('BUM') - treating all present or future problems as
ultimately solvable in order to ensure a continuity of
employment and spending in the area and face-saving for
engineers and decision-making politicians.   Construction
problems in the past had always been dealt with, having a
virtually unlimited use of public money, by "MESS" (Ministry
Engineering Silly Solutions).  
  cartoon  
      As each new method of dealing with disasters on
site was announced the public sighed and said, ah, they must
know what they are doing.   However, looking back over the
years at the activities of these "sticking-plaster" engineers,
especially in relation to the Clyde dam, it left one with the
sinking feeling that indeed if they didn't seem to know what
they were doing, they were going to let as few people know
about that as possible and treat all objections as ill-informed
and emotional.  
    Even the possible commissioning of the dam posed
several problems.   Normally, planning, design construction
and commissioning was undertaken in a fairly close sequence
(such as at, for instance, the Manapouri project).   The idea
was to install the generators, draught tubes and penstocks (and
all other equipment) and while the installation engineers were
there on the spot - fill the reservoir and make whatever
construction and engineering adjustments that were required so
that when the first water came into the generators, all was
well - there would be no mal-alignments (even one millimetre
out is serious) and there would be no chance of equipment
failure or any blow-ups or blow-outs.  
    Unfortunately in the case of Clyde the manufacturer 's
installing engineers were now spread all around the world on
other jobs and would have to be tracked down and returned at a
time convenient to them and certainly, at very great cost!
  Commissioning electricity generators is a very specialised
job and not just any engineer can do it!  
    Early in 1990 a campaign by the State Owned Enterprises
was launched advocating a greater freedom for them from public
scrutiny - especially by means of the Official Information Act.
  It was   "intrusive"   they said.
  Submissions from Government agencies urged, instead, for a
much tougher scrutiny - especially to overcome the
  "commercial sensitivity"   clauses which were
used so often to block requests for information as to what the
S.O.E 's were doing.   It is hoped that requests for
information in the future on the safety of the Clyde dam
through the Official Information Act would   be   granted
without any resistance, even to "lay people."  
    Rather unforgettable, as a "disaster on site," was the
huge reinforced concrete canopy Ministry engineers designed and
built to fit over the eastern entrance to the Homer tunnel at
Milford to deal with the frequent avalanches of snow and rubble
that continually blocked the road.   One day in 1951 a huge
avalanche of snow swept it away so completely engineers could
not even find traces of its existence.  
    In 1981 the Ruahihi canal in the North Island was swept
away - said to be caused by an engineering fault.   In 1983
the Wheao canal was devastated by a torrent of water, silt and
boulders that - after breaching a canal wall, poured down the
hillside, destroying the power station and other buildings.
  The project had been declared officially open the day
before by the then Prime Minister, Robert Muldoon with Bill
Birch - both prime movers of the "think big" strategy of the
former National Government which "masterminded" the hasty
choice of site and construction (without proper investigation)
of the Clyde dam in the vain hope that some big user from
overseas would come along and buy the power generated from
it.  
    The "mega-dam" is well on the way out these days.
  All but seven of the world 's hundred largest dams were
built since World War Two.   Dams, such as the Aswan dam in
Egypt and the Ataturk dam which was nearing completion in
Turkey was, or would be, environmentally destructive.   Also,
because of their size (like the Yacyreta dam in Argentina) they
mounted huge problems with cost over-runs&semi; the Yacyreta dam
costing $US6 billion - some three times the amount budgeted.
  The World Bank now looks very closely at large dam projects
for these sorts of cost problems and also for the environmental
damage they cause.  
    One of the worst cases of inexcusable ecological damage
was the construction of the Balbina dam in Brazil in 1988 -
flooding huge tracts of virgin forest&semi; over 2,000 square
kilometres!   The World Bank 's project to dam the Narmada
river in India is being fiercely resisted by the 100,000 people
it will displace.   They are succeeding.   The other dam on
the river&semi; the Sardar Sarovar was commenced in 1987 and will
displace 67,000 people.   Local studies of both projects
declare them to be "dodgy," economically and likely to grind to
a halt through lack of finance, leaving behind vast
environmental damage.  
    Periodic reviews of the safety of dams occur from time
to time.   One of the more successful dams in NZ 's history
was the Waitaki dam, designed and built in the 1920 's.
  Alterations proposed include enlarging the ability of the
dam to take away flood waters (a 25 percent increase).   The
significance of uplift pressures underneath the dam are
beginning to be realised and are now being
monitored.      
  

      Augmenting the eye    
      EVER SINCE 1609  , when Galileo pointed his
rickety "spy glass" at the heavens and made discoveries that turned
the scientific world upside down, people have been star-gazing with
the help of ever bigger and better instruments.  
    To see more, the most obvious solution is to increase the
size of the telescope.   By doing so, we collect more light
(each doubling of the diameter produces a fourfold increase in
the amount of light collected), and stars that were previously
below the threshold of visibility can now be seen.   These may be
inherently faint objects quite close to us, such as asteroids or
the so-called "brown dwarf" stars, or they may be very bright
objects out on the edge of the observable universe - the
quasars.  
    As the size of telescopes is increased, so is their
resolution - their ability to distinguish fine detail or to see as
separate two objects close together.   By eye, we can just
separate two 
  photo  
  caption  
objects one arc minute - (1/60') apart, whereas the giant Hale
telescope in Mt Palomar, with its 5m (200-inch) diameter mirror,
reduces this to almost one arc second - a sixtyfold
improvement.  
    Recently completed or planned are the "new technology
telescopes", their mirrors made up of between 10 and 20
computer-controlled segments which can be independently
oriented to keep the mirror true to shape, whatever its angle of
tilt.   At the European Southern Observatory in Chile an array of
four 8m telescopes is planned.   These will work in unison, with
the light-collecting ability of a single 16m diameter instrument,
or be able to function as four independent telescopes when
needed.  
    But size is not the only consideration.   Getting a
better view also means getting a clearer view, and that means
reducing the depth of atmosphere we have to look through.   The
components of the atmosphere, particularly water vapour, cause
the atmosphere to act as a filter, and wavelengths like the
ultra-
  diagram  
  caption  
violet and infra-red are almost completely blocked.   Worse is
the fact that the atmosphere is not steady or uniform in
composition, so that light from the stars is being refracted first
one way and then another.   This is why stars appear to twinkle,
their images dancing back and forth as if seen through the surface
of a swimming pool.  
    The best viewing sites are as high as possible, with clean,
steady air and wind streams.   The greatest collection of major
telescopes on earth is at 4150m on the summit of Mauna Kea,
Hawaii, well above sea fogs and low-level cloud, and in
relatively non-turbulent oceanic air.  
    Clearly, the ultimate viewing conditions are to
   box  
 be found in space, and the solution is to lift the telescope into
orbit.   This was accomplished with the successful launching of
the Hubble Space Telescope from the space shuttle Discovery in
April, and now we have a medium-size telescope in the ideal optical
environment of space.  
    Unfortunately, a design flaw in the Hubble 's primary mirror
(discovered after the launch) means that the telescope 's
observations in the visible part of the spectrum will be
severely affected.   If working properly, Hubble would have
been able to see objects 10 times smaller than the largest
telescope on earth could.   For example, while giant
telescopes like the Hale 200-inch can resolve to roughly the
stellar equivalent of reading the headlines of a newspaper at 1km,
the Hubble is theoretically capable of reading the fine print as
well.  
    Now such improvements in resolution will have to wait until
the mirror system can be repaired and that may mean bringing
Hubble back to earth.  
    No telescope can "see" anything without a light detector of
some sort.   Originally this was the astronomer peering through
the eyepiece, but in the Hubble he is replaced by the Charged
Coupled Device, the "retina" of the video camera.   By
converting light into electrical signals which are then
transmitted to the astronomers on earth, the CCD has made
observing from space possible.   Like photographic emulsion, the
CCD can be given long exposures to faint objects, thus building up
a detectable image.   So, in time, the Hubble will let us see
more and fainter detail than ever before.   We will also be able
to see at all wavelengths of the ultra-violet, which are filtered
out by water vapour at ground level.  
    From nineteenth century   astronomers'   pencil
drawings, we have moved through monochromatic and three colour
photographic reproductions into the garish world of
colour-coded CCD images with their arbitrary fluorescent blues,
lurid greens and searing magentas.   One result of these
developments is that the art of peering at the night sky
through a telescope appears to have fallen into disrepute - an
activity more suited to Boy Scouts in pursuit of badges.  
    But no one who has spent time looking through even a modest
telescope in a light-polluted urban sky would agree with this
attitude.   The mid-winter view of the Sagittarius star cloud is
addictive.   Once seen, its reappearance is impatiently awaited
each autumn.   This is also true of the Orion nebula in
summer.  
    Eighty years ago astronomy, like sketching, was one of
the polite accomplishments, and modest refractors were in
popular demand.   During the 1920s, amateur telescope-making
became the core of many astronomical societies, and the
manufacturing of parabolic mirrors one of their major functions.
  By the late 1950s, the availability of the commercially
produced Schmidt-Cassegrain telescopes seduced many amateurs
away from the work bench.   However, a handful of dedicated
amateur telescope-makers have kept the craft alive and abreast of
developments.  
    Such people belong to one of two schools: there are the
craftsmen who produce instruments of a quality, ingenuity and
performance that would put them in the five-figure bracket if
priced commercially&semi; on the other hand, the photon-grabbers
place a premium on seeing the stars and cheerfully accept that
their telescopes may look like something left over in a builder 's
yard.   But these assemblages of second-hand plywood,
  plumbers'   fittings and scroungings from car wreckers work,
and with care will continue to do so for years at a price
affordable by anyone.    
        DNA research opens door for disease
identification    
  photo    caption  
      D  R PETER GEORGE and his medical colleagues
are hearing the drum-beat of scientific revolution in their
research laboratory at Christchurch Hospital.  
    The revolutionaries may be infinitely small, but their
invisibility to the naked eye is no indication of their central
role in shaping life itself.  
    Every cell in the human body, except red blood cells,
contains identical genetic material in the shape of 23 pairs of
chromosomes.  
    Each chromosome is a long strand of deoxyribonucleic acid
(DNA) composed of two long, twisted ropes, each containing a
sequence of four chemicals - guanine, cytosine, thymine and
adenine.  
    The exact order of this chemical quartet determines an
individual 's genetic make-up.   It is a tune which can be
played in seemingly endless variations.  
    DNA is found in the white blood cells, semen, bone marrow,
dental pulp and hair roots.  
    The significance of DNA in genetics emerged in 1953
when two scientists at Cambridge University, James Watson and
Rosalind Franklin, identified DNA 's spiral-like structure 82 years
after DNA was first discovered in trout sperm taken from fish
caught in the River Rhine.  
      "I see an extraordinary potential for human
betterment ahead of us,"   Watson said later.
      "We have at our disposal the ultimate tool for
understanding ourselves."      
    For more than three decades, DNA has continued to mesmerise
the scientific world.   Its central role in shaping life and
importance in transmitting both physical characteristics and
genetic disorders, have opened new windows and transformed our
understanding of biology.   DNA will continue to play a major
part in medicine, law, and ethical issues.  
    For Peter George and a small research team at Christchurch
Hospital, research includes studies into the use of DNA in
identifying genetically determined diseases such as cystic
fibrosis, thrombosis and muscular dystrophy.  
    The revolution dawned two years ago when a new technique -
the polymerase chain reaction - opened up opportunities for fast,
inexpensive and quick identification of individual cases.  
    Using the new diagnostic method, researchers can analyse
individual DNA structures from a single cell, reading its unique
codes to decipher abnormalities with precise sensitivity.  
    The technique allows for portions of any human gene to be
amplified using enzymes.   Its sensitivity allows tissue as
small as a single hair root to be used.  
      "During the past 10 to 15 years, we have moved from
the stage of examining these questions from the angle of proteins
to studies involving DNA,"   says Dr George.  
      "  Until 1988, this was fairly difficult.
  The polymerase chain reaction has now made the difficult
easy and has revolutionised applications in medical research
and testing.  
    "    We can now work on cases which take days to
complete instead of weeks or even months.   We can work for
considerably less expense.   Previously, this work was so
expensive that you simply didn't do it.  
    "    The new technique promises such a revolution in
laboratory practice that every clinician should attempt to
understand its implications for medicine."      
    Previous methods required sophisticated laboratory
facilities often unavailable in New Zealand hospitals.  
    They were also time-consuming, taking up to two or three
weeks to complete, and were expensive, costing up to $2000 to
produce a result for an individual case or family.  
    The polymerase chain reaction allows results to be obtained
in 48 hours and costs less than $100 for an individual test.  
      "When the gene responsible for any particular
disease has been identified and sequenced, it is a simple job to
analyse its pathological mutations,"   Peter George
says.  
    Cystic fibrosis has attracted increasing attention
since the gene, which causes the condition, was isolated.   The
situation leading to a genetic disease involves the gene dictating
the way in which a protein should be made.  
    But sometimes an error occurs.   The genetic morse code
misses a dot or a dash and cannot be interpreted correctly.  
    The altered message has also changed the lives of between
250 to 300 cystic fibrosis cases in New Zealand, which cause
retardation and chronic lung and digestive disorders.   The
disease is genetically transferred from one generation to the
next.  
    The polymerase chain reaction now allows researchers to
detect the disease in families with a history of cystic
fibrosis.  
      "It is an enormously powerful tool to have at our
disposal, one which can help families immensely,"   Dr
George says.  
    The work at Christchurch Hospital is one link in a long
chain of international research, each link exploring DNA 's role in
genetics.  
    "    DNA is the stuff of life - its basic
structure.   But it cannot work by itself.     It depends on
other factors,"   Peter George says.  
      "  Conceptually, it contains all the vital
information, transmitting a code which contains information on
how many, how to  'make' an individual.   It tells us why
individuals differ, why one person is male and the other female.  
      "  Obviously, the code has to work in context with
other factors, but it remains the stuff of life in the sense
that it contains the message on how characteristics are transmitted
from one generation to another."      
    Forensic scientists are also continuing to examine the
uses of DNA finger-printing.   But this potentially powerful tool
is also highly controversial, increasingly surrounded by an
international legal and medical debate about its uses.  
    In Auckland recently the
    finger-orinting    finger-printing     technique
was used during a court case, a move strongly criticised by the
defendant 's counsel.  
      "There have been other cases in the United States
which have been thrown out,"   Peter George says.  
        "The problem with genetic fingerprinting is
that it is a tremendously powerful technique in theory, but in
practice it would be difficult to apply at a regular standard for
use as evidence in a court hearing."  
    "In the United States the standard to which it
is applied has not always been sufficiently high for admission as
evidence.  
    "    It is a powerful technique, one with applications
within the medical environment to decide questions of
paternity."      
    According to a recent report by the Scottish Crown Law
Office, DNA profiling offers the opportunity for positive
identification to be made not only to a point beyond reasonable
doubt, but to one of scientific certainty.  
      "Despite concerns expressed about DNA following
recent criticisms of the techniques used by laboratories in the
United States, there is no reason to doubt that DNA profiling can,
in appropriate cases, provide conclusive evidence to link a
suspect to a place, a victim or, in civil cases, to establish a
familial relationship,"   the report to this year 's
Commonwealth Law Minister 's Conference in Christchurch
said.  
        "The value of such evidence strengthens the
case for ensuring that samples for testing should be readily
obtainable.  
      "  Such concerns as there are about DNA profiling can
be met by imposing stringent quality controls on those carrying
out the tests."        
        Through The Looking Glass    
    "      T  oday 's fascination with
"user interfaces" is an artifact of how we currently
operate computers - with screens, keyboards, and pointing devices,
just as job control languages grew from punched card batch
systems.     Near term technological developments promise
to replace user interfaces with something very
different,"   writes Autodesk 's John Walker, in his paper
whose title I 've stolen.  
    And at Anzgraph   '89   Autodesk were showing an
amazing movie about an emerging technology.   It 's the
computer game to end them all&semi; cyberspace.  
    "    If video games are movies that involve the
player, cyberspace is an amusement park where anything that can be
imagined and programmed can be experienced.     The richness of
the experience that will be available in cyberspace can barely be
imagined today,"   writes Walker.  
    Virtual reality, virtuality,
    artifical    artificial     reality - these are
some of the other terms that have been used to denote a
computer-simulated world that provides stereoscopic imagery of
three-dimensional objects, senses the user 's head position and
rapidly updates the perceived scene, and offers some means of
interacting with simulated objects.   Once through the
looking glass, the user can move about inside his model and see
objects from this side or that, can reach for and move objects.
  And so far we 're only talking about visual and kinesthetic
feedback.   The more sensory feedback a cyberspace system could
provide - sounds, temperature, smell, wind in the face - the
richer and more "real" would be the simulated world.  
      It 's not a new idea.   Many science fiction
writers have based stories around the notion, and in 1968 Ivan
Sutherland built a helmet with two CRTs attached to the
ceiling and mechanisms to detect head movement and position.
  But in those days the computer power to generate real-time 3D
images   drawing    blurb   just wasn't available.
  Today, with fast CPUs and special purpose graphics hardware not
only available but at decreasing cost, cyberspace is a
practical direction for development.  
    The cyberspace system starring in the Autodesk movie at
Anzgraph   '89   had two small video monitors mounted on a
helmet worn by the user.   Attached to the helmet was a
tracking device like the Polhemus Navigator which tells the
position without attached wires.   Each monitor was
attached to a separate graphics controller which rendered the
view of the three dimensional model of the world from that eyes
viewpoint, updating the display as the head moved.   Then
there was a kind of glove with a navigator attached, with which
the user could point to or grasp objects in cyberspace.  
    The obvious application of such a system is to 3D
engineering and architectural design, but cyberspace is a
general purpose technology which could be used for as yet
unanticipated applications.  
    One application already discovered, though, is
    entertainmet    entertainment    : the NASA Ames
Group, which developed a cyberspace system and demonstrated it to
members of the US Congress, reports that one of the problems in
their demonstrations was that people liked it so much that they
would spend all day in the cyberspace lab if not forcibly
removed!  
    And already there 's a whole rash of other fascinating
applications.   A US company called, of all things, Cyberspace,
has brought out a laptop computer with no screen.   Using a
lightweight eyepiece worn on a headband, the display appears to
float in mid-air about two feet from the user - a crisp, full-sized
image that no-one else can see.   The eye-piece, developed by
Reflection Technologies Inc., uses light-emitting elements, lenses
and a tiny oscillating mirror to make the image appear in front
of the user.   It 's being designed into dozens of products
from a variety of companies.   Then there 's a line of portable
information equipment being developed by Hughes Aircraft Co, and an
electronic book from Selectronics Inc that will allow a
bookshelf of information to be carried in a shirt pocket, or
clipped to a belt.  
    What next?      

  

        SECTION TWO    
        New Zealand      
        INTRODUCTION TO THE NEW ZEALAND
SECTION      
    In this section the aim has been to provide an historical
record of the many and varied models produced by this country 's
manufacturers over the years by illustrating as many as possible
of the total.   Some readers may be surprised to learn how many
different models there were, and they may also be surprised to
learn how many different brandnames there were.   When the very
large numbers of imported sets are included, the total number of
brandnames which appeared on the New Zealand market at one time
or another is quite staggering.   Furthermore, it is surprising
to find how many of these sets have survived to become
present-day collectibles, and the end is not in sight.  
    Since publication of the previous book  *  ,
  *  reference     more N.Z. brandnames
have come to light, so rather than list them as an addendum a
completely new list has been included at the end of this section.
  Many of these names have been provided by eagle-eyed members
of the N.Z. Vintage Radio Society who have taken the trouble to
send them in.  
     A not infrequent complaint voiced by present-day radio
collectors, often in respect to N.Z. radios, is - "Why didn't
they put the model number on the chassis?"   Why, indeed!
  Not that lack of such identification was peculiar to locally
made sets.   The answer, at least in the case of many small
pre-war manufacturers, is simply that there weren't any.
  Model numbers, that is.   But even if there had been, it
wouldn't have done any good because said manufacturer never
issued such a thing as a circuit diagram.   Perhaps it is even
more frustrating to find a set which does have a model number
but, for reasons best known to himself, the manufacturer did not
see fit to mark the number on the chassis.   And to add insult
to injury, some   manufacturers'   service sheets contained
the words - "When ordering spare parts always quote the model
number".   Ho, Hum!  
        AKRAD RADIO CORP. and PYE LTD      
    The brandname originally used on radios made by Akrad was
"Futura", but after 1940 the name "Pacific" was substituted.
  Although the Pacific name had previously been owned by
another company, Pacific Radio Ltd, it was then out of use due
to the demise of the former owner.   Upon recommencement of
radio manufacture after the war, Akrad continued to use the name
Pacific, and in addition introduced the name "Regent".   These
two names continued to be used side by side for the same models
until 1953 when Pye came on the scene.  
    With the launching of Pye radios on the N.Z. market
Akrad 's two earlier brandnames were   photo   discontinued
and the name "Clipper" was introduced, mainly for use on low
priced unfranchised sets, though in some cases Clipper sets were
directly equivalent to certain Pye models.   A third brandname,
used for a short period after 1959, was "Astor".   This name
was of Australian origin and its use by Akrad came about as a
result of the takeover of Radio Corporation of N.Z. by Pye/Akrad
at this time.    
  photo  
        BELL RADIO-TELEVISION CORPORATION      
    Bell commenced business in 1950, taking over from the
earlier Antone Radio Co.   For the next twenty years small
plastic cased radios continued to be churned out.   The number
of different models which used the same cabinet were the 5-valve
"Colt", the 3-valve "Champ", the 3-valve "Cadet" and a 4-valve
version of the Colt using a solid-state rectifier.  
    Then there were wooden cased versions of the Colt with
cabinets of solid oak, oak veneer and walnut veneer.   A
5-valve dual wave model was also available and was normally
supplied in a walnut veneer cabinet, when it was known as "The
Planet".   All in all quite a remarkable production record for
any small radio, which ended in 1967 when a transistorised
version known as the "Solid State Colt" was marketed.  
    The cabinet die used for locally moulding the plastic
cabinets came from Australia where it had originally been used
to produce the same cabinets for Airzone.   Because the Airzone
model did not have a tone control it needed only two controls,
but as the Colts had three controls it was necessary to provide
an extra hold in the cabinet front in the space originally
provided for the Airzone decal.  
  photo  
        COLLIER &amp; BEALE LTD      
    At the end of World War II Collier &amp; Beale Ltd, in common
with others in the industry, picked up the threads of peacetime
production and had their first post-war sets leaving the factory
in 1946.   Subsequently, quite a large range of models  was
marketed under the (unfranchised) Pacemaker name.  
    In the early post-war C &amp; B continued their policy of
supplying private-brand sets to the few remaining customers, the
Electric Lamphouse (Ensign) and His Masters Voice (HMV) being two
of them.   The existing names Cromwell and Gulbransen were
carried on as brandnames for franchised dealers for a few years
but both were discontinued after 1958.  
    In 1955 an important change in the company fortunes
occurred when the General Electric Co. of England (GEC) acquired
a majority holding.   One noticeable effect of this change was
reflected in the different manufacturing policy introduced.
  For example, where C &amp; B had always used heavy gauge cadmium
plated steel chassis they now used thin tinplate.   This was
obviously a cost cutting procedure which extended to such things
as using 100-volt working capacitors in place of the formerly
used 400-volt types.   Needless to say, reliability dropped
sharply as a result.  
  photo  
    Another change occurring after 1956 was the introduction
of model names in place of model numbers.   Prior to this any
names used, for example "Petit" (model 5155), were simply
telegraphic codewords, the only exception being the 1949 "Little
Jewell" (model 518N).   It is true that in pre-war days there
were a few model names used, for example the Radion "Little
Aristocrat", "Parliament", "Elstree" and "Rugby", but these were
exceptions.  
  photo  
        DOMINION RADIO &amp; ELECTRICAL CORP.      
    This firm was set up in 1939 expressly for the purpose
of manufacturing Philco radios in New Zealand, but the outbreak
of World War II soon halted production.   Philco radios
produced in the early post-war years were in any cases almost
"carbon copies" of pre-war models, an example being the 155, an
8-valve 3-band set of 1945 which was almost identical to the 1941
model 157.   As in pre-war days, the use of chromium-plated
chassis was initially continued, but within a year or two had
ceased.  
    The use of American-style model numbering which
incorporated a year of issue indicator had been introduced in
1941, as in the case of models 41-710 and 41-722 for example, but
was not continued with after the war&semi; in fact the American
influence on local production waned steadily from this time
onwards and by the early 1950s had almost vanished.  
      PHILCO RADIOS    
    Up to 1956 only Philco radios had been made by
  photo   Dreco, but at this time a new unfranchised
brandname, "La Gloria", was introduced, initially for the purpose
of marketing a line of low priced radiograms.   A limited range
of radios was also sold under   photo   the La Gloria name
over the next few years.   On the other hand, the name Philco
had, by 1962, been withdrawn from the market-place and to counter
this loss a "new" name "Majestic", was introduced in its place.
  Although also seen on television sets, this name did not
remain in use for long and had disappeared by 1965.   Another
brandname, used only on portable record players, tape recorders
and transistor radios, was "DRECO", but this name also soon
disappeared.  
  photo  
        HIS MASTER'S VOICE N.Z. LTD      
    Before World War II HMV had imported nearly all the sets
sold by them, only a very few being of local manufacture.   In
the post-war years they continued to make radios and radiograms
in their Wellington factory until 1957, during which period
approximately 62 different models were produced.   In addition,
there were 10 models made by other manufacturers, mainly Collier
&amp; Beale.  
    An unusual pre-war model of 1939 vintage was a 2-valve
TRF which had the distinction of being the smallest set made in
New Zealand in those days.   A set to have what must have been
the most unusual cabinet styling seen in this country was a 1946
5-valve model housed in a mirror-glass cabinet, or rather it had
a wooden cabinet overlaid with mirror glass.   It remains the
only locally made example of its type.  
    Commencing in 1945, a system of model identification
using three numerals followed by one or two letters was
introduced.   In this system the first two numerals indicated
the year of design, rather than the year of issue, though these
two dates often coincided, of course, while the third numeral
indicated the number of valves employed.   In addition various
letters were used as suffixes to distinguish different models
having the same number of valves but made in the same year.
  These suffixes were:  
    A = used to distinguish between two models  ,   AW
= all wave, D = dual-wave, S/B = band spread, P = portable, R =
car radio, R/G = radiogram  ,   RP = record player (without
radio), C/RG = radio with auto changer, T/RG = table model
radiogram.   Examples of the complete coding are: 465 = 1946
5 valves, 465D = 1946 5-valve dual-wave, 467SB = 1946 7-valve
bandspread, 526 D/CRG = 1952 6-valve dual-wave gram with auto
changer.  
    Commencing in 1947, model names were assigned in addition
to model numbers and this practice was continued right through
to the end of receiver production.   Unfortunately for those
concerned, then as now, HMV did not mark model numbers on chassis
made prior to 1948 which made     indentification  
  identification     difficult unless one had a set of
service manuals.   These manuals were particularly useful as,
in addition to the usual technical information, they contained
pictures of each model.  
  photo  
    The need for an improved system of coding was apparent
by the end of 1952 when there were no less than five different
5-valve models issued in that year.   Commencing towards the
end of 1954 a new model-coding system was introduced which
consisted of four numerals only without any suffixes.   In this
system the first two numerals indicated the year of issue and the
remaining two indicated serially the number of models issued that
year, with subsequent issues being numbered 5402, 5403 and so
on.  
    After 1957 HMV ceased manufacture and had sets supplied
by other firms such as Akrad, Green &amp; Hall and Philips.  
        JOHNS LTD - WELLMADE LTD      
    Although Johns Ltd had established a separate factory in
1928 they did not commence using the brandname "Well-Mayde" until
1931.   Even then this name never appeared on the front of any
sets&semi; it was to be found only on a nameplate on the chassis.
  In 1933 the name "Companion" was introduced in its place and
remained in use thereafter.  
    A simple system of chassis model coding was introduced
in 1931 which indicated the number of valves used and the year
of issue:  
    WM31 3 valves (plus metal rectifier) 1931
  WM61 6 valves (inc. rectifier) 1931
  WM81 8 valves (inc. rectifier) 1931  
    Cabinet styles were given model names and it is
interesting to note, that for the years 1931 and 1932 Maori names
were chosen, these being Ariki, Kiwi, Rangatira and Tui.  
    A slight alteration to the coding occurred in 1932 when
the letters "WM" were changed to "SG" to indicate Screen Grid,
an important sales feature in at the time.   As yet all models
were still of the TRF variety, superhets not appearing until 1933
when the coding was changed once again by dropping the first two
letters.   By 1935, with the advent of shortwave coverage, the
suffixes BC, DW and AW were added to the model numbers thus:
65BC, 65DW and so on&semi; later "AW" was changed to "TW" to indicate
Triple Wave.   For some reason that year 's system applied only
to AC models, battery sets being assigned model names
only.  
  photo  
        PHILIPS IN NEW ZEALAND      
    Although most of the Philips and Mullard receivers
illustrated here are examples of N.Z. made sets&semi; a few British
and Dutch models have been included for the sake of convenience
in presentation.  
    Philips commenced radio manufacture in this country on
a small scale in 1939 but the factory output was soon affected
by wartime conditions and, in common with other local
manufacturers, they   photo   were compelled to cease
production in 1942.   Prior to opening their own factory a few
models had been "made on behalf" by the Radio Corporation of N.Z.
between the years 1934-36, such sets being required to fill gaps
in the imported range.   Likewise, a few sets were imported
from Australia at much the same time.   Following the end of
the war, all Philips and Mullard radios sold in this country were
locally produced&semi; the last valve-operated model being marketed
in 1969.  
        RADIO CORPORATION OF N.Z. LTD -
COLUMBUS      
    The Columbus brandname was introduced in 1937 as a
"house" brand following a change in company policy which saw all
previous "private" brand production, with the exception of
Courtenay, phased out.   From then on nearly all models
produced by Radio Corp. were marketed under both brandnames,
though Courtenay was not so widely distributed as Columbus and
never became as well known.   In pre-war days the cabinet
styles of the two brands differed considerably but in later years
the differences were generally not so pronounced.   Nearly all
models were marketed under both names, though there were one or
two exceptions.  
    An interesting Columbus set made in 1939 was
  photo   the 10-valve model 88 all-wave console which
featured a special Philips low-noise RF valve, EF8G, and had
variable IF selectivity together with motorised push-button
tuning which could be remotely controlled.   It was one of only
three models which had a push-pull output stage.   Altogether
a most impressive, not to say unique, combination of
features.  
    Two top-of-the-line models, marketed in both brands, were
the 90 and 91.   The former was a swept up version of the
earlier 75, while the 91 was a 9-valve model having push-pull
output.   This latter feature was rarely used by Radio Corp.,
in fact apart from the above mentioned 88, only one other set,
the 13-valve high fidelity model 99 of 1946, had a push-pull
output stage.  
    Although small numbers of transistor radios were produced
from 1957 on, by 1960 radio production at the Wellington factory
had ceased.   One of the last valve-equipped sets to carry the
Columbus name was the model RG11 radiogram of 1960, but this was
made at Akrad 's Waihi factory.  
  photo  
        RADIO CORPORATION OF N.Z. LTD -
COURTENAY      
    The brandname Courtenay was originally introduced in 1930
on receivers made by W. Marks, and this name was carried on after
the Columbus name was introduced in 1937.   Courtenay radios
were originally distributed by the Stewart Hardware Co. but in
1935 distribution was taken over by Turnbull &amp; Jones Ltd.   In
1956 the Courtenay name was withdrawn from the market when
Turnbull &amp; Jones gave up handling radios.  
  photo  
        RADIO CORPORATION OF NEW ZEALAND
LTD      
    Because at one time there were so many different private
brandnames appearing on sets made by Radio Corporation they will
be given only the briefest mention.   Of the names appearing
between 1933 and 1937 only two ever became well known - "Pacific"
and "Stella".   Amongst the lesser known brands were Acme,
Audiola and CQ.  
    Not surprisingly, most of the brandowners wanted their
sets to look as different as possible from Courtenay or Columbus
and, as can be seen from the accompanying pictures, many of them
went to some trouble to accomplish this.   Particularly in the
case of Pacific was this endeavour noticeable where Art Deco
styling was favoured.  
    On the other hand the cabinets used by Stella were in
most cases quite similar to those used by Columbus and Courtenay,
particularly in the 1937 models which were the last private brand
sets issued.  
  photo  
          RADIO LTD - RADIO 1936 LTD - ULTIMATE EKCO
LTD      
    The first mains-operated Ultimate radio, produced in
1929, was an electrified version of the earlier battery-operated
Screen Grid Four.   It used exactly the same sized metal
cabinet and the same circuitry.   The main difference was the
use of 5-pin sockets for the first three valves, which were not
indirectly heated AC types.   A separate power pack was
used.  
    It is interesting to note that, following the tradition
established in 1927 when the all-wave   photo   battery sets
were launched, for the first three years AC receivers were made
only in all-wave form.   At this early date shortwave reception
was still very much of a novelty and very few manufacturers in
any countries had produced all-wave receivers for general use
before 1933.   Even though these Ultimate sets could boast
single-dial tuning they were still regenerative TRFs using
plug-in coils, which meant that they were sets for the enthusiast
rather than the ordinary listener.  
    However, by 1932 production of all-wave receivers had
been almost completely overshadowed by the arrival of
superheterodynes having BC band coverage only.   This state of
affairs continued for the next few years even though Radio Ltd
carried on their pioneering role by becoming the first N.Z.
manufacturer to make all-wave superhets.   By this time public
interest in shortwave reception had waned and not until several
years had elapsed did multi-wave receivers become popular.
  For the record, it should be mentioned that in 1932 Radio Ltd
produced a self-powered shortwave converter suitable for use on
any make or model of broadcast receiver.  
    A form of model identification for sales purposes was
introduced in 1931, using a three-letter code which indicated the
number of valves and the price of each model, thus: 856 indicated
8 valves &pound;56, 527 indicated 5 valves
&pound;27, and so on.   This system was carried on
through 1933 and was also used with the Courier brandline
marketed during this period.   In the case of Courier the same
actual numerals were used but their relative positions were
interchanged, thus 514 Ultimate became 145 Courier, and so on.
  Chassis identification was by means of an alpha-numeric code
die-stamped into the rear chassis apron or flange, the figures
preceding the serial number.  
    During 1934 cabinet styles became identified by model
names and the former three-digit system was discontinued.
  Chassis models were henceforth identified by figures
die-stamped on a small metal nameplate fastened to the rear
chassis apron.   Examples of the new coding are 4K, 5N, NS, but
there was no complete uniformity.   During 1935 the letters C,
D, E, F, L or X were used singly or in combinations such as CAU,
CES, LR, XC and so on.  
    Not until 1936 was a standardised system adopted wherein
the first numeral indicated the year of issue, thus A = 1936, B
= 1937, C = 1937-38, C and D = 1939, E = 1940, F = 1941.   The
fact that "C" had also been used in 1935 seems to have been an
unfortunate oversight.  
    So far, so good, but for some unfathomable reason Radio
1936 Ltd did not often make use of this coding when preparing
circuit diagrams, here it was (and is!) most frustrating to find
descriptions only being used, thus "6-valve broadcast 1938 model"
or "5-valve dual wave 1937 model".  
    By 1939, however, gradual change was taking place whereby
the actual chassis models came to be   photo   included on
the circuit diagrams.   But even as late as 1940 some circuit
diagrams were still being printed without accompanying model
numbers.  
  photo  
    Upon recommencement of production after the war, a new
system of chassis coding came into use.   All radios were
allocated model identification commencing with the letter "R"
(for radio), while all electrical appliances used "E" (for
electrical).   Initially just two letters sufficed but after
about three years it became necessary to use three letters.
  To summarise: RA to RY from 1946 to 1948, RAA to RAZ from
1948 to 1951, RBA to RBZ from 1951 to 1953, RCA to RCX from 1953
to 1959, RDA to RDZ from 1956 to 1959, REC to REU from 1959 to
1964.   Note: there is some overlap in the later periods.  
    Manufacture of transistorised radios commenced in 1957
and these used the same coding system.   After the closure of
the Quay Street factory in 1967 any radios bearing the Ultimate
name were made by Akrad and used a different
system.     